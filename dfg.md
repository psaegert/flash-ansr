# A: Improved Amortization (PhD Student 1 under PI 1)
Focuses on improvements to the amortization of Symbolic Regression capabilities without much focus on case-based specialization.
The goal is to develop a highly powerful, general foundational model for Symbolic Regression.

## Amortized Control and Prompting Capabilities
Problem:
1. Limited support for incorporating prior knowledge
    Often have prior knowledge or preferences for expressions.
    Currently, the ability to incorporate hard prior knowledge is limited, and is not way to specify soft preferences like the desired realism of generated expressions.
2. No incentive to generate high-quality (e.g. realistic) expressions
    Current amortized training approaches use randomly sampled expressions (Lample & Charton) or use the output of case-based methods as training targets.
    Thus, the generated expressions do not follow any specific quality criteria such as realism, specificity or elegance.

Idea:
Supply the model with varying amounts of additional context during training, and optimize the likelihood of expressions conditioned on that context.
-> Trains the model directly to incorporate prior knowledge and preferences into the generation process.
-> Provides a mechanism for specifying and enforcing desired properties in the generated expressions.

Method:
1. Specify high-level properties
    a. Develop measures for soft preferences
        - Conciseness and elegance (ratio of simplified length to original length)
        - Specificity (via entropy of data generated by an expression)
        - Realism and naturalness (Train GAN or use existing SOTA general-purpose LLM)
        - Decomposability and separability (check if expression has components A(.) # B(.) with # a binary arithmetic operator)
    b. Extend types of prior knowledge
        - Units of variables (expression must be dimensionally consistent and match the units of the dependent variable)
        - Shape Constraints
        - Data Uncertainty (support for homoscedastic and heteroscedastic uncertainty)
2. Conditional training
    - Sample varying amounts of knowledge and preferences during training
    - Maximize the likelihood of expressions conditioned on that context

## Estimating long-term consequences with value-functions
Problem:
Standard next-token prediction optimizes each token choice greedily based on the immediate likelihood of next tokens in the training data.
This leads to two main issues.
1. Exposure Bias
    The model never sees its own mistakes during training, leading to suboptimal choices in later tokens during inference.
2. Short-sightedness
    A locally likely token choice might lead to a globally poor or ill-fitting full expression down the line of autoregressive generation.

Idea:
1. Value-based decoding inspired by AlphaGo/Zero
    Use a value function to estimate the final quality of an expression based on its current state, and use this estimate to guide the search for expressions.
    -> Allows the model to consider the long-term consequences of its choices, rather than just the immediate likelihood of next tokens.
2. Diffusion paradigm
    Instead of autoregressive next-token prediction, use a diffusion architecture to model the generation of expressions.
    -> The diffusion process iteratively refines the entire expression, allowing for more coherent and globally optimal expressions.

Method:
1. Amortized Reinforcement Learning and Value-based decoding
    a. Learn Value Functions with RL
        - Add heads for various value functions to the model (These include the fit-error and the high-level properties defined above)
        - During training, expressions are sampled from the current policy, value-based search or an external model.
        - Each completed expression is evaluated by the measures
            - Fit-error (e.g. RMSE)
            - High-level properties (e.g. conciseness, realism)
        - The estimates of the value functions and the policy are updated through RL, e.g. using PPO or DrGRPO.
        - This RLFT stage is added after Supervised Training and will use SOTA PEFT methods for efficient training (QLoRA) and to avoid catastrophic forgetting (KL-penalty).
    b. Value-based decoding
        - Use the value functions to guide the search for expressions during inference.
            - The value functions are combined into a single value function by a weighted user-defined combination which enables prioritization of certain properties.
        - The value-based search for expressions is done using
            - MCTS
            - PUCT
            - Beam Search (on the value function)
2. Diffusion Architecture
    - Use a discrete diffusion architecture to model the generation of expressions.


# B: Hybridization (PhD Student 2 under PI 2)
Combines and unifies the amortized Symbolic Regression capabilities with case-based specialization.

## Guided Inference Loop
Problem:
Limitations of current approaches.
- Fully amortized SR: Has large amount of knowledge, is fast, but lacks refinement capabilities.
- Fully case-based SR: Iteratively refines expression candidates, but starts from scratch, is slow, and uses random or heuristic mutations.
- Existing hybrid approaches: Use amortized models to generate an initial population, but do not refine them.

Idea:
Use the amortized model for both the initial population and informed mutation of the population.
-> Faster refinement of expressions by leveraging the knowledge encoded in the amortized model for more effective mutations.

Method:

## Diverse Training Targets by CB methods for amortized training
Problem:

Idea:

Method:

## Case-Based Adaptation/Specialization of the Amortized Model
Problem:

Idea:

Method:


# C: Application & Software  (PhD Student 1 & 2 under PI 1 & 2)

## Application: Effective Theories
Problem:

Idea:

Method:

## Software