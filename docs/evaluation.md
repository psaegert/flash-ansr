# Evaluation

## Read this first
- Quick run: `flash_ansr evaluate-run -c configs/evaluation/run_flash_ansr_nguyen.yaml -v`.
- Outputs: pickles with entries containing `expression`, `log_prob`, `fits` (per-dataset metrics), and optional `placeholder` entries when data generation fails but counts must stay aligned.
- Scope: shared engine covers FlashANSR, PySR, NeSymReS, and skeleton-pool baselines via a single YAML config.

## General workflow

- Use `flash_ansr evaluate-run -c <config>` as the single entrypoint; configs live under `configs/evaluation/` (and `configs/evaluation/scaling/` for sweeps).
- Each config wires a `data_source`, a `model_adapter`, and a `runner` (persistence/resume). The same structure covers FlashANSR, PySR, NeSymReS, and baselines.
- `runner.resume` allows checkpointed pickles to continue; placeholders are inserted when sample generation fails so counts stay consistent.
- `datasets_per_expression` controls how many deterministic datasets per skeleton/equation are generated; sampling mode is removed.

## Models and baselines

- **FlashANSR**: Default adapter; supports generation overrides (beam/softmax/MCTS) and prompt options. See the example configs under `configs/evaluation/run_flash_ansr_*.yaml` and scaling sweeps.
- **PySR**: Adapter expects PySR installed; config fields mirror PySR runtime knobs (timeout, iterations, parsimony). Watchdog helper: `scripts/evaluate_PySR.py`.
- **NeSymReS**: Adapter expects external checkout + checkpoint paths; exposes beam width/restarts. See `run_nesymres.yaml` and scaling configs.
- **SkeletonPoolModel (baseline)**: Transformer-free baseline that samples skeletons from a provided pool and only refines constants. Configure via `model_adapter.type: skeleton_pool` (or add a dedicated config entry) with pool path/config, `samples`, `unique`, `ignore_holdouts`, and `seed`. Useful for ablations and replication.
- **BruteForceModel (planned)**: Placeholder for exhaustive search baseline; document here once added.

## Express

Use, copy or modify a config in `./configs`:

```
./configs
├── my_config
│   ├── dataset_train.yaml          # Link to skeleton pool and padding for training
│   ├── dataset_val.yaml            # Link to skeleton pool and padding for validation
│   ├── tokenizer.yaml              # Tokenizer settings
│   ├── model.yaml                  # Model settings and link to simplipy engine
│   ├── skeleton_pool_train.yaml    # Sampling and holdout settings for training
│   ├── skeleton_pool_val.yaml      # Sampling and holdout settings for validation
│   └── train.yaml                  # Data and schedule for training
```

Use the helper scripts to import data, build validation sets, and kick off training:

```sh
./scripts/import_test_sets.sh                     # optional, required only once per checkout
./scripts/generate_validation_set.sh my_config    # prepares validation skeletons
./scripts/train.sh my_config                      # trains using configs/my_config
```

For more information see below.

## Manual

### 0. Prerequisites

Test data structured as follows:

```sh
./data/ansr-data/test_set
├── fastsrb
│   └── expressions.yaml
```

The test data can be cloned from the Hugging Face data repository:

```sh
git clone https://huggingface.co/psaegert/ansr-data data/ansr-data
```

### 1. Import test data

External datasets must be imported into the ANSR format:

```sh
flash_ansr import-data -i "{{ROOT}}/data/ansr-data/test_set/soose_nc/nc.csv" -p "soose" -e "dev_7-3" -b "{{ROOT}}/configs/test_set/skeleton_pool.yaml" -o "{{ROOT}}/data/ansr-data/test_set/soose_nc/skeleton_pool" -v
flash_ansr import-data -i "{{ROOT}}/data/ansr-data/test_set/feynman/FeynmanEquations.csv" -p "feynman" -e "dev_7-3" -b "{{ROOT}}/configs/test_set/skeleton_pool.yaml" -o "{{ROOT}}/data/ansr-data/test_set/feynman/skeleton_pool" -v
flash_ansr import-data -i "{{ROOT}}/data/ansr-data/test_set/nguyen/nguyen.csv" -p "nguyen" -e "dev_7-3" -b "{{ROOT}}/configs/test_set/skeleton_pool.yaml" -o "{{ROOT}}/data/ansr-data/test_set/nguyen/skeleton_pool" -v
```

with

- `-i` the input file

- `-p` the name of the parser implemented in `./src/flash_ansr/compat/convert_data.py`

- `-e` the SimpliPy engine version to use for simplification

- `-b` the config of a base skeleton pool to add the data to

- `-o` the output directory for the resulting skeleton pool

- `-v` verbose output

This will create and save a skeleton pool with the parsed imported skeletons in the specified directory:

```sh
./data/ansr-data/test_set/<test_set>
└── skeleton_pool
    ├── skeleton_pool.yaml
    └── skeletons.pkl
```

### 2. Generate validation data

Validation data is generated by randomly sampling according to the settings in the skeleton pool config:

```sh
flash_ansr generate-skeleton-pool -c {{ROOT}}/configs/${CONFIG}/skeleton_pool_val.yaml -o {{ROOT}}/data/ansr-data/${CONFIG}/skeleton_pool_val -s 5000 -v
```

with

- `-c` the skeleton pool config
- `-o` the output directory to save the skeleton pool
- `-s` the number of unique skeletons to sample
- `-v` verbose output

### 3. Train the model

```sh
flash_ansr train -c {{ROOT}}/configs/${CONFIG}/train.yaml -o {{ROOT}}/models/ansr-models/${CONFIG} -v -ci 100000 -vi 10000
```

with

- `-c` the training config
- `-o` the output directory to save the model and checkpoints
- `-v` verbose output
- `-ci` the interval to save checkpoints
- `-vi` the interval for validation

### 4. Evaluate the model

⚡ANSR, PySR, NeSymReS, and the FastSRB benchmark now run through a shared evaluation engine. Each run is configured in a single YAML that wires a **data source**, a **model adapter**, and runtime **runner** settings. The new CLI subcommand looks like this:

```sh
flash_ansr evaluate-run -c configs/evaluation/run_flash_ansr_nguyen.yaml -v
```

Use `-n/--limit`, `--save-every`, `-o/--output-file`, `--experiment <name>`, or `--no-resume` to temporarily override the config without editing the file. When a config defines multiple experiments (see `configs/evaluation/scaling/`), omitting `--experiment` now runs **all** of them sequentially; pass an explicit name if you only want a single sweep entry.

#### 4.1 Config-driven workflow

Every run config (see `configs/evaluation/*.yaml`) follows the same structure:

```yaml
run:
  data_source:  # how to create evaluation samples
    ...
  model_adapter:  # which model/baseline to call
    ...
  runner:        # bookkeeping + persistence
    limit: 5000
    save_every: 250
    output: "{{ROOT}}/results/evaluation/v22.4-60M/nguyen.pkl"
    resume: true
```

- **`data_source`** selects where problems come from. `type: skeleton_dataset` streams from a `FlashANSRDataset`, while `type: fastsrb` reads the FastSRB YAML benchmark. Common knobs include `n_support`, `noise_level`, and target sizes. Provide `datasets_per_expression` to iterate each skeleton or FastSRB equation deterministically with a fixed number of generated datasets (handy for reproducible evaluation sweeps).
- **`model_adapter`** declares the solver. Supported values today are `flash_ansr`, `pysr`, and `nesymres`, each with their own required fields (model paths, timeout, beam width, etc.).
- **`runner`** controls persistence: `limit` caps the number of processed samples, `save_every` checkpoints incremental progress to `output`, and `resume` decides whether to load previous results from that file.

When `resume` is enabled the engine simply reloads the existing pickle, skips that many deterministic samples, and keeps writing to the same file. If a dataset cannot be generated within `max_trials`, the runner now appends a placeholder entry (`placeholder=True`, `placeholder_reason=...`) so the results length still reflects every attempted expression/dataset pair. Downstream analysis can filter those placeholders, but their presence keeps pause/resume logic trivial and avoids juggling extra state files. Skeleton dataset evaluations remain sequential—`datasets_per_expression` (default `1`) controls how many deterministic datasets are emitted per skeleton, and the previous random sampling mode has been removed.

Running `flash_ansr evaluate-run ...` loads the config, resumes any previously saved pickle, instantiates the requested data/model pair, and streams results back into the same output file.

#### 4.2 Example run configs

Ready-to-use configs for the most common set-ups live under `configs/evaluation/`. Adjust the dataset/model paths as needed.

##### 4.2.1 FlashANSR on curated sets

`configs/evaluation/run_flash_ansr_nguyen.yaml` evaluates a FlashANSR checkpoint on the Nguyen test set with `n_support=512` and writes `results/evaluation/v22.4-60M/nguyen.pkl`. Run it with:

```sh
flash_ansr evaluate-run -c configs/evaluation/run_flash_ansr_nguyen.yaml -v
```

Update the dataset path or the `runner.limit` to sweep different test suites without touching the CLI.

##### 4.2.2 FastSRB benchmark

`configs/evaluation/run_fastsrb.yaml` feeds the FastSRB YAML benchmark through the same FlashANSR model (2 draws per equation, 512 support points). Launch it via:

```sh
flash_ansr evaluate-run -c configs/evaluation/run_fastsrb.yaml -v
```

Tweak `data_source.datasets_per_expression`, `eq_ids`, or `noise_level` inside the YAML to match your experiment.

##### 4.2.3 PySR baseline

1. Install [PySR](https://github.com/MilesCranmer/PySR) alongside flash-ansr in the active environment.
2. Use `configs/evaluation/run_pysr_nguyen.yaml` to mirror the Nguyen evaluation protocol (timeout 60 s, 100 iterations, parsimony 1e-3).
3. Launch the run:

   ```sh
   flash_ansr evaluate-run -c configs/evaluation/run_pysr_nguyen.yaml -v
   ```

The config automatically reuses the dataset's bundled SimpliPy engine; provide `model_adapter.simplipy_engine` in the YAML if you need an explicit override.

PySR occasionally stalls on long sweeps, so `scripts/evaluate_PySR.py` now wraps `flash_ansr evaluate-run` with a watchdog that restarts the command whenever CPU usage idles out. It accepts the same core flags (`-c/--config`, `--experiment`, `-n/--limit`, `-o/--output-file`, `--save-every`, `--no-resume`) and forwards anything after `--` directly to `flash_ansr evaluate-run`. Example:

```sh
python scripts/evaluate_PySR.py \
  -c configs/evaluation/scaling/pysr_v23_val.yaml \
  --experiment pysr_v23_iter_00256 -v
```

Set `--eval-python` if your PySR installation lives in a separate conda env, and use `--log-file` to collect watchdog diagnostics under a dedicated path.

##### 4.2.4 NeSymReS baseline

1. Clone [NeuralSymbolicRegressionThatScales](https://github.com/SymposiumOrganization/NeuralSymbolicRegressionThatScales) and download the `100M` checkpoint as described in their README.
2. Install flash-ansr and NeSymReS (see their instructions: `pip install -e src/ && pip install lightning`).
3. Update `configs/evaluation/run_nesymres.yaml` so `eq_setting_path`, `config_path`, and `weights_path` point to your checkout, and adjust the output location if desired.
4. Run the evaluation:

   ```sh
   flash_ansr evaluate-run -c configs/evaluation/run_nesymres.yaml -v
   ```

The adapter handles SimpliPy compilation (`simplipy_engine: "dev_7-3"` by default) and exposes `beam_width`/`n_restarts` knobs inside the config.

##### 4.2.5 Compute-scaling sweeps

New multi-experiment configs under `configs/evaluation/scaling/` capture the compute-scaling curves requested for ⚡ANSR, PySR, and NeSymReS on both FastSRB and the `v23.x` validation skeleton pool. Each file defines a set of named experiments (`flash_ansr_fastsrb_choices_00032`, `pysr_v23_iter_08192`, `nesymres_fastsrb_beam_00128`, …) that vary the relevant runtime parameter:

- **FlashANSR**: `generation_overrides.kwargs.choices` steps from 1 → 16,384 using SoftmaxSampling.
- **PySR**: `niterations` mirrors the same powers-of-two sweep.
- **NeSymReS**: `beam_width` ranges from 1 → 256.

Run the entire sweep with a single command (`flash_ansr evaluate-run -c <config>`). To focus on one entry, keep using `--experiment`. For example, to run FlashANSR on FastSRB with 1,024 choices:

```sh
flash_ansr evaluate-run \
  -c configs/evaluation/scaling/flash_ansr_fastsrb.yaml \
  --experiment flash_ansr_fastsrb_choices_01024 -v
```

Outputs are namespaced under `results/evaluation/scaling/<model>/<dataset>/...` so sweeps can run back-to-back.

#### 4.3 Legacy CLI commands

The historical subcommands (`flash_ansr evaluate`, `evaluate-fastsrb`, `evaluate-pysr`, `evaluate-nesymres`) continue to work, but they are now thin shims around the new engine and will be retired in a future release. Prefer `flash_ansr evaluate-run` plus a run config so every tool—⚡ANSR, PySR, NeSymReS, or FastSRB—shares a single, reproducible entry point.
