{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeff14f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, AMP dtype: torch.bfloat16\n",
      "\n",
      "--- Starting Benchmark ---\n",
      "Model dim: 512, Heads: 8, Encoder Blocks: 6, Decoder Blocks: 2\n",
      "Batch size: 128, Input set size: 512, Output set size: 64\n",
      "Checkpointing: Enabled\n",
      "Running warm-up iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.82it/s]\n",
      "100%|██████████| 50/50 [00:17<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark Results ---\n",
      "Total time for 50 iterations: 17.69 seconds\n",
      "Average time per iteration: 353.88 ms\n",
      "Throughput: 361.71 samples/sec\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# A good practice for enabling/disabling checkpointing globally\n",
    "USE_CHECKPOINTING = True\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # The output is cast to the input's dtype, preventing issues with mixed precision.\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU Feed-Forward Network.\n",
    "    This is a memory-efficient and performant alternative to the standard FFN.\n",
    "    Reference: \"GLU Variants Improve Transformer\" (https://arxiv.org/abs/2002.05202)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or 4 * dim\n",
    "        # Use a heuristic for the intermediate dim, as in Llama models\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = (hidden_dim + 7) & -8 # Multiple of 8\n",
    "\n",
    "        self.w13 = nn.Linear(dim, 2 * hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate, up = self.w13(x).chunk(2, dim=-1)\n",
    "        x = F.silu(gate) * up\n",
    "        x = self.w2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention block. Fuses QKV for self-attention, uses separate Q/KV for cross-attention.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_q: int,\n",
    "        dim_kv: int,\n",
    "        dim_out: int,\n",
    "        n_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        is_self_attention: bool = False # Explicit flag\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if dim_out % n_heads != 0:\n",
    "            raise ValueError(f\"dim_out ({dim_out}) must be divisible by n_heads ({n_heads})\")\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim_out // n_heads\n",
    "        self.dropout = dropout\n",
    "        self.is_self_attention = is_self_attention\n",
    "\n",
    "        if self.is_self_attention:\n",
    "            if dim_q != dim_kv:\n",
    "                raise ValueError(\"For self-attention, dim_q must be equal to dim_kv.\")\n",
    "            self.w_qkv = nn.Linear(dim_q, 3 * dim_out, bias=bias)\n",
    "        else:\n",
    "            self.w_q = nn.Linear(dim_q, dim_out, bias=bias)\n",
    "            self.w_kv = nn.Linear(dim_kv, 2 * dim_out, bias=bias)\n",
    "\n",
    "        self.w_o = nn.Linear(dim_out, dim_out, bias=bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key_value: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seq_len_q, _ = query.shape\n",
    "        seq_len_kv = key_value.shape[1]\n",
    "\n",
    "        if self.is_self_attention:\n",
    "            # The caller guarantees query is key_value\n",
    "            q, k, v = self.w_qkv(query).chunk(3, dim=-1)\n",
    "        else:\n",
    "            q = self.w_q(query)\n",
    "            k, v = self.w_kv(key_value).chunk(2, dim=-1)\n",
    "\n",
    "        q = q.view(batch_size, seq_len_q, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len_kv, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len_kv, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=attn_mask,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, -1)\n",
    "        return self.w_o(attn_output)\n",
    "\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    \"\"\"Multihead Attention Block with pre-normalization, FFN, and optional checkpointing.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_q: int,\n",
    "        dim_kv: int,\n",
    "        dim: int,\n",
    "        n_heads: int,\n",
    "        ffn_hidden_dim: Optional[int] = None,\n",
    "        dropout: float = 0.0,\n",
    "        use_checkpointing: bool = False,\n",
    "        is_self_attention: bool = False # Add flag here\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "\n",
    "        self.norm_q = RMSNorm(dim_q)\n",
    "        self.norm_kv = RMSNorm(dim_kv) if dim_kv != dim_q else self.norm_q\n",
    "\n",
    "        self.attention = MultiheadAttentionBlock(\n",
    "            dim_q=dim_q, dim_kv=dim_kv, dim_out=dim, n_heads=n_heads,\n",
    "            dropout=dropout, is_self_attention=is_self_attention # Pass flag down\n",
    "        )\n",
    "        self.norm_ffn = RMSNorm(dim)\n",
    "        self.ffn = SwiGLU(dim=dim, hidden_dim=ffn_hidden_dim, dropout=dropout)\n",
    "\n",
    "    def _forward(self, query: torch.Tensor, key_value: torch.Tensor) -> torch.Tensor:\n",
    "        normed_q = self.norm_q(query)\n",
    "        normed_kv = self.norm_kv(key_value)\n",
    "        attn_output = self.attention(normed_q, normed_kv)\n",
    "\n",
    "        if query.shape[-1] == self.dim:\n",
    "            x = query + attn_output\n",
    "        else:\n",
    "            x = attn_output\n",
    "\n",
    "        x = x + self.ffn(self.norm_ffn(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key_value: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training and self.use_checkpointing:\n",
    "            return checkpoint(self._forward, query, key_value, use_reentrant=False)\n",
    "        else:\n",
    "            return self._forward(query, key_value)\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim: int, n_heads: int, ffn_hidden_dim: Optional[int] = None, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.mab = MAB(\n",
    "            dim_q=dim, dim_kv=dim, dim=dim, n_heads=n_heads,\n",
    "            ffn_hidden_dim=ffn_hidden_dim, dropout=dropout,\n",
    "            use_checkpointing=USE_CHECKPOINTING,\n",
    "            is_self_attention=True # This is true self-attention\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mab(x, x)\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_out: int, n_heads: int, n_inducing_points: int, ffn_hidden_dim: Optional[int] = None, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.inducing_points = nn.Parameter(torch.randn(1, n_inducing_points, dim_out))\n",
    "        nn.init.xavier_uniform_(self.inducing_points)\n",
    "\n",
    "        self.mab_cross = MAB(\n",
    "            dim_q=dim_out, dim_kv=dim_in, dim=dim_out, n_heads=n_heads,\n",
    "            ffn_hidden_dim=ffn_hidden_dim, dropout=dropout,\n",
    "            use_checkpointing=USE_CHECKPOINTING, is_self_attention=False # This is cross-attention\n",
    "        )\n",
    "        self.mab_self = MAB(\n",
    "            dim_q=dim_in, dim_kv=dim_out, dim=dim_out, n_heads=n_heads,\n",
    "            ffn_hidden_dim=ffn_hidden_dim, dropout=dropout,\n",
    "            use_checkpointing=USE_CHECKPOINTING, is_self_attention=False # This is cross-attention\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        inducing = self.inducing_points.expand(batch_size, -1, -1)\n",
    "        h = self.mab_cross(inducing, x)\n",
    "        return self.mab_self(x, h)\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim: int, n_heads: int, n_seeds: int, ffn_hidden_dim: Optional[int] = None, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.seed_vectors = nn.Parameter(torch.randn(1, n_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.seed_vectors)\n",
    "        self.mab = MAB(\n",
    "            dim_q=dim, dim_kv=dim, dim=dim, n_heads=n_heads,\n",
    "            ffn_hidden_dim=ffn_hidden_dim, dropout=dropout,\n",
    "            use_checkpointing=USE_CHECKPOINTING, is_self_attention=False # This is cross-attention\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        seeds = self.seed_vectors.expand(batch_size, -1, -1)\n",
    "        return self.mab(seeds, x)\n",
    "\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, output_dim: int, model_dim: int = 256, n_heads: int = 8,\n",
    "        n_isab: int = 2, n_dec_blocks: int = 1, n_inducing_points: Union[int, List[int]] = 32,\n",
    "        n_seeds: int = 1, ffn_hidden_dim: Optional[int] = None, dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(n_inducing_points, int):\n",
    "            n_inducing_points = [n_inducing_points] * n_isab\n",
    "        elif len(n_inducing_points) != n_isab:\n",
    "            raise ValueError(f\"n_inducing_points must be an int or list of length {n_isab}\")\n",
    "\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            ISAB(model_dim, model_dim, n_heads, n_inducing_points[i], ffn_hidden_dim, dropout)\n",
    "            for i in range(n_isab)\n",
    "        ])\n",
    "        self.pooling = PMA(model_dim, n_heads, n_seeds, ffn_hidden_dim, dropout)\n",
    "        self.decoder = nn.ModuleList([\n",
    "            SAB(model_dim, n_heads, ffn_hidden_dim, dropout) for _ in range(n_dec_blocks)\n",
    "        ])\n",
    "        self.output_norm = RMSNorm(model_dim)\n",
    "        self.output_projection = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_projection(x)\n",
    "        for isab in self.encoder:\n",
    "            x = isab(x)\n",
    "        x = self.pooling(x)\n",
    "        for sab in self.decoder:\n",
    "            x = sab(x)\n",
    "        x = self.output_norm(x)\n",
    "        x = self.output_projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Benchmark ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    print(f\"Using device: {device}, AMP dtype: {amp_dtype}\")\n",
    "\n",
    "    # Model and data parameters\n",
    "    batch_size, set_size, input_dim_feat = 128, 512, 32\n",
    "    input_dim = input_dim_feat * 10\n",
    "    output_dim = 10\n",
    "    model_dim = 512\n",
    "    n_heads = 8\n",
    "    n_isab = 6\n",
    "    n_dec_blocks = 2\n",
    "    n_inducing_points = 64\n",
    "    n_seeds = 64 # This determines the output set size\n",
    "\n",
    "    model = SetTransformer(\n",
    "        input_dim=input_dim, output_dim=output_dim, model_dim=model_dim, n_heads=n_heads,\n",
    "        n_isab=n_isab, n_dec_blocks=n_dec_blocks,\n",
    "        n_inducing_points=n_inducing_points, n_seeds=n_seeds, dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    x = torch.randn(batch_size, set_size, input_dim, device=device)\n",
    "    y = torch.randn(batch_size, n_seeds, output_dim, device=device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scaler = torch.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
    "\n",
    "    print(\"\\n--- Starting Benchmark ---\")\n",
    "    print(f\"Model dim: {model_dim}, Heads: {n_heads}, Encoder Blocks: {n_isab}, Decoder Blocks: {n_dec_blocks}\")\n",
    "    print(f\"Batch size: {batch_size}, Input set size: {set_size}, Output set size: {n_seeds}\")\n",
    "    print(f\"Checkpointing: {'Enabled' if USE_CHECKPOINTING else 'Disabled'}\")\n",
    "\n",
    "    # Warm-up iterations\n",
    "    print(\"Running warm-up iterations...\")\n",
    "    for _ in tqdm(range(5)):\n",
    "        with torch.autocast(device_type=device.type, dtype=amp_dtype):\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark loop\n",
    "    num_iterations = 50\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=device.type, dtype=amp_dtype):\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_iter = total_time / num_iterations\n",
    "    throughput = (batch_size * num_iterations) / total_time\n",
    "\n",
    "    print(\"\\n--- Benchmark Results ---\")\n",
    "    print(f\"Total time for {num_iterations} iterations: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per iteration: {avg_time_per_iter * 1000:.2f} ms\")\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, AMP dtype: torch.bfloat16\n",
      "\n",
      "--- Starting NSRTransformer Benchmark ---\n",
      "Model parameters: 113.36M\n",
      "Model dim: 512, Heads: 8, Encoder Blocks: 8, Decoder Layers: 8\n",
      "Batch size: 128, Input set size: 512, Target seq len: 256\n",
      "Checkpointing: Enabled\n",
      "Running warm-up iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.27it/s]\n",
      "100%|██████████| 50/50 [00:38<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark Results ---\n",
      "Total time for 50 iterations: 39.25 seconds\n",
      "Average time per iteration: 784.94 ms\n",
      "Throughput: 163.07 samples/sec\n",
      "\n",
      "--- Inference Benchmark Results ---\n",
      "Total time for 50 iterations: 8.42 seconds\n",
      "Average time per iteration: 168.49 ms\n",
      "Throughput: 759.70 samples/sec\n"
     ]
    }
   ],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"Implements Rotary Positional Embeddings (RoPE).\"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 4096, base: int = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        t = torch.arange(self.max_seq_len, device=self.inv_freq.device)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_seq_len {self.max_seq_len}\")\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor, xk: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Applies rotary embeddings to query and key tensors.\"\"\"\n",
    "    # cos, sin have shape (1, 1, seq_len, head_dim)\n",
    "    # xq, xk have shape (batch, n_heads, seq_len, head_dim)\n",
    "    xq_out = (xq * cos) + (rotate_half(xq) * sin)\n",
    "    xk_out = (xk * cos) + (rotate_half(xk) * sin)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Decoder attention with RoPE and causal masking support.\"\"\"\n",
    "    def __init__(self, dim: int, n_heads: int, dropout: float = 0.0, use_rope: bool = False):\n",
    "        super().__init__()\n",
    "        if dim % n_heads != 0:\n",
    "            raise ValueError(f\"dim ({dim}) must be divisible by n_heads ({n_heads})\")\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        self.w_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.w_k = nn.Linear(dim, dim, bias=False)\n",
    "        self.w_v = nn.Linear(dim, dim, bias=False)\n",
    "        self.w_o = nn.Linear(dim, dim, bias=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key_value: torch.Tensor,\n",
    "        rope_emb: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        is_causal: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seq_len_q, _ = query.shape\n",
    "        seq_len_kv = key_value.shape[1]\n",
    "\n",
    "        q = self.w_q(query)\n",
    "        k = self.w_k(key_value)\n",
    "        v = self.w_v(key_value)\n",
    "\n",
    "        q = q.view(batch_size, seq_len_q, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len_kv, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len_kv, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if self.use_rope:\n",
    "            if rope_emb is None:\n",
    "                raise ValueError(\"rope_emb must be provided when use_rope is True\")\n",
    "            cos, sin = rope_emb\n",
    "            q, k = apply_rotary_emb(q, k, cos, sin)\n",
    "\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            is_causal=is_causal,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, -1)\n",
    "        return self.w_o(attn_output)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"A single block of the Transformer Decoder.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        n_heads: int,\n",
    "        ffn_hidden_dim: Optional[int] = None,\n",
    "        dropout: float = 0.0,\n",
    "        use_checkpointing: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "\n",
    "        self.self_attn_norm = RMSNorm(dim)\n",
    "        self.self_attention = Attention(dim=dim, n_heads=n_heads, dropout=dropout, use_rope=True)\n",
    "\n",
    "        self.cross_attn_norm = RMSNorm(dim)\n",
    "        self.encoder_mem_norm = RMSNorm(dim)\n",
    "        self.cross_attention = Attention(dim=dim, n_heads=n_heads, dropout=dropout, use_rope=False)\n",
    "\n",
    "        self.ffn_norm = RMSNorm(dim)\n",
    "        self.ffn = SwiGLU(dim=dim, hidden_dim=ffn_hidden_dim, dropout=dropout)\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_memory: torch.Tensor,\n",
    "        rope_emb: tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        x = x + self.self_attention(\n",
    "            self.self_attn_norm(x), self.self_attn_norm(x),\n",
    "            rope_emb=rope_emb, is_causal=True\n",
    "        )\n",
    "        x = x + self.cross_attention(\n",
    "            self.cross_attn_norm(x), self.encoder_mem_norm(encoder_memory)\n",
    "        )\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_memory: torch.Tensor,\n",
    "        rope_emb: tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        if self.training and self.use_checkpointing:\n",
    "            # Re-define ckpt_fn to be compatible with checkpoint's argument handling\n",
    "            def ckpt_fn(x, encoder_memory, cos, sin):\n",
    "                return self._forward(x, encoder_memory, (cos, sin))\n",
    "            cos, sin = rope_emb\n",
    "            return checkpoint(ckpt_fn, x, encoder_memory, cos, sin, use_reentrant=False)\n",
    "        else:\n",
    "            return self._forward(x, encoder_memory, rope_emb)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"State-of-the-art Transformer Decoder.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        model_dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        max_seq_len: int = 4096,\n",
    "        ffn_hidden_dim: Optional[int] = None,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.tok_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "        self.rope = RotaryEmbedding(dim=model_dim // n_heads, max_seq_len=max_seq_len)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(\n",
    "                dim=model_dim, n_heads=n_heads, ffn_hidden_dim=ffn_hidden_dim,\n",
    "                dropout=dropout, use_checkpointing=USE_CHECKPOINTING,\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_norm = RMSNorm(model_dim)\n",
    "        self.output_projection = nn.Linear(model_dim, vocab_size, bias=False)\n",
    "        self.output_projection.weight = self.tok_embeddings.weight # Weight tying\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, encoder_memory: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        rope_emb = self.rope(h, seq_len=seq_len)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, encoder_memory, rope_emb)\n",
    "\n",
    "        h = self.output_norm(h)\n",
    "        logits = self.output_projection(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# --- Final Encoder-Decoder Model ---\n",
    "\n",
    "class SetEncoder(nn.Module):\n",
    "    \"\"\"The SetTransformer repurposed as a permutation-invariant encoder.\"\"\"\n",
    "    def __init__(\n",
    "        self, input_dim: int, model_dim: int, n_heads: int, n_isab: int, n_sab: int,\n",
    "        n_inducing_points: Union[int, List[int]], n_seeds: int,\n",
    "        ffn_hidden_dim: Optional[int] = None, dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(n_inducing_points, int):\n",
    "            n_inducing_points = [n_inducing_points] * n_isab\n",
    "        elif len(n_inducing_points) != n_isab:\n",
    "            raise ValueError(f\"n_inducing_points must be an int or list of length {n_isab}\")\n",
    "\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            ISAB(model_dim, model_dim, n_heads, n_inducing_points[i], ffn_hidden_dim, dropout)\n",
    "            for i in range(n_isab)\n",
    "        ])\n",
    "        self.pooling = PMA(model_dim, n_heads, n_seeds, ffn_hidden_dim, dropout)\n",
    "        self.decoder = nn.ModuleList([\n",
    "            SAB(model_dim, n_heads, ffn_hidden_dim, dropout) for _ in range(n_sab)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_projection(x)\n",
    "        for isab in self.encoder:\n",
    "            x = isab(x)\n",
    "        x = self.pooling(x)\n",
    "        for sab in self.decoder:\n",
    "            x = sab(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NSRTransformer(nn.Module):\n",
    "    \"\"\"Neural Set-to-Sequence Responder: SetTransformer Encoder + SOTA Decoder.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Encoder params\n",
    "        input_dim: int,\n",
    "        n_isab: int,\n",
    "        n_sab: int,\n",
    "        n_inducing_points: Union[int, List[int]],\n",
    "        n_seeds: int,\n",
    "        # Decoder params\n",
    "        vocab_size: int,\n",
    "        n_dec_layers: int,\n",
    "        max_seq_len: int,\n",
    "        # Shared params\n",
    "        model_dim: int,\n",
    "        n_heads: int,\n",
    "        ffn_hidden_dim: Optional[int] = None,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = SetEncoder(\n",
    "            input_dim=input_dim, model_dim=model_dim, n_heads=n_heads,\n",
    "            n_isab=n_isab, n_sab=n_sab, n_inducing_points=n_inducing_points,\n",
    "            n_seeds=n_seeds, ffn_hidden_dim=ffn_hidden_dim, dropout=dropout,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            vocab_size=vocab_size, model_dim=model_dim, n_layers=n_dec_layers,\n",
    "            n_heads=n_heads, max_seq_len=max_seq_len,\n",
    "            ffn_hidden_dim=ffn_hidden_dim, dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src_set: torch.Tensor, tgt_seq: torch.Tensor) -> torch.Tensor:\n",
    "        encoder_memory = self.encoder(src_set)\n",
    "        logits = self.decoder(tgt_seq, encoder_memory)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# --- Benchmark ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    print(f\"Using device: {device}, AMP dtype: {amp_dtype}\")\n",
    "\n",
    "    # Model and data parameters\n",
    "    batch_size, set_size, input_dim = 128, 512, 32*10\n",
    "    vocab_size, tgt_seq_len = 1024, 256\n",
    "    model_dim = 512\n",
    "    n_heads = 8\n",
    "    n_isab = 8\n",
    "    n_sab = 8\n",
    "    n_dec_layers = 8\n",
    "    n_inducing_points = 64\n",
    "    n_seeds = 64 # Number of memory vectors from encoder\n",
    "    max_seq_len = 1024\n",
    "\n",
    "    model = NSRTransformer(\n",
    "        input_dim=input_dim, n_isab=n_isab, n_sab=n_sab, n_inducing_points=n_inducing_points,\n",
    "        n_seeds=n_seeds, vocab_size=vocab_size, n_dec_layers=n_dec_layers,\n",
    "        max_seq_len=max_seq_len, model_dim=model_dim, n_heads=n_heads, dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # Dummy data\n",
    "    src_set = torch.randn(batch_size, set_size, input_dim, device=device)\n",
    "    tgt_tokens = torch.randint(0, vocab_size, (batch_size, tgt_seq_len), device=device)\n",
    "    # For loss calculation, targets are shifted\n",
    "    labels = torch.randint(0, vocab_size, (batch_size, tgt_seq_len), device=device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scaler = torch.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\n--- Starting NSRTransformer Benchmark ---\")\n",
    "    print(f\"Model parameters: {num_params / 1e6:.2f}M\")\n",
    "    print(f\"Model dim: {model_dim}, Heads: {n_heads}, Encoder Blocks: {n_isab}, Decoder Layers: {n_dec_layers}\")\n",
    "    print(f\"Batch size: {batch_size}, Input set size: {set_size}, Target seq len: {tgt_seq_len}\")\n",
    "    print(f\"Checkpointing: {'Enabled' if USE_CHECKPOINTING else 'Disabled'}\")\n",
    "\n",
    "    # Warm-up iterations\n",
    "    print(\"Running warm-up iterations...\")\n",
    "    for _ in tqdm(range(5)):\n",
    "        with torch.autocast(device_type=device.type, dtype=amp_dtype):\n",
    "            logits = model(src_set, tgt_tokens)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), labels.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark loop\n",
    "    num_iterations = 50\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=device.type, dtype=amp_dtype):\n",
    "            logits = model(src_set, tgt_tokens)\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_iter = total_time / num_iterations\n",
    "    throughput = (batch_size * num_iterations) / total_time\n",
    "\n",
    "    print(\"\\n--- Benchmark Results ---\")\n",
    "    print(f\"Total time for {num_iterations} iterations: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per iteration: {avg_time_per_iter * 1000:.2f} ms\")\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")\n",
    "\n",
    "\n",
    "    # Test inference speed\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Warm-up\n",
    "        for _ in tqdm(range(5)):\n",
    "            with torch.autocast(device_type=device.type, dtype=amp_dtype):\n",
    "                logits = model(src_set, tgt_tokens)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for _ in tqdm(range(num_iterations)):\n",
    "            with torch.autocast(device_type=device.type, dtype=amp_dtype):\n",
    "                logits = model(src_set, tgt_tokens)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_per_iter = total_time / num_iterations\n",
    "        throughput = (batch_size * num_iterations) / total_time\n",
    "\n",
    "        print(\"\\n--- Inference Benchmark Results ---\")\n",
    "        print(f\"Total time for {num_iterations} iterations: {total_time:.2f} seconds\")\n",
    "        print(f\"Average time per iteration: {avg_time_per_iter * 1000:.2f} ms\")\n",
    "        print(f\"Throughput: {throughput:.2f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bb3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-ansr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
