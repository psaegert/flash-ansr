{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from flash_ansr.models.encoders import SAB, MAB\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(111, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4019, -2.6210, -2.3781])\n",
      "tensor([2.0597, 2.4595, 2.4762])\n",
      "tensor([4.4617, 5.0805, 4.8543])\n"
     ]
    }
   ],
   "source": [
    "mins = X.min(dim=0).values\n",
    "maxs = X.max(dim=0).values\n",
    "ranges = maxs - mins\n",
    "print(mins)\n",
    "print(maxs)\n",
    "print(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mask(X: torch.Tensor) -> torch.Tensor:\n",
    "    mins = X.min(dim=0).values\n",
    "    maxs = X.max(dim=0).values\n",
    "    ranges = maxs - mins\n",
    "    max_range_dim = ranges.argmax()\n",
    "    \n",
    "    split_value = torch.median(X[:, max_range_dim])\n",
    "    # print(f\"Splitting {X.shape=} on dimension {max_range_dim} at value {split_value}\")\n",
    "\n",
    "    return X[:, max_range_dim] < split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True,  True, False, False,  True,  True,  True, False,\n",
       "        False,  True,  True, False, False, False,  True, False, False, False,\n",
       "        False,  True,  True,  True, False,  True, False, False, False,  True,\n",
       "        False, False,  True, False, False, False, False, False, False,  True,\n",
       "         True,  True,  True, False, False,  True,  True, False, False, False,\n",
       "         True,  True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         True, False,  True,  True, False, False, False,  True,  True,  True,\n",
       "        False,  True, False, False, False,  True, False,  True,  True, False,\n",
       "        False, False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        False, False, False,  True,  True, False,  True, False, False,  True,\n",
       "        False, False,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_mask(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(128, 111, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mask_batched(X: torch.Tensor) -> torch.Tensor:\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D tensor, got {X.ndim}D tensor of shape {X.shape}\")\n",
    "\n",
    "    mins = X.min(dim=1).values\n",
    "    maxs = X.max(dim=1).values\n",
    "    ranges = maxs - mins\n",
    "    max_range_dim = ranges.argmax(dim=1)\n",
    "\n",
    "    # print(max_range_dim.shape)    \n",
    "    split_value = torch.median(X[np.arange(X.shape[0]), :, max_range_dim], dim=1).values\n",
    "    # print(split_value.shape)\n",
    "\n",
    "    # print(X[np.arange(X.shape[0]), :, max_range_dim].shape)\n",
    "    # print(split_value.shape)\n",
    "\n",
    "    return X[np.arange(X.shape[0]), :, max_range_dim] < split_value.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 111])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = split_mask_batched(X)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7040, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.55 ms ± 23.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "X = torch.randn(128, 512, 3)\n",
    "mask = split_mask_batched(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 μs ± 52.8 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "X = torch.randn(128, 512, 3, device=device)\n",
    "mask = split_mask_batched(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, X: torch.Tensor, left=None, right=None):\n",
    "        self.X = X\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tree(X: torch.Tensor, max_depth: int = 5, min_n_leaf: int | None = None, depth: int = 0) -> torch.Tensor:\n",
    "#     if min_n_leaf is None:\n",
    "#         min_n_leaf = np.sqrt(X.shape[0])\n",
    "\n",
    "#     if depth + 1 >= max_depth or X.shape[0] <= min_n_leaf:\n",
    "#         return Node(X)\n",
    "\n",
    "#     mask = split_mask(X)\n",
    "\n",
    "#     return Node(X, create_tree(X[mask], max_depth, min_n_leaf, depth + 1), create_tree(X[~mask], max_depth, min_n_leaf, depth + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(X: torch.Tensor, max_depth: int = 5, depth: int = 0) -> torch.Tensor:\n",
    "    if depth + 1 >= max_depth:\n",
    "        return Node(X)\n",
    "\n",
    "    if X.shape[0] > 1:\n",
    "        mask = split_mask(X)\n",
    "        return Node(X, create_tree(X[mask], max_depth, depth + 1), create_tree(X[~mask], max_depth, depth + 1))\n",
    "    \n",
    "    return Node(X, create_tree(X, max_depth, depth + 1), create_tree(X, max_depth, depth + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(512, 3)\n",
    "tree = create_tree(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(4, 3)\n",
    "tree = create_tree(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# X = torch.randn(512, 3)\n",
    "# tree = create_tree(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can make this batched!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree_batched(X: torch.Tensor, max_depth: int = 5, depth: int = 0) -> torch.Tensor:\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D tensor, got {X.ndim}D tensor of shape {X.shape}\")\n",
    "\n",
    "    if depth + 1 >= max_depth:\n",
    "        return Node(X)\n",
    "\n",
    "    if X.shape[1] > 1:\n",
    "        mask_batched = split_mask_batched(X)\n",
    "        X_left = torch.stack([X[i][mask_batched[i]] for i in range(X.shape[0])], dim=0)\n",
    "        X_right = torch.stack([X[i][~mask_batched[i]] for i in range(X.shape[0])], dim=0)    \n",
    "        # print(X_left.shape)\n",
    "        # print(X_right.shape)    \n",
    "        return Node(X, create_tree_batched(X_left, max_depth, depth + 1), create_tree_batched(X_right, max_depth, depth + 1))\n",
    "    \n",
    "    return Node(X, create_tree_batched(X, max_depth, depth + 1), create_tree_batched(X, max_depth, depth + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(128, 111, 3)\n",
    "tree_batched = create_tree_batched(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.6 ms ± 925 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "X = torch.randn(128, 512, 3)\n",
    "tree_batched = create_tree_batched(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 ms ± 48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "X = torch.randn(128, 512, 3, device=device)\n",
    "tree_batched = create_tree_batched(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 13, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_batched.left.left.left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeSetEncoder(nn.Module):\n",
    "    def __init__(self, depth: int, features_in: int, features_hidden: int = 512, n_heads: int = 8, clean_path: bool = True):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.features_in = features_in\n",
    "        self.features_hidden = features_hidden\n",
    "\n",
    "        self.linear_in = nn.Linear(features_in, features_hidden)\n",
    "\n",
    "        self.sabs = nn.ModuleList([SAB(features_hidden, n_heads=n_heads, clean_path=clean_path) for _ in range(depth)])\n",
    "        self.mabs_lr = nn.ModuleList([MAB(features_hidden,n_heads=n_heads, clean_path=clean_path) for _ in range(depth)])\n",
    "        self.mabs_rl = nn.ModuleList([MAB(features_hidden, n_heads=n_heads, clean_path=clean_path) for _ in range(depth)])\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if X.ndim > 3:\n",
    "            raise ValueError(f\"Expected 2D or 3D tensor, got {X.ndim}D tensor of shape {X.shape}\")\n",
    "        elif X.ndim == 2:\n",
    "            X = X.unsqueeze(0)\n",
    "        elif X.ndim != 3:\n",
    "            raise ValueError(f\"Expected 2D or 3D tensor, got {X.ndim}D tensor of shape {X.shape}\")\n",
    "\n",
    "        tree_batched = create_tree_batched(X.cpu(), max_depth=self.depth)\n",
    "        return self.encode_tree(tree_batched)\n",
    "\n",
    "    def encode_tree(self, tree: Node, depth: int = 0) -> list[tuple[int, int]]:\n",
    "        if tree.left is None and tree.right is None:\n",
    "            # Leaf node\n",
    "            return self.linear_in(tree.X.to(device))\n",
    "\n",
    "        X_left = self.encode_tree(tree.left, depth + 1)\n",
    "        X_right = self.encode_tree(tree.right, depth + 1)\n",
    "\n",
    "        X_left = self.sabs[depth](X_left)\n",
    "        X_right = self.sabs[depth](X_right)\n",
    "\n",
    "        X_lr = self.mabs_lr[depth](X_left, X_right)\n",
    "\n",
    "        X_lr_rl = self.mabs_rl[depth](X_right, X_lr)\n",
    "\n",
    "        return X_lr_rl\n",
    "    \n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,761,408 params\n"
     ]
    }
   ],
   "source": [
    "encoder = TreeSetEncoder(depth=5, features_in=3).to(device)\n",
    "print(f'{encoder.n_params:,} params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.8 ms ± 3.94 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "X = torch.randn(128, 512, 3, device=device)\n",
    "z = encoder.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.3 ms ± 1.69 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "X = torch.randn(128, 512, 3, device=device)\n",
    "with torch.no_grad():\n",
    "    z = encoder.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 33, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(128, 512, 3, device=device)\n",
    "z = encoder.forward(X.to(device))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(128, 128, 3, device=device)\n",
    "z = encoder.forward(X.to(device))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(128, 10, 3, device=device)\n",
    "z = encoder.forward(X.to(device))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(128, 1, 3, device=device)\n",
    "z = encoder.forward(X.to(device))\n",
    "z.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-ansr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
