{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f2f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        x = x / rms\n",
    "        return self.weight * x\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Rotary Positional Embeddings (RoPE)\"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 4096, base: int = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self._build_cache()\n",
    "\n",
    "    def _build_cache(self):\n",
    "        seq_idx = torch.arange(self.max_seq_len, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(seq_idx, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, :, None, :])\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, :, None, :])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> torch.Tensor:\n",
    "        return self.apply_rotary_emb(x, self.cos_cached[:, :seq_len], \n",
    "                                     self.sin_cached[:, :seq_len])\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_rotary_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = x[..., : x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "        rotated = torch.cat(\n",
    "            (-x2 * sin[..., :x2.shape[-1]] + x1 * cos[..., :x1.shape[-1]],\n",
    "             x2 * cos[..., :x2.shape[-1]] + x1 * sin[..., :x1.shape[-1]]),\n",
    "            dim=-1\n",
    "        )\n",
    "        return rotated\n",
    "\n",
    "\n",
    "class MultiheadAttentionWithRoPE(nn.Module):\n",
    "    \"\"\"Pure Multi-head Attention mechanism with RoPE\n",
    "\n",
    "    This module only handles the attention computation.\n",
    "    Normalization, dropout, and residual connections are handled externally.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        max_seq_len: int = 4096,\n",
    "        attn_dropout: float = 0.1,\n",
    "        rope_base: int = 10000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Rotary embeddings\n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim, max_seq_len, rope_base)\n",
    "\n",
    "        # Attention dropout (applied to attention weights)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        causal_mask: bool = False,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass - returns raw attention output without residual\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch, seq_len, d_model]\n",
    "            causal_mask: Whether to apply causal masking\n",
    "            attention_mask: Optional custom attention mask\n",
    "\n",
    "        Returns:\n",
    "            Attention output of shape [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Compute Q, K, V projections\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        # Apply rotary positional embeddings to Q and K\n",
    "        q = self.rope(q, seq_len)\n",
    "        k = self.rope(k, seq_len)\n",
    "\n",
    "        # Transpose for attention computation\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply causal mask if requested\n",
    "        if causal_mask:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Apply custom attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Reshape back to [batch, seq_len, d_model]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # Apply output projection\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Feed-Forward Network with configurable activation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: Optional[int] = None,\n",
    "        activation: str = \"swiglu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "\n",
    "        self.activation_type = activation\n",
    "\n",
    "        if activation == \"swiglu\":\n",
    "            self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "            self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "            self.w3 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        else:\n",
    "            self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "            self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.activation_type == \"swiglu\":\n",
    "            return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "        elif self.activation_type == \"gelu\":\n",
    "            return self.w2(F.gelu(self.w1(x)))\n",
    "        else:\n",
    "            return self.w2(F.relu(self.w1(x)))\n",
    "        \n",
    "class CausalInductionSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Causal ISAB that properly maintains causality\n",
    "\n",
    "    This version ensures inducing points only attend to past positions\n",
    "    while maintaining computational efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, n_inducing_points: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_inducing_points = n_inducing_points\n",
    "\n",
    "        # Learnable inducing points\n",
    "        self.inducing_points = nn.Parameter(torch.randn(n_inducing_points, d_model))\n",
    "\n",
    "        # Attention layers\n",
    "        self.compress_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.decompress_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Layer norms\n",
    "        self.compress_norm = RMSNorm(d_model)\n",
    "        self.decompress_norm = RMSNorm(d_model) \n",
    "\n",
    "        # Feedforward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _create_causal_inducing_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Create mask ensuring inducing points only see past positions\n",
    "\n",
    "        For each sequence position i, inducing points can only attend to positions 0...i\n",
    "        This creates a block-diagonal pattern in the attention.\n",
    "\n",
    "        Returns:\n",
    "            mask: [seq_len * n_inducing_points, seq_len] attention mask\n",
    "        \"\"\"\n",
    "        n_ind = self.n_inducing_points\n",
    "\n",
    "        # Create a mask where inducing point j at position i can see positions 0...i\n",
    "        mask = torch.zeros(seq_len * n_ind, seq_len, device=device)\n",
    "\n",
    "        for pos in range(seq_len):\n",
    "            # For position pos, all inducing points can see positions 0...pos\n",
    "            start_idx = pos * n_ind\n",
    "            end_idx = (pos + 1) * n_ind\n",
    "            mask[start_idx:end_idx, :pos+1] = 1\n",
    "\n",
    "        # Convert to attention mask format (0 = attend, -inf = don't attend)\n",
    "        mask = (1 - mask) * float('-inf')\n",
    "        mask[mask != mask] = 0  # Replace NaN with 0\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, causal_mask: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with causal compression\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, d_model]\n",
    "            causal_mask: Whether to apply causal masking\n",
    "\n",
    "        Returns:\n",
    "            Output tensor [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Step 1: Causal compression\n",
    "        x_norm = self.compress_norm(x)\n",
    "\n",
    "        if causal_mask:\n",
    "            # Create position-specific inducing points for causality\n",
    "            # Each position gets its own set of inducing points that can only see past\n",
    "            inducing = self.inducing_points.unsqueeze(0).expand(seq_len, -1, -1)  # [seq_len, n_ind, d_model]\n",
    "            inducing = inducing.reshape(seq_len * self.n_inducing_points, self.d_model)  # [seq_len * n_ind, d_model]\n",
    "            inducing = inducing.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, seq_len * n_ind, d_model]\n",
    "\n",
    "            # Create causal mask for compression\n",
    "            compress_mask = self._create_causal_inducing_mask(seq_len, device)\n",
    "\n",
    "            # Compress with causal attention\n",
    "            compressed, _ = self.compress_attn(\n",
    "                query=inducing,\n",
    "                key=x_norm,\n",
    "                value=x_norm,\n",
    "                attn_mask=compress_mask,\n",
    "                need_weights=False\n",
    "            )\n",
    "\n",
    "            # Reshape back to [batch, seq_len, n_inducing_points, d_model]\n",
    "            compressed = compressed.view(batch_size, seq_len, self.n_inducing_points, self.d_model)\n",
    "\n",
    "            # Step 2: Causal decompression\n",
    "            # For each position, attend only to inducing points from that position\n",
    "            output = []\n",
    "            for i in range(seq_len):\n",
    "                # Get inducing points for position i\n",
    "                ind_i = compressed[:, i, :, :]  # [batch, n_inducing_points, d_model]\n",
    "\n",
    "                # Decompress to position i\n",
    "                x_norm_i = self.decompress_norm(x[:, i:i+1, :])  # [batch, 1, d_model]\n",
    "                out_i, _ = self.decompress_attn(\n",
    "                    query=x_norm_i,\n",
    "                    key=ind_i,\n",
    "                    value=ind_i,\n",
    "                    need_weights=False\n",
    "                )\n",
    "                output.append(out_i)\n",
    "\n",
    "            output = torch.cat(output, dim=1)  # [batch, seq_len, d_model]\n",
    "\n",
    "        else:\n",
    "            # Non-causal path: standard ISAB\n",
    "            inducing = self.inducing_points.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "            # Compress\n",
    "            compressed, _ = self.compress_attn(\n",
    "                query=inducing,\n",
    "                key=x_norm,\n",
    "                value=x_norm,\n",
    "                need_weights=False\n",
    "            )\n",
    "\n",
    "            # Decompress\n",
    "            x_norm = self.decompress_norm(x)\n",
    "            output, _ = self.decompress_attn(\n",
    "                query=x_norm,\n",
    "                key=compressed,\n",
    "                value=compressed,\n",
    "                need_weights=False\n",
    "            )\n",
    "\n",
    "        # Add residual\n",
    "        x = x + self.dropout(output)\n",
    "\n",
    "        # FFN with residual\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MemoryEfficientCausalISAB(nn.Module):\n",
    "    \"\"\"Memory-efficient Causal ISAB using chunked processing\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, n_inducing_points: int, \n",
    "                 dropout: float = 0.1, chunk_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_inducing_points = n_inducing_points\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        # Single set of inducing points (not position-specific)\n",
    "        self.inducing_points = nn.Parameter(torch.randn(n_inducing_points, d_model))\n",
    "\n",
    "        # Attention layers with memory-efficient settings\n",
    "        self.compress_attn = nn.MultiheadAttention(\n",
    "            d_model, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.decompress_attn = nn.MultiheadAttention(\n",
    "            d_model, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Norms and FFN\n",
    "        self.compress_norm = RMSNorm(d_model)\n",
    "        self.decompress_norm = RMSNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, causal_mask: bool = True) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        if causal_mask:\n",
    "            # Process in chunks to save memory\n",
    "            output = torch.zeros_like(x)\n",
    "\n",
    "            for start_idx in range(0, seq_len, self.chunk_size):\n",
    "                end_idx = min(start_idx + self.chunk_size, seq_len)\n",
    "                chunk_len = end_idx - start_idx\n",
    "\n",
    "                # Process chunk with causal attention to inducing points\n",
    "                x_chunk = x[:, :end_idx]  # Can see all previous positions\n",
    "                x_norm_chunk = self.compress_norm(x_chunk)\n",
    "\n",
    "                # Inducing points for this chunk\n",
    "                inducing = self.inducing_points.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "                # Compress: inducing points attend to all positions up to end_idx\n",
    "                compressed, _ = self.compress_attn(\n",
    "                    query=inducing,\n",
    "                    key=x_norm_chunk,\n",
    "                    value=x_norm_chunk,\n",
    "                    need_weights=False\n",
    "                )\n",
    "\n",
    "                # Decompress only for current chunk positions\n",
    "                x_norm_current = self.decompress_norm(x[:, start_idx:end_idx])\n",
    "                decompressed, _ = self.decompress_attn(\n",
    "                    query=x_norm_current,\n",
    "                    key=compressed,\n",
    "                    value=compressed,\n",
    "                    need_weights=False\n",
    "                )\n",
    "\n",
    "                output[:, start_idx:end_idx] = decompressed\n",
    "\n",
    "            # Residual connection\n",
    "            x = x + self.dropout(output)\n",
    "        else:\n",
    "            # Non-causal: standard ISAB (already efficient)\n",
    "            inducing = self.inducing_points.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            x_norm = self.compress_norm(x)\n",
    "            compressed, _ = self.compress_attn(\n",
    "                query=inducing, key=x_norm, value=x_norm, need_weights=False\n",
    "            )\n",
    "            x_norm = self.decompress_norm(x)\n",
    "            output, _ = self.decompress_attn(\n",
    "                query=x_norm, key=compressed, value=compressed, need_weights=False\n",
    "            )\n",
    "            x = x + self.dropout(output)\n",
    "\n",
    "        # FFN with residual\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block with unified pre-norm pattern\n",
    "\n",
    "    Handles all normalization, dropout, and residual connections\n",
    "    for both attention and FFN sub-layers uniformly.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: Optional[int] = None,\n",
    "        max_seq_len: int = 4096,\n",
    "        dropout: float = 0.1,\n",
    "        rope_base: int = 10000,\n",
    "        activation: str = \"swiglu\",\n",
    "        eps: float = 1e-6\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sub-layer components\n",
    "        self.attention = MultiheadAttentionWithRoPE(\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            max_seq_len=max_seq_len,\n",
    "            attn_dropout=dropout,  # Internal attention dropout\n",
    "            rope_base=rope_base\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            activation=activation\n",
    "        )\n",
    "\n",
    "        # Layer normalizations (pre-norm configuration)\n",
    "        self.attn_norm = RMSNorm(d_model, eps=eps)\n",
    "        self.ffn_norm = RMSNorm(d_model, eps=eps)\n",
    "\n",
    "        # Dropout for residual connections\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        causal_mask: bool = False,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with consistent pre-norm pattern\n",
    "\n",
    "        Pattern for each sub-layer:\n",
    "        1. Store residual\n",
    "        2. Apply normalization\n",
    "        3. Apply sub-layer (attention or FFN)\n",
    "        4. Apply dropout\n",
    "        5. Add residual\n",
    "        \"\"\"\n",
    "        # Attention sub-layer with pre-norm\n",
    "        residual = x\n",
    "        x_norm = self.attn_norm(x)\n",
    "        attn_output = self.attention(x_norm, causal_mask=causal_mask, \n",
    "                                     attention_mask=attention_mask)\n",
    "        x = residual + self.dropout(attn_output)\n",
    "\n",
    "        # FFN sub-layer with pre-norm\n",
    "        residual = x\n",
    "        x_norm = self.ffn_norm(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = residual + self.dropout(ffn_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Complete Transformer Encoder with multiple layers\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: Optional[int] = None,\n",
    "        max_seq_len: int = 4096,\n",
    "        dropout: float = 0.1,\n",
    "        rope_base: int = 10000,\n",
    "        activation: str = \"swiglu\",\n",
    "        eps: float = 1e-6\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                d_ff=d_ff,\n",
    "                max_seq_len=max_seq_len,\n",
    "                dropout=dropout,\n",
    "                rope_base=rope_base,\n",
    "                activation=activation,\n",
    "                eps=eps\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.final_norm = RMSNorm(d_model, eps=eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        causal_mask: bool = False,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_hidden_states: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = [] if return_hidden_states else None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask=causal_mask, attention_mask=attention_mask)\n",
    "            if return_hidden_states:\n",
    "                hidden_states.append(x)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if return_hidden_states:\n",
    "            return x, hidden_states\n",
    "        return x\n",
    "    \n",
    "class CausalInductionTransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block using Causal Induction Self-Attention\n",
    "\n",
    "    This block replaces standard self-attention with the Causal Induction\n",
    "    Self-Attention mechanism for improved efficiency on long sequences.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        n_inducing_points: int,\n",
    "        d_ff: Optional[int] = None,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"swiglu\",\n",
    "        eps: float = 1e-6,\n",
    "        use_external_ffn: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Causal Induction Self-Attention Block (already includes FFN)\n",
    "        self.cisab = CausalInductionSelfAttentionBlock(\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            n_inducing_points=n_inducing_points,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Optional additional FFN layer for deeper processing\n",
    "        self.use_external_ffn = use_external_ffn\n",
    "        if use_external_ffn:\n",
    "            self.ffn = FeedForwardNetwork(\n",
    "                d_model=d_model,\n",
    "                d_ff=d_ff,\n",
    "                activation=activation\n",
    "            )\n",
    "            self.ffn_norm = RMSNorm(d_model, eps=eps)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        causal_mask: bool = True,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the CISAB block\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, d_model]\n",
    "            causal_mask: Whether to apply causal masking\n",
    "            attention_mask: Optional attention mask (not used in CISAB)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Apply Causal Induction Self-Attention\n",
    "        x = self.cisab(x, causal_mask=causal_mask)\n",
    "\n",
    "        # Optional additional FFN\n",
    "        if self.use_external_ffn:\n",
    "            residual = x\n",
    "            x_norm = self.ffn_norm(x)\n",
    "            ffn_output = self.ffn(x_norm)\n",
    "            x = residual + self.dropout(ffn_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalInductionTransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer Encoder using Causal Induction Self-Attention Blocks\n",
    "\n",
    "    This encoder replaces standard self-attention with causal induction\n",
    "    attention for improved efficiency, especially on long sequences.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        n_inducing_points: int,\n",
    "        d_ff: Optional[int] = None,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"swiglu\",\n",
    "        eps: float = 1e-6,\n",
    "        use_external_ffn: bool = True,\n",
    "        varying_inducing_points: Optional[list] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the Causal Induction Transformer Encoder\n",
    "\n",
    "        Args:\n",
    "            n_layers: Number of encoder layers\n",
    "            d_model: Dimension of the model\n",
    "            n_heads: Number of attention heads\n",
    "            n_inducing_points: Number of inducing points for compression\n",
    "            d_ff: Dimension of feedforward network (default: 4 * d_model)\n",
    "            dropout: Dropout rate\n",
    "            activation: Activation function for FFN\n",
    "            eps: Epsilon for layer normalization\n",
    "            use_external_ffn: Whether to use additional FFN after CISAB\n",
    "            varying_inducing_points: Optional list of inducing points per layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Allow varying number of inducing points per layer\n",
    "        if varying_inducing_points is not None:\n",
    "            assert len(varying_inducing_points) == n_layers, \\\n",
    "                \"Length of varying_inducing_points must match n_layers\"\n",
    "            inducing_points_per_layer = varying_inducing_points\n",
    "        else:\n",
    "            inducing_points_per_layer = [n_inducing_points] * n_layers\n",
    "\n",
    "        # Create encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            CausalInductionTransformerEncoderBlock(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                n_inducing_points=inducing_points_per_layer[i],\n",
    "                d_ff=d_ff,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "                eps=eps,\n",
    "                use_external_ffn=use_external_ffn\n",
    "            )\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.final_norm = RMSNorm(d_model, eps=eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        causal_mask: bool = True,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_hidden_states: bool = False\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, list]]:\n",
    "        \"\"\"Forward pass through the encoder\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, d_model]\n",
    "            causal_mask: Whether to apply causal masking\n",
    "            attention_mask: Optional attention mask (passed but not used by CISAB)\n",
    "            return_hidden_states: Whether to return intermediate hidden states\n",
    "\n",
    "        Returns:\n",
    "            Output tensor [batch, seq_len, d_model]\n",
    "            Optionally: (output, hidden_states) if return_hidden_states=True\n",
    "        \"\"\"\n",
    "        hidden_states = [] if return_hidden_states else None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask=causal_mask, attention_mask=attention_mask)\n",
    "            if return_hidden_states:\n",
    "                hidden_states.append(x)\n",
    "\n",
    "        # Apply final normalization\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if return_hidden_states:\n",
    "            return x, hidden_states\n",
    "        return x\n",
    "\n",
    "\n",
    "class HybridTransformerEncoder(nn.Module):\n",
    "    \"\"\"Hybrid Transformer that can mix standard and CISAB blocks\n",
    "\n",
    "    This allows combining standard attention for early layers (better for\n",
    "    local patterns) with CISAB for later layers (better for long-range).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        n_inducing_points: int,\n",
    "        d_ff: Optional[int] = None,\n",
    "        max_seq_len: int = 4096,\n",
    "        dropout: float = 0.1,\n",
    "        rope_base: int = 10000,\n",
    "        activation: str = \"swiglu\",\n",
    "        eps: float = 1e-6,\n",
    "        cisab_layers: Optional[list] = None,\n",
    "        cisab_start_layer: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"Initialize Hybrid Transformer Encoder\n",
    "\n",
    "        Args:\n",
    "            cisab_layers: List of layer indices to use CISAB (e.g., [2, 3, 5])\n",
    "            cisab_start_layer: Alternative - use CISAB from this layer onwards\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Determine which layers use CISAB\n",
    "        if cisab_layers is not None:\n",
    "            use_cisab = [i in cisab_layers for i in range(n_layers)]\n",
    "        elif cisab_start_layer is not None:\n",
    "            use_cisab = [i >= cisab_start_layer for i in range(n_layers)]\n",
    "        else:\n",
    "            # Default: use CISAB for second half of layers\n",
    "            use_cisab = [i >= n_layers // 2 for i in range(n_layers)]\n",
    "\n",
    "        # Create mixed layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            if use_cisab[i]:\n",
    "                layer = CausalInductionTransformerEncoderBlock(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    n_inducing_points=n_inducing_points,\n",
    "                    d_ff=d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                    eps=eps,\n",
    "                    use_external_ffn=True\n",
    "                )\n",
    "            else:\n",
    "                layer = TransformerEncoderBlock(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    d_ff=d_ff,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    dropout=dropout,\n",
    "                    rope_base=rope_base,\n",
    "                    activation=activation,\n",
    "                    eps=eps\n",
    "                )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.final_norm = RMSNorm(d_model, eps=eps)\n",
    "\n",
    "        # Store configuration for inspection\n",
    "        self.use_cisab = use_cisab\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        causal_mask: bool = False,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_hidden_states: bool = False\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, list]]:\n",
    "        \"\"\"Forward pass through hybrid encoder\"\"\"\n",
    "        hidden_states = [] if return_hidden_states else None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask=causal_mask, attention_mask=attention_mask)\n",
    "            if return_hidden_states:\n",
    "                hidden_states.append(x)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if return_hidden_states:\n",
    "            return x, hidden_states\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4b30ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Standard Transformer Causality...\n",
      "✓ Standard Transformer causality tests passed\n",
      "\n",
      "Testing CISAB Causality...\n",
      "✓ CISAB causality tests passed\n",
      "\n",
      "Testing Causal Induction Transformer...\n",
      "✓ Causal Induction Transformer causality tests passed\n",
      "\n",
      "Testing Hybrid Transformer...\n",
      "✓ Hybrid Transformer causality tests passed\n",
      "\n",
      "✅ All causality tests passed successfully!\n",
      "\n",
      "Running additional functionality tests...\n",
      "✅ All tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import warnings\n",
    "\n",
    "# Assuming the transformer code is in a module called 'transformers'\n",
    "# from transformers import (\n",
    "#     RMSNorm, RotaryPositionalEmbedding, MultiheadAttentionWithRoPE,\n",
    "#     FeedForwardNetwork, CausalInductionSelfAttentionBlock,\n",
    "#     TransformerEncoderBlock, TransformerEncoder,\n",
    "#     CausalInductionTransformerEncoderBlock, CausalInductionTransformerEncoder,\n",
    "#     HybridTransformerEncoder\n",
    "# )\n",
    "\n",
    "class TestRMSNorm:\n",
    "    \"\"\"Test Root Mean Square Layer Normalization\"\"\"\n",
    "\n",
    "    def test_output_shape(self):\n",
    "        \"\"\"Test that RMSNorm preserves input shape\"\"\"\n",
    "        norm = RMSNorm(dim=128)\n",
    "        x = torch.randn(2, 10, 128)\n",
    "        output = norm(x)\n",
    "        assert output.shape == x.shape\n",
    "\n",
    "    def test_normalization_effect(self):\n",
    "        \"\"\"Test that RMSNorm actually normalizes\"\"\"\n",
    "        norm = RMSNorm(dim=64)\n",
    "        x = torch.randn(4, 8, 64) * 10  # Large variance input\n",
    "        output = norm(x)\n",
    "\n",
    "        # Check that RMS is approximately 1 after normalization\n",
    "        rms = torch.sqrt(torch.mean(output ** 2, dim=-1))\n",
    "        assert torch.allclose(rms, torch.ones_like(rms), atol=0.1)\n",
    "\n",
    "    def test_gradient_flow(self):\n",
    "        \"\"\"Test gradient flow through RMSNorm\"\"\"\n",
    "        norm = RMSNorm(dim=32)\n",
    "        x = torch.randn(2, 5, 32, requires_grad=True)\n",
    "        output = norm(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        assert x.grad is not None\n",
    "        assert not torch.isnan(x.grad).any()\n",
    "\n",
    "\n",
    "class TestRotaryPositionalEmbedding:\n",
    "    \"\"\"Test Rotary Positional Embeddings\"\"\"\n",
    "\n",
    "    def test_output_shape(self):\n",
    "        \"\"\"Test RoPE preserves input shape\"\"\"\n",
    "        rope = RotaryPositionalEmbedding(dim=64, max_seq_len=100)\n",
    "        x = torch.randn(2, 10, 8, 64)  # [batch, seq, heads, dim]\n",
    "        output = rope(x, seq_len=10)\n",
    "        assert output.shape == x.shape\n",
    "\n",
    "    def test_position_dependency(self):\n",
    "        \"\"\"Test that different positions get different embeddings\"\"\"\n",
    "        rope = RotaryPositionalEmbedding(dim=64, max_seq_len=100)\n",
    "        x = torch.ones(1, 3, 1, 64)  # Same input at different positions\n",
    "        output = rope(x, seq_len=3)\n",
    "\n",
    "        # Different positions should have different outputs\n",
    "        assert not torch.allclose(output[0, 0], output[0, 1])\n",
    "        assert not torch.allclose(output[0, 1], output[0, 2])\n",
    "\n",
    "    def test_deterministic(self):\n",
    "        \"\"\"Test that RoPE is deterministic for same position\"\"\"\n",
    "        rope = RotaryPositionalEmbedding(dim=64, max_seq_len=100)\n",
    "        x = torch.randn(2, 10, 4, 64)\n",
    "        output1 = rope(x, seq_len=10)\n",
    "        output2 = rope(x, seq_len=10)\n",
    "        assert torch.allclose(output1, output2)\n",
    "\n",
    "\n",
    "class TestCausality:\n",
    "    \"\"\"Extensive tests for causal masking in attention mechanisms\"\"\"\n",
    "\n",
    "    def test_standard_attention_causality(self):\n",
    "        \"\"\"Test that causal mask prevents attending to future positions\"\"\"\n",
    "        attn = MultiheadAttentionWithRoPE(\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            max_seq_len=100\n",
    "        )\n",
    "        attn.eval()  # Disable dropout for deterministic testing\n",
    "\n",
    "        batch_size = 2\n",
    "        seq_len = 10\n",
    "        d_model = 64\n",
    "\n",
    "        # Create input where each position has a unique pattern\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "        # Test with causal mask\n",
    "        with torch.no_grad():\n",
    "            output_causal = attn(x, causal_mask=True)\n",
    "\n",
    "        # Perturb future positions and check if past positions remain unchanged\n",
    "        for pos in range(seq_len - 1):\n",
    "            x_perturbed = x.clone()\n",
    "            # Significantly perturb all positions after 'pos'\n",
    "            x_perturbed[:, pos+1:, :] = x_perturbed[:, pos+1:, :] + 100.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_perturbed = attn(x_perturbed, causal_mask=True)\n",
    "\n",
    "            # Output at positions up to 'pos' should remain unchanged\n",
    "            assert torch.allclose(\n",
    "                output_causal[:, :pos+1, :],\n",
    "                output_perturbed[:, :pos+1, :],\n",
    "                atol=1e-5\n",
    "            ), f\"Causality violated at position {pos}\"\n",
    "\n",
    "    def test_attention_mask_shape(self):\n",
    "        \"\"\"Test that attention scores have correct causal structure\"\"\"\n",
    "        d_model = 32\n",
    "        n_heads = 2\n",
    "        seq_len = 8\n",
    "\n",
    "        # Create a custom attention module that exposes attention weights\n",
    "        class AttentionWithWeights(MultiheadAttentionWithRoPE):\n",
    "            def forward(self, x, causal_mask=False, attention_mask=None):\n",
    "                batch_size, seq_len, _ = x.shape\n",
    "\n",
    "                # Compute Q, K, V\n",
    "                q = self.q_proj(x)\n",
    "                k = self.k_proj(x)\n",
    "                v = self.v_proj(x)\n",
    "\n",
    "                # Reshape for multi-head\n",
    "                q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "                k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "                v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "                # Apply RoPE\n",
    "                q = self.rope(q, seq_len)\n",
    "                k = self.rope(k, seq_len)\n",
    "\n",
    "                # Transpose\n",
    "                q = q.transpose(1, 2)\n",
    "                k = k.transpose(1, 2)\n",
    "                v = v.transpose(1, 2)\n",
    "\n",
    "                # Compute scores\n",
    "                scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "                if causal_mask:\n",
    "                    mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "                    scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "                # Return both output and attention scores for testing\n",
    "                attn_weights = torch.softmax(scores, dim=-1)\n",
    "                return attn_weights\n",
    "\n",
    "        attn = AttentionWithWeights(d_model=d_model, n_heads=n_heads)\n",
    "        x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            attn_weights = attn(x, causal_mask=True)\n",
    "\n",
    "        # Check that attention weights are zero for future positions\n",
    "        for head in range(n_heads):\n",
    "            for i in range(seq_len):\n",
    "                for j in range(i + 1, seq_len):\n",
    "                    assert attn_weights[0, head, i, j] < 1e-6, \\\n",
    "                        f\"Non-zero attention from position {i} to future position {j}\"\n",
    "\n",
    "    def test_transformer_encoder_causality(self):\n",
    "        \"\"\"Test causality in full transformer encoder\"\"\"\n",
    "        encoder = TransformerEncoder(\n",
    "            n_layers=2,\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            dropout=0.0  # No dropout for deterministic testing\n",
    "        )\n",
    "        encoder.eval()\n",
    "\n",
    "        batch_size = 2\n",
    "        seq_len = 12\n",
    "        x = torch.randn(batch_size, seq_len, 64)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_causal = encoder(x, causal_mask=True)\n",
    "\n",
    "        # Test that modifying future doesn't affect past\n",
    "        for pos in range(seq_len - 1):\n",
    "            x_modified = x.clone()\n",
    "            x_modified[:, pos+1:, :] = torch.randn_like(x_modified[:, pos+1:, :]) * 10\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_modified = encoder(x_modified, causal_mask=True)\n",
    "\n",
    "            assert torch.allclose(\n",
    "                output_causal[:, :pos+1, :],\n",
    "                output_modified[:, :pos+1, :],\n",
    "                atol=1e-5\n",
    "            ), f\"Encoder causality violated at position {pos}\"\n",
    "\n",
    "\n",
    "class TestCISABCausality:\n",
    "    \"\"\"Extensive tests for Causal Induction Self-Attention Block causality\"\"\"\n",
    "\n",
    "    def test_cisab_basic_causality(self):\n",
    "        \"\"\"Test that CISAB maintains causality\"\"\"\n",
    "        cisab = CausalInductionSelfAttentionBlock(\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            n_inducing_points=8,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        cisab.eval()\n",
    "\n",
    "        batch_size = 2\n",
    "        seq_len = 16\n",
    "        x = torch.randn(batch_size, seq_len, 64)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_causal = cisab(x, causal_mask=True)\n",
    "\n",
    "        # Modify future positions\n",
    "        for pos in range(seq_len - 1):\n",
    "            x_modified = x.clone()\n",
    "            # Add large perturbation to future positions\n",
    "            x_modified[:, pos+1:, :] = x_modified[:, pos+1:, :] + 50.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_modified = cisab(x_modified, causal_mask=True)\n",
    "\n",
    "            # Check past positions remain unchanged\n",
    "            assert torch.allclose(\n",
    "                output_causal[:, :pos+1, :],\n",
    "                output_modified[:, :pos+1, :],\n",
    "                atol=1e-4\n",
    "            ), f\"CISAB causality violated at position {pos}\"\n",
    "\n",
    "    def test_cisab_inducing_mask_structure(self):\n",
    "        \"\"\"Test that inducing point mask has correct structure\"\"\"\n",
    "        cisab = CausalInductionSelfAttentionBlock(\n",
    "            d_model=32,\n",
    "            n_heads=2,\n",
    "            n_inducing_points=4,\n",
    "            dropout=0.0\n",
    "        )\n",
    "\n",
    "        seq_len = 8\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "        # Get the mask\n",
    "        mask = cisab._create_causal_inducing_mask(seq_len, device)\n",
    "\n",
    "        # Check mask structure\n",
    "        n_ind = 4\n",
    "        for pos in range(seq_len):\n",
    "            start_idx = pos * n_ind\n",
    "            end_idx = (pos + 1) * n_ind\n",
    "\n",
    "            # Inducing points at position 'pos' should only see positions 0...pos\n",
    "            for ind_idx in range(start_idx, end_idx):\n",
    "                # Should be able to attend to positions 0...pos (mask = 0)\n",
    "                for j in range(pos + 1):\n",
    "                    assert mask[ind_idx, j] == 0, \\\n",
    "                        f\"Inducing point {ind_idx} at position {pos} cannot see position {j}\"\n",
    "\n",
    "                # Should not attend to positions after pos (mask = -inf)\n",
    "                for j in range(pos + 1, seq_len):\n",
    "                    assert mask[ind_idx, j] == float('-inf'), \\\n",
    "                        f\"Inducing point {ind_idx} at position {pos} can see future position {j}\"\n",
    "\n",
    "    def test_cisab_vs_non_causal(self):\n",
    "        \"\"\"Test that causal and non-causal CISAB produce different results\"\"\"\n",
    "        cisab = CausalInductionSelfAttentionBlock(\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            n_inducing_points=8,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        cisab.eval()\n",
    "\n",
    "        x = torch.randn(2, 10, 64)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_causal = cisab(x, causal_mask=True)\n",
    "            output_non_causal = cisab(x, causal_mask=False)\n",
    "\n",
    "        # Outputs should be different\n",
    "        assert not torch.allclose(output_causal, output_non_causal, atol=1e-3)\n",
    "\n",
    "\n",
    "class TestCausalInductionTransformer:\n",
    "    \"\"\"Test full Causal Induction Transformer\"\"\"\n",
    "\n",
    "    def test_encoder_output_shape(self):\n",
    "        \"\"\"Test output shapes of CISAB encoder\"\"\"\n",
    "        encoder = CausalInductionTransformerEncoder(\n",
    "            n_layers=3,\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            n_inducing_points=8\n",
    "        )\n",
    "\n",
    "        x = torch.randn(2, 20, 64)\n",
    "        output = encoder(x)\n",
    "        assert output.shape == x.shape\n",
    "\n",
    "    def test_encoder_causality_preservation(self):\n",
    "        \"\"\"Test that multi-layer CISAB encoder preserves causality\"\"\"\n",
    "        encoder = CausalInductionTransformerEncoder(\n",
    "            n_layers=4,\n",
    "            d_model=128,\n",
    "            n_heads=8,\n",
    "            n_inducing_points=16,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        encoder.eval()\n",
    "\n",
    "        batch_size = 2\n",
    "        seq_len = 24\n",
    "        x = torch.randn(batch_size, seq_len, 128)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_original = encoder(x, causal_mask=True)\n",
    "\n",
    "        # Test independence from future\n",
    "        for pos in range(0, seq_len - 1, 3):  # Test every 3rd position for speed\n",
    "            x_future_modified = x.clone()\n",
    "            x_future_modified[:, pos+1:, :] = torch.randn_like(x_future_modified[:, pos+1:, :]) * 5\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_modified = encoder(x_future_modified, causal_mask=True)\n",
    "\n",
    "            assert torch.allclose(\n",
    "                output_original[:, :pos+1, :],\n",
    "                output_modified[:, :pos+1, :],\n",
    "                atol=1e-4\n",
    "            ), f\"Multi-layer CISAB causality violated at position {pos}\"\n",
    "\n",
    "    def test_varying_inducing_points(self):\n",
    "        \"\"\"Test encoder with different inducing points per layer\"\"\"\n",
    "        encoder = CausalInductionTransformerEncoder(\n",
    "            n_layers=4,\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            n_inducing_points=8,  # Default\n",
    "            varying_inducing_points=[4, 8, 12, 16]  # Increasing\n",
    "        )\n",
    "\n",
    "        x = torch.randn(2, 15, 64)\n",
    "        output = encoder(x)\n",
    "        assert output.shape == x.shape\n",
    "\n",
    "\n",
    "class TestHybridTransformer:\n",
    "    \"\"\"Test Hybrid Transformer mixing standard and CISAB blocks\"\"\"\n",
    "\n",
    "    def test_hybrid_construction(self):\n",
    "        \"\"\"Test that hybrid transformer constructs correctly\"\"\"\n",
    "        encoder = HybridTransformerEncoder(\n",
    "            n_layers=6,\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            n_inducing_points=8,\n",
    "            cisab_layers=[2, 3, 5]  # Specific layers use CISAB\n",
    "        )\n",
    "\n",
    "        # Check that correct layers are CISAB\n",
    "        assert encoder.use_cisab == [False, False, True, True, False, True]\n",
    "\n",
    "    def test_hybrid_causality(self):\n",
    "        \"\"\"Test causality in hybrid transformer\"\"\"\n",
    "        encoder = HybridTransformerEncoder(\n",
    "            n_layers=4,\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            n_inducing_points=8,\n",
    "            cisab_start_layer=2,  # Last 2 layers use CISAB\n",
    "            dropout=0.0\n",
    "        )\n",
    "        encoder.eval()\n",
    "\n",
    "        x = torch.randn(2, 16, 64)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_original = encoder(x, causal_mask=True)\n",
    "\n",
    "        # Test causality preservation\n",
    "        for pos in range(0, 15, 2):\n",
    "            x_modified = x.clone()\n",
    "            x_modified[:, pos+1:, :] = torch.randn_like(x_modified[:, pos+1:, :]) * 10\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_modified = encoder(x_modified, causal_mask=True)\n",
    "\n",
    "            assert torch.allclose(\n",
    "                output_original[:, :pos+1, :],\n",
    "                output_modified[:, :pos+1, :],\n",
    "                atol=1e-4\n",
    "            ), f\"Hybrid transformer causality violated at position {pos}\"\n",
    "\n",
    "\n",
    "class TestEdgeCases:\n",
    "    \"\"\"Test edge cases and special scenarios\"\"\"\n",
    "\n",
    "    def test_single_token_sequence(self):\n",
    "        \"\"\"Test with sequence length of 1\"\"\"\n",
    "        encoder = TransformerEncoder(\n",
    "            n_layers=2,\n",
    "            d_model=32,\n",
    "            n_heads=4\n",
    "        )\n",
    "\n",
    "        x = torch.randn(2, 1, 32)\n",
    "        output = encoder(x, causal_mask=True)\n",
    "        assert output.shape == x.shape\n",
    "\n",
    "    def test_very_long_sequence(self):\n",
    "        \"\"\"Test with long sequences\"\"\"\n",
    "        encoder = CausalInductionTransformerEncoder(\n",
    "            n_layers=2,\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            n_inducing_points=32  # More inducing points for long sequence\n",
    "        )\n",
    "\n",
    "        x = torch.randn(1, 512, 64)\n",
    "        output = encoder(x, causal_mask=True)\n",
    "        assert output.shape == x.shape\n",
    "\n",
    "    def test_gradient_flow_through_cisab(self):\n",
    "        \"\"\"Test gradient flow through CISAB\"\"\"\n",
    "        cisab = CausalInductionSelfAttentionBlock(\n",
    "            d_model=32,\n",
    "            n_heads=2,\n",
    "            n_inducing_points=4\n",
    "        )\n",
    "\n",
    "        x = torch.randn(2, 8, 32, requires_grad=True)\n",
    "        output = cisab(x, causal_mask=True)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        assert x.grad is not None\n",
    "        assert not torch.isnan(x.grad).any()\n",
    "        assert not torch.isinf(x.grad).any()\n",
    "\n",
    "    def test_attention_mask_compatibility(self):\n",
    "        \"\"\"Test that custom attention masks work\"\"\"\n",
    "        encoder = TransformerEncoder(\n",
    "            n_layers=2,\n",
    "            d_model=64,\n",
    "            n_heads=4\n",
    "        )\n",
    "\n",
    "        batch_size = 2\n",
    "        seq_len = 10\n",
    "        x = torch.randn(batch_size, seq_len, 64)\n",
    "\n",
    "        # Create custom attention mask (e.g., for padding)\n",
    "        attention_mask = torch.ones(seq_len, seq_len)\n",
    "        attention_mask[:, 7:] = 0  # Mask out positions 7-9\n",
    "\n",
    "        output = encoder(x, attention_mask=attention_mask)\n",
    "        assert output.shape == x.shape\n",
    "\n",
    "\n",
    "class TestNumericalStability:\n",
    "    \"\"\"Test numerical stability of components\"\"\"\n",
    "\n",
    "    def test_rmsnorm_stability(self):\n",
    "        \"\"\"Test RMSNorm with extreme values\"\"\"\n",
    "        norm = RMSNorm(dim=64)\n",
    "\n",
    "        # Very small values\n",
    "        x_small = torch.randn(2, 5, 64) * 1e-8\n",
    "        output_small = norm(x_small)\n",
    "        assert not torch.isnan(output_small).any()\n",
    "        assert not torch.isinf(output_small).any()\n",
    "\n",
    "        # Very large values\n",
    "        x_large = torch.randn(2, 5, 64) * 1e8\n",
    "        output_large = norm(x_large)\n",
    "        assert not torch.isnan(output_large).any()\n",
    "        assert not torch.isinf(output_large).any()\n",
    "\n",
    "    def test_attention_numerical_stability(self):\n",
    "        \"\"\"Test attention with extreme values\"\"\"\n",
    "        attn = MultiheadAttentionWithRoPE(\n",
    "            d_model=32,\n",
    "            n_heads=2\n",
    "        )\n",
    "\n",
    "        # Input with large variance\n",
    "        x = torch.randn(2, 8, 32) * 100\n",
    "        output = attn(x, causal_mask=True)\n",
    "        assert not torch.isnan(output).any()\n",
    "        assert not torch.isinf(output).any()\n",
    "\n",
    "\n",
    "def test_comprehensive_causality_suite():\n",
    "    \"\"\"Run comprehensive causality tests across all architectures\"\"\"\n",
    "\n",
    "    print(\"Testing Standard Transformer Causality...\")\n",
    "    test_causal = TestCausality()\n",
    "    test_causal.test_standard_attention_causality()\n",
    "    test_causal.test_transformer_encoder_causality()\n",
    "    print(\"✓ Standard Transformer causality tests passed\")\n",
    "\n",
    "    print(\"\\nTesting CISAB Causality...\")\n",
    "    test_cisab = TestCISABCausality()\n",
    "    test_cisab.test_cisab_basic_causality()\n",
    "    test_cisab.test_cisab_inducing_mask_structure()\n",
    "    print(\"✓ CISAB causality tests passed\")\n",
    "\n",
    "    print(\"\\nTesting Causal Induction Transformer...\")\n",
    "    test_ci_transformer = TestCausalInductionTransformer()\n",
    "    test_ci_transformer.test_encoder_causality_preservation()\n",
    "    print(\"✓ Causal Induction Transformer causality tests passed\")\n",
    "\n",
    "    print(\"\\nTesting Hybrid Transformer...\")\n",
    "    test_hybrid = TestHybridTransformer()\n",
    "    test_hybrid.test_hybrid_causality()\n",
    "    print(\"✓ Hybrid Transformer causality tests passed\")\n",
    "\n",
    "    print(\"\\n✅ All causality tests passed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run all tests\n",
    "    test_comprehensive_causality_suite()\n",
    "\n",
    "    # Additional basic functionality tests\n",
    "    print(\"\\nRunning additional functionality tests...\")\n",
    "\n",
    "    # Test RMSNorm\n",
    "    test_norm = TestRMSNorm()\n",
    "    test_norm.test_output_shape()\n",
    "    test_norm.test_normalization_effect()\n",
    "\n",
    "    # Test RoPE\n",
    "    test_rope = TestRotaryPositionalEmbedding()\n",
    "    test_rope.test_output_shape()\n",
    "    test_rope.test_position_dependency()\n",
    "\n",
    "    # Test edge cases\n",
    "    test_edge = TestEdgeCases()\n",
    "    test_edge.test_single_token_sequence()\n",
    "    test_edge.test_gradient_flow_through_cisab()\n",
    "\n",
    "    print(\"✅ All tests completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51abea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cisab_model = CausalInductionTransformerEncoder(\n",
    "    n_layers=8,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_inducing_points=64,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "X = torch.randn(32, 512, 512).to(device)  # [batch, seq_len, d_model]\n",
    "Z = cisab_model(X, causal_mask=True)  # [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5f289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b20a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b071686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448cf7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-ansr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
