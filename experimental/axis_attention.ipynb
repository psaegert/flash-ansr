{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AxisAttentionBase(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size_Q: int,\n",
    "        size_KV: int,\n",
    "        size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        self.W_q = nn.Linear(size_Q, size)\n",
    "        self.W_k = nn.Linear(size_KV, size)\n",
    "        self.W_v = nn.Linear(size_KV, size)\n",
    "\n",
    "        self.fc_o = nn.Linear(size, size)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key_value: torch.Tensor) -> torch.Tensor:\n",
    "        q: torch.Tensor = self.W_q(query)\n",
    "        k: torch.Tensor = self.W_k(key_value)\n",
    "        v: torch.Tensor = self.W_v(key_value)\n",
    "\n",
    "        A = torch.einsum('bqd,bkd->bqk', q, k) / math.sqrt(self.size)\n",
    "        A = F.softmax(A, dim=-1)\n",
    "\n",
    "        output = torch.einsum('bqk,bkd->bqd', A, v)\n",
    "\n",
    "        output = output + F.relu(self.fc_o(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = AxisAttentionBase(512, 512, 512).to(device)\n",
    "X = torch.randn(128, 10, 512).to(device)\n",
    "output = aa(X, X)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import string\n",
    "\n",
    "class AxisAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_Q: int,\n",
    "        dim_KV: int,\n",
    "        dim_attn: int,\n",
    "        dropout: float = 0.0,\n",
    "        axis: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an AxisAttention module.\n",
    "\n",
    "        Args:\n",
    "            dim_Q: Input dimension of query tensor\n",
    "            dim_KV: Input dimension of key-value tensor\n",
    "            dim_attn: Internal attention dimension\n",
    "            dropout: Dropout probability after softmax\n",
    "            axis: Axis to perform attention over (default: 1)\n",
    "            use_residual: Whether to use residual connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_attn = dim_attn\n",
    "        self.axis = axis\n",
    "\n",
    "        # Projection layers\n",
    "        self.proj_q = nn.Linear(dim_Q, dim_attn)\n",
    "        self.proj_k = nn.Linear(dim_KV, dim_attn)\n",
    "        self.proj_v = nn.Linear(dim_KV, dim_attn)\n",
    "\n",
    "        # Output projection\n",
    "        self.proj_out = nn.Linear(dim_attn, dim_Q)\n",
    "\n",
    "        # Dropout for attention weights\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cached_expressions: dict = {}\n",
    "\n",
    "        self.attention_normalizer = math.sqrt(dim_attn)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key_value: torch.Tensor, verbose: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform attention along the specified axis.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor of shape (..., dim_Q)\n",
    "            key_value: Key-value tensor of shape (..., dim_KV)\n",
    "            verbose: Whether to print intermediate shapes\n",
    "\n",
    "        Returns:\n",
    "            Output tensor with the same shape as query\n",
    "        \"\"\"\n",
    "        # Project inputs to attention space\n",
    "        q = self.proj_q(query)      # (..., dim_attn)\n",
    "        k = self.proj_k(key_value)  # (..., dim_attn)\n",
    "        v = self.proj_v(key_value)  # (..., dim_attn)\n",
    "\n",
    "        if len(self.cached_expressions) != 5:\n",
    "            # Ensure axis is positive\n",
    "            axis = self.axis if self.axis >= 0 else query.ndim + self.axis\n",
    "            if axis == query.ndim or axis == key_value.ndim:\n",
    "                raise ValueError(f\"Axis cannot be the last dimension of the tensors. Got {axis=} alghough the tensors have {query.ndim=} and {key_value.ndim=} dimensions.\")\n",
    "\n",
    "            # Generate einsum notation\n",
    "            ndim = query.ndim\n",
    "\n",
    "            # Create dimension labels using lowercase letters for all dimensions\n",
    "            available_letters = string.ascii_lowercase[3:]\n",
    "            dims = available_letters[:ndim-1]\n",
    "\n",
    "            # Build einsum expressions\n",
    "            batch_dims = ''.join([dims[i] for i in range(ndim-1) if i != axis])\n",
    "            self.cached_expressions[\"q_expr\"] = batch_dims + ('a' if axis < ndim-1 else '') + 'c'\n",
    "            self.cached_expressions[\"k_expr\"] = batch_dims + ('b' if axis < ndim-1 else '') + 'c'\n",
    "            self.cached_expressions[\"v_expr\"] = batch_dims + ('b' if axis < ndim-1 else '') + 'c'\n",
    "\n",
    "            # Output should match query's shape\n",
    "            self.cached_expressions[\"out_expr\"] = batch_dims + ('a' if axis < ndim-1 else '') + 'c'\n",
    "\n",
    "            # Attention map expression\n",
    "            self.cached_expressions[\"attn_expr\"] = batch_dims + ('ab' if axis < ndim-1 else '')\n",
    "\n",
    "        # Compute attention scores\n",
    "        if verbose:\n",
    "            print(f'{self.cached_expressions[\"q_expr\"]},{self.cached_expressions[\"k_expr\"]}->{self.cached_expressions[\"attn_expr\"]}')\n",
    "        attn = torch.einsum(f'{self.cached_expressions[\"q_expr\"]},{self.cached_expressions[\"k_expr\"]}->{self.cached_expressions[\"attn_expr\"]}', q, k) * self.attention_normalizer\n",
    "\n",
    "        # Apply softmax along the key dimension (last dimension of attention tensor)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Apply attention to values\n",
    "        if verbose:\n",
    "            print(f'{self.cached_expressions[\"attn_expr\"]},{self.cached_expressions[\"v_expr\"]}->{self.cached_expressions[\"out_expr\"]}')\n",
    "        output = torch.einsum(f'{self.cached_expressions[\"attn_expr\"]},{self.cached_expressions[\"v_expr\"]}->{self.cached_expressions[\"out_expr\"]}', attn, v)\n",
    "\n",
    "        # Add residual connection if requested\n",
    "        output = query + self.proj_out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order 1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dac,dbc->dab\n",
      "dab,dbc->dac\n",
      "torch.Size([128, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = AxisAttention(512, 512, 512).to(device)\n",
    "X = torch.randn(128, 10, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order 2 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfac,dfbc->dfab\n",
      "dfab,dfbc->dfac\n",
      "torch.Size([128, 10, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = AxisAttention(512, 512, 512).to(device)\n",
    "X = torch.randn(128, 10, 3, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deac,debc->deab\n",
      "deab,debc->deac\n",
      "torch.Size([128, 10, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = AxisAttention(512, 512, 512, axis=2).to(device)\n",
    "X = torch.randn(128, 10, 3, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order 3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfgac,dfgbc->dfgab\n",
      "dfgab,dfgbc->dfgac\n",
      "torch.Size([128, 10, 3, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = AxisAttention(512, 512, 512, axis=1).to(device)\n",
    "X = torch.randn(128, 10, 3, 6, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degac,degbc->degab\n",
      "degab,degbc->degac\n",
      "torch.Size([128, 10, 3, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = AxisAttention(512, 512, 512, axis=2).to(device)\n",
    "X = torch.randn(128, 10, 3, 6, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defac,defbc->defab\n",
      "defab,defbc->defac\n",
      "torch.Size([128, 10, 3, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = AxisAttention(512, 512, 512, axis=3).to(device)\n",
    "X = torch.randn(128, 10, 3, 6, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import string\n",
    "\n",
    "class MultiheadAxisAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_Q: int,\n",
    "        dim_KV: int,\n",
    "        dim_out: int | None = None,\n",
    "        num_heads: int = 8,\n",
    "        head_dim: int | None = None,\n",
    "        dropout: float = 0.0,\n",
    "        axis: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a MultiheadAxisAttention module.\n",
    "\n",
    "        Args:\n",
    "            dim_Q: Input dimension of query tensor\n",
    "            dim_KV: Input dimension of key-value tensor\n",
    "            dim_out: Output dimension (defaults to dim_Q if None)\n",
    "            num_heads: Number of attention heads\n",
    "            head_dim: Dimension of each attention head (if None, calculated as dim_Q // num_heads)\n",
    "            dropout: Dropout probability after softmax\n",
    "            axis: Axis to perform attention over (default: 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.axis = axis\n",
    "\n",
    "        # Set output dimension\n",
    "        self.dim_out = dim_out if dim_out is not None else dim_Q\n",
    "\n",
    "        # Set head dimension\n",
    "        self.head_dim = head_dim if head_dim is not None else dim_Q // num_heads\n",
    "        if self.head_dim * num_heads != dim_Q and head_dim is None:\n",
    "            raise ValueError(f\"dim_Q ({dim_Q}) must be divisible by num_heads ({num_heads})\")\n",
    "\n",
    "        # Total dimension for all heads combined\n",
    "        self.total_head_dim = self.head_dim * num_heads\n",
    "\n",
    "        # Projection layers\n",
    "        self.proj_q = nn.Linear(dim_Q, self.total_head_dim)\n",
    "        self.proj_k = nn.Linear(dim_KV, self.total_head_dim)\n",
    "        self.proj_v = nn.Linear(dim_KV, self.total_head_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.proj_out = nn.Linear(self.total_head_dim, self.dim_out)\n",
    "\n",
    "        # Dropout for attention weights\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Cache for einsum expressions\n",
    "        self.cached_expressions: dict[str, str] = {}\n",
    "\n",
    "        # Scaling factor for attention scores\n",
    "        self.attention_normalizer = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "    def _generate_einsum_expressions(self, query_shape: tuple[int, ...]) -> None:\n",
    "        \"\"\"Generate and cache einsum expressions based on input tensor shapes\"\"\"\n",
    "        # Ensure axis is positive\n",
    "        axis = self.axis if self.axis >= 0 else len(query_shape) + self.axis\n",
    "        if axis >= len(query_shape) - 1:\n",
    "            raise ValueError(f\"Axis cannot be the last dimension. Got {axis=} but tensor has {len(query_shape)} dimensions.\")\n",
    "\n",
    "        # Generate einsum notation\n",
    "        ndim = len(query_shape)\n",
    "\n",
    "        # Reserve 'd' for head dimension, 'a' for query attention, 'b' for key attention, 'c' for embedding\n",
    "        available_letters = string.ascii_lowercase[4:]  # Skip a, b, c, d\n",
    "        if ndim - 2 > len(available_letters):  # -2 for attention axis and embedding dim\n",
    "            raise ValueError(f\"Tensor has too many dimensions: {ndim}\")\n",
    "\n",
    "        # Assign dimension labels\n",
    "        batch_dims = []\n",
    "        letter_idx = 0\n",
    "\n",
    "        for i in range(ndim - 1):  # Exclude the last dimension (embedding)\n",
    "            if i == axis:\n",
    "                # For the attention axis, use 'a'\n",
    "                batch_dims.append('a')\n",
    "            else:\n",
    "                # For other dimensions, use remaining letters\n",
    "                batch_dims.append(available_letters[letter_idx])\n",
    "                letter_idx += 1\n",
    "\n",
    "        batch_expr = ''.join(batch_dims)\n",
    "\n",
    "        # Create expressions with head dimension 'h'\n",
    "        # Query: batch_dims + head + embedding\n",
    "        self.cached_expressions[\"q_expr\"] = batch_expr + 'dc'\n",
    "\n",
    "        # Key and Value: replace 'a' with 'b' for the attention dimension\n",
    "        k_batch = batch_expr.replace('a', 'b')\n",
    "        self.cached_expressions[\"k_expr\"] = k_batch + 'dc'\n",
    "        self.cached_expressions[\"v_expr\"] = k_batch + 'dc'\n",
    "\n",
    "        # Attention map: batch_dims (with both 'a' and 'b') + head\n",
    "        attn_batch = batch_expr.replace('a', 'ab')\n",
    "        self.cached_expressions[\"attn_expr\"] = attn_batch + 'd'\n",
    "\n",
    "        # Output: same as query but with embedding dimension\n",
    "        self.cached_expressions[\"out_expr\"] = batch_expr + 'dc'\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key_value: torch.Tensor, verbose: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform multi-head attention along the specified axis.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor of shape (..., dim_Q)\n",
    "            key_value: Key-value tensor of shape (..., dim_KV)\n",
    "            verbose: Whether to print intermediate shapes and expressions\n",
    "\n",
    "        Returns:\n",
    "            Output tensor with shape (..., dim_out)\n",
    "        \"\"\"\n",
    "        batch_shape = query.shape[:-1]\n",
    "\n",
    "        # Project inputs to attention space\n",
    "        q = self.proj_q(query)      # (..., total_head_dim)\n",
    "        k = self.proj_k(key_value)  # (..., total_head_dim)\n",
    "        v = self.proj_v(key_value)  # (..., total_head_dim)\n",
    "\n",
    "        # Reshape to separate head dimension\n",
    "        q = q.view(*batch_shape, self.num_heads, self.head_dim)\n",
    "        k = k.view(*key_value.shape[:-1], self.num_heads, self.head_dim)\n",
    "        v = v.view(*key_value.shape[:-1], self.num_heads, self.head_dim)\n",
    "\n",
    "        # Generate einsum expressions if not cached\n",
    "        if not self.cached_expressions:\n",
    "            self._generate_einsum_expressions(query.shape)\n",
    "\n",
    "        # Compute attention scores\n",
    "        if verbose:\n",
    "            print(f\"Q shape: {q.shape}\")\n",
    "            print(f\"K shape: {k.shape}\")\n",
    "            print(f\"Einsum: {self.cached_expressions['q_expr']},{self.cached_expressions['k_expr']}->{self.cached_expressions['attn_expr']}\")\n",
    "\n",
    "        attn = torch.einsum(\n",
    "            f\"{self.cached_expressions['q_expr']},{self.cached_expressions['k_expr']}->{self.cached_expressions['attn_expr']}\", \n",
    "            q, k\n",
    "        ) * self.attention_normalizer\n",
    "\n",
    "        # Apply softmax along the key dimension (dimension 'b')\n",
    "        attn = F.softmax(attn, dim=-2)  # -2 corresponds to 'b' dimension\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Attention shape: {attn.shape}\")\n",
    "            print(f\"V shape: {v.shape}\")\n",
    "            print(f\"Einsum: {self.cached_expressions['attn_expr']},{self.cached_expressions['v_expr']}->{self.cached_expressions['out_expr']}\")\n",
    "\n",
    "        # Apply attention to values\n",
    "        output = torch.einsum(\n",
    "            f\"{self.cached_expressions['attn_expr']},{self.cached_expressions['v_expr']}->{self.cached_expressions['out_expr']}\", \n",
    "            attn, v\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Output before reshape: {output.shape}\")\n",
    "\n",
    "        # Reshape back to combine heads\n",
    "        output = output.reshape(*batch_shape, self.total_head_dim)\n",
    "\n",
    "        # Apply output projection\n",
    "        output = self.proj_out(output)\n",
    "\n",
    "        # Add residual connection\n",
    "        output = query + output\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order 1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([128, 10, 8, 64])\n",
      "K shape: torch.Size([128, 10, 8, 64])\n",
      "Einsum: eadc,ebdc->eabd\n",
      "Attention shape: torch.Size([128, 10, 10, 8])\n",
      "V shape: torch.Size([128, 10, 8, 64])\n",
      "Einsum: eabd,ebdc->eadc\n",
      "Output before reshape: torch.Size([128, 10, 8, 64])\n",
      "torch.Size([128, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = MultiheadAxisAttention(512, 512, 512).to(device)\n",
    "X = torch.randn(128, 10, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order 2 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([128, 10, 3, 8, 64])\n",
      "K shape: torch.Size([128, 10, 3, 8, 64])\n",
      "Einsum: eafdc,ebfdc->eabfd\n",
      "Attention shape: torch.Size([128, 10, 10, 3, 8])\n",
      "V shape: torch.Size([128, 10, 3, 8, 64])\n",
      "Einsum: eabfd,ebfdc->eafdc\n",
      "Output before reshape: torch.Size([128, 10, 3, 8, 64])\n",
      "torch.Size([128, 10, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = MultiheadAxisAttention(512, 512, 512).to(device)\n",
    "X = torch.randn(128, 10, 3, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([128, 10, 3, 8, 64])\n",
      "K shape: torch.Size([128, 10, 3, 8, 64])\n",
      "Einsum: efadc,efbdc->efabd\n",
      "Attention shape: torch.Size([128, 10, 3, 3, 8])\n",
      "V shape: torch.Size([128, 10, 3, 8, 64])\n",
      "Einsum: efabd,efbdc->efadc\n",
      "Output before reshape: torch.Size([128, 10, 3, 8, 64])\n",
      "torch.Size([128, 10, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = MultiheadAxisAttention(512, 512, 512, axis=2).to(device)\n",
    "X = torch.randn(128, 10, 3, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order 3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "K shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "Einsum: eafgdc,ebfgdc->eabfgd\n",
      "Attention shape: torch.Size([128, 10, 10, 3, 6, 8])\n",
      "V shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "Einsum: eabfgd,ebfgdc->eafgdc\n",
      "Output before reshape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "torch.Size([128, 10, 3, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = MultiheadAxisAttention(512, 512, 512, axis=1).to(device)\n",
    "X = torch.randn(128, 10, 3, 6, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "K shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "Einsum: efagdc,efbgdc->efabgd\n",
      "Attention shape: torch.Size([128, 10, 3, 3, 6, 8])\n",
      "V shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "Einsum: efabgd,efbgdc->efagdc\n",
      "Output before reshape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "torch.Size([128, 10, 3, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = MultiheadAxisAttention(512, 512, 512, axis=2).to(device)\n",
    "X = torch.randn(128, 10, 3, 6, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "K shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "Einsum: efgadc,efgbdc->efgabd\n",
      "Attention shape: torch.Size([128, 10, 3, 6, 6, 8])\n",
      "V shape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "Einsum: efgabd,efgbdc->efgadc\n",
      "Output before reshape: torch.Size([128, 10, 3, 6, 8, 64])\n",
      "torch.Size([128, 10, 3, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "aa = MultiheadAxisAttention(512, 512, 512, axis=3).to(device)\n",
    "X = torch.randn(128, 10, 3, 6, 512).to(device)\n",
    "output = aa(X, X, verbose=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxisSAB(nn.Module):\n",
    "    # https://github.com/juho-lee/set_transformer\n",
    "    def __init__(self, input_size: int, output_size: int, n_heads: int, axis: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.mab = MultiheadAxisAttention(input_size, input_size, output_size, n_heads, axis=axis)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mab(X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "asab = AxisSAB(512, 512, 8).to(device)\n",
    "X = torch.randn(128, 10, 512).to(device)\n",
    "output = asab(X)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "asab = AxisSAB(512, 512, 8, axis=2).to(device)\n",
    "X = torch.randn(128, 10, 3, 512).to(device)\n",
    "output = asab(X)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxisISAB(nn.Module):\n",
    "    # https://github.com/juho-lee/set_transformer\n",
    "    def __init__(self, input_size: int, output_size: int, n_heads: int, n_induce: int, axis: int = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.inducing_points = nn.Parameter(torch.Tensor(n_induce, output_size))\n",
    "        nn.init.xavier_uniform_(self.inducing_points)\n",
    "\n",
    "        self.mab0 = MultiheadAxisAttention(output_size, input_size, output_size, n_heads, axis=axis)\n",
    "        self.mab1 = MultiheadAxisAttention(input_size, output_size, output_size, n_heads, axis=axis)\n",
    "\n",
    "        self.axis = axis\n",
    "        self.n_induce = n_induce\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Format inducing points to align with the attention axis\n",
    "        formatted_induction_points = self._format_inducing_points(X)\n",
    "\n",
    "        # Apply attention mechanisms\n",
    "        H = self.mab0(formatted_induction_points, X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "    def _format_inducing_points(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Format inducing points to align with the attention axis of X.\n",
    "\n",
    "        Args:\n",
    "            X: Input tensor of shape (..., input_size)\n",
    "\n",
    "        Returns:\n",
    "            Formatted inducing points tensor with shape matching X except:\n",
    "            - The attention axis dimension is replaced with n_induce\n",
    "            - The last dimension is output_size\n",
    "        \"\"\"\n",
    "        # Ensure axis is positive\n",
    "        axis = self.axis if self.axis >= 0 else X.ndim + self.axis\n",
    "\n",
    "        # Start with the basic inducing points: [n_induce, output_size]\n",
    "        points = self.inducing_points\n",
    "\n",
    "        # Create the target shape:\n",
    "        # - Same as X for all batch dimensions before the attention axis\n",
    "        # - n_induce at the attention axis\n",
    "        # - Same as X for all batch dimensions after the attention axis\n",
    "        # - output_size as the last dimension\n",
    "        target_shape = list(X.shape)\n",
    "        target_shape[axis] = self.n_induce\n",
    "\n",
    "        # Create a list of dimensions to expand\n",
    "        expand_shape = list(target_shape)\n",
    "\n",
    "        # Build reshape pattern by adding singleton dimensions\n",
    "        view_shape = [1] * (X.ndim - 1)  # -1 because points already has the last dimension\n",
    "        view_shape[axis] = self.n_induce\n",
    "\n",
    "        # Reshape inducing points to have singleton dimensions in all batch dims\n",
    "        points = points.view(*view_shape, -1)\n",
    "\n",
    "        # Expand to match the target shape (efficiently reuses memory)\n",
    "        points = points.expand(*expand_shape)\n",
    "\n",
    "        return points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "aisab = AxisISAB(512, 512, 8, 10).to(device)\n",
    "X = torch.randn(128, 10, 512).to(device)\n",
    "output = aisab(X)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "aisab = AxisISAB(512, 512, 8, 10, axis=2).to(device)\n",
    "X = torch.randn(128, 10, 3, 512).to(device)\n",
    "output = aisab(X)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxisPMA(nn.Module):\n",
    "    \"\"\"\n",
    "    Pooling by Multihead Attention with axis support.\n",
    "    Adapted from https://github.com/juho-lee/set_transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, size: int, n_heads: int, n_seeds: int, axis: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a Pooling by Multihead Attention module with axis support.\n",
    "\n",
    "        Args:\n",
    "            size: Dimension of input and output features\n",
    "            n_heads: Number of attention heads\n",
    "            n_seeds: Number of seed vectors (output set size)\n",
    "            axis: Axis to perform attention over (default: 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.S = nn.Parameter(torch.Tensor(n_seeds, size))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "\n",
    "        self.mab = MultiheadAxisAttention(size, size, size, n_heads, axis=axis)\n",
    "        self.axis = axis\n",
    "        self.n_seeds = n_seeds\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply pooling attention to input tensor.\n",
    "\n",
    "        Args:\n",
    "            X: Input tensor of shape (..., size)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor with n_seeds elements along the attention axis\n",
    "        \"\"\"\n",
    "        # Format seed vectors to align with the attention axis\n",
    "        formatted_seeds = self._format_seeds(X)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        return self.mab(formatted_seeds, X)\n",
    "\n",
    "    def _format_seeds(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Format seed vectors to align with the attention axis of X.\n",
    "\n",
    "        Args:\n",
    "            X: Input tensor of shape (..., size)\n",
    "\n",
    "        Returns:\n",
    "            Formatted seed vectors with shape matching X's batch dimensions,\n",
    "            n_seeds at the attention axis, and size as the feature dimension\n",
    "        \"\"\"\n",
    "        # Ensure axis is positive\n",
    "        axis = self.axis if self.axis >= 0 else X.ndim + self.axis\n",
    "\n",
    "        # Get the batch shape (all dimensions except the last one)\n",
    "        batch_shape = list(X.shape[:-1])\n",
    "\n",
    "        # Create the target shape for the seeds:\n",
    "        # - Same as X for all batch dimensions except the attention axis\n",
    "        # - n_seeds at the attention axis\n",
    "        # - size as the last dimension\n",
    "        target_shape = batch_shape.copy()\n",
    "        target_shape[axis] = self.n_seeds\n",
    "\n",
    "        # Create a view shape with singleton dimensions for all batch dims\n",
    "        view_shape = [1] * len(batch_shape)\n",
    "        view_shape[axis] = self.n_seeds\n",
    "\n",
    "        # Reshape seed vectors to have singleton dimensions in all batch dims\n",
    "        seeds = self.S.view(*view_shape, -1)\n",
    "\n",
    "        # Expand to match the target shape (efficiently reuses memory)\n",
    "        seeds = seeds.expand(*target_shape, X.shape[-1])\n",
    "\n",
    "        return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "apma = AxisPMA(512, 8, 3).to(device)\n",
    "X = torch.randn(128, 10, 512).to(device)\n",
    "output = apma(X)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "apma = AxisPMA(512, 8, 3, axis=2).to(device)\n",
    "X = torch.randn(128, 10, 16, 512).to(device)\n",
    "output = apma(X)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-ansr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
