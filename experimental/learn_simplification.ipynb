{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_ansr.expressions import ExpressionSpace\n",
    "from flash_ansr.expressions.utils import codify, num_to_constants, flatten_nested_list\n",
    "from flash_ansr import get_path\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Generator, Callable, Any\n",
    "import json\n",
    "import warnings\n",
    "from scipy.optimize import curve_fit, OptimizeWarning\n",
    "from copy import deepcopy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'v7.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_path('configs', MODEL, 'expression_space.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = ExpressionSpace.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expression_generator(hashes_of_size: dict[int, list[tuple[str]]], non_leaf_nodes: dict[str, int]) -> Generator[tuple[str], None, None]:\n",
    "    # Append existing trees to every operator\n",
    "    for new_root_operator, arity in non_leaf_nodes.items():\n",
    "        # Start with the smallest arity-tuples of trees\n",
    "        for child_lengths in sorted(itertools.product(list(hashes_of_size.keys()), repeat=arity), key=lambda x: sum(x)):\n",
    "            # Check all possible combinations of child trees\n",
    "            for child_combination in itertools.product(*[hashes_of_size[child_length] for child_length in child_lengths]):\n",
    "                yield (new_root_operator,) + tuple(itertools.chain.from_iterable(child_combination))\n",
    "\n",
    "def exist_constants_that_fit(expression: list[str], variables: list[str], X: np.ndarray, y_target: np.ndarray, debug: bool = False):\n",
    "    if isinstance(expression, tuple):\n",
    "        expression = list(expression)\n",
    "\n",
    "    executable_prefix_expression = space.operators_to_realizations(expression)\n",
    "    prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "    code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "    code = codify(code_string, variables + constants)\n",
    "    f = space.code_to_lambda(code)\n",
    "\n",
    "    def pred_function(X: np.ndarray, *constants: np.ndarray | None) -> float:\n",
    "        if len(constants) == 0:\n",
    "            y = f(*X.T)\n",
    "        y =  f(*X.T, *constants)\n",
    "\n",
    "        # If the numbers are complex, return nan\n",
    "        if np.iscomplexobj(y):\n",
    "            return np.full(y.shape, np.nan)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    p0 = np.random.normal(loc=0, scale=5, size=len(constants))\n",
    "\n",
    "    is_valid = np.isfinite(X).all(axis=1) & np.isfinite(y_target)\n",
    "\n",
    "    if not np.any(is_valid):\n",
    "        if debug:\n",
    "            print(\"No valid data points\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=OptimizeWarning)\n",
    "            popt, _ = curve_fit(pred_function, X[is_valid], y_target[is_valid].flatten(), p0=p0)\n",
    "    except RuntimeError:\n",
    "        if debug:\n",
    "            print(\"RuntimeError\")\n",
    "        return False\n",
    "\n",
    "    y = f(*X.T, *popt)\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.full(X.shape[0], y)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Constants: {popt}\")\n",
    "        print(f\"y_target: {y_target}\")\n",
    "        print(f\"y: {y}\")\n",
    "        print(f'All close: {np.allclose(y_target, y, equal_nan=True)}')\n",
    "\n",
    "    return np.allclose(y_target, y, equal_nan=True)\n",
    "\n",
    "def remap_expression(source_expression: list[str], dummy_variables: list[str], variable_mapping: dict | None = None):\n",
    "    source_expression = deepcopy(source_expression)\n",
    "    if variable_mapping is None:\n",
    "        variable_mapping = {}\n",
    "        for i, token in enumerate(source_expression):\n",
    "            if token in dummy_variables:\n",
    "                if token not in variable_mapping:\n",
    "                    variable_mapping[token] = f'_{len(variable_mapping)}'\n",
    "    \n",
    "    for i, token in enumerate(source_expression):\n",
    "        if token in dummy_variables:\n",
    "            source_expression[i] = variable_mapping[token]\n",
    "\n",
    "    return source_expression, variable_mapping\n",
    "\n",
    "def prefix_to_tree(expression: list[str], operator_arity: dict[str, int]) -> list:\n",
    "    def build_tree(index):\n",
    "        if index >= len(expression):\n",
    "            return None, index\n",
    "\n",
    "        token = expression[index]\n",
    "\n",
    "        # If token is not an operator or is an operator with arity 0\n",
    "        if isinstance(token, dict) or token not in operator_arity or operator_arity[token] == 0:\n",
    "            return [token], index + 1\n",
    "\n",
    "        # If token is an operator\n",
    "        operands = []\n",
    "        current_index = index + 1\n",
    "\n",
    "        # Process operands based on the operator's arity\n",
    "        for _ in range(operator_arity[token]):\n",
    "            if current_index >= len(expression):\n",
    "                break\n",
    "\n",
    "            subtree, current_index = build_tree(current_index)\n",
    "            if subtree:\n",
    "                operands.append(subtree)\n",
    "\n",
    "        return [token, operands], current_index\n",
    "\n",
    "    result, _ = build_tree(0)\n",
    "    return result\n",
    "\n",
    "def safe_f(f: Callable, X: np.ndarray, constants: np.ndarray | None = None):\n",
    "    try:\n",
    "        if constants is None:\n",
    "            y = f(*X.T)\n",
    "        else:\n",
    "            y = f(*X.T, *constants)\n",
    "        if not isinstance(y, np.ndarray) or y.shape[0] == 1:\n",
    "            y = np.full(X.shape[0], y)\n",
    "        return y\n",
    "    except ZeroDivisionError:\n",
    "        return np.full(X.shape[0], np.nan)\n",
    "    \n",
    "def pattern_match(tree: list, pattern: list, mapping: dict[str, Any] | None = None) -> tuple[bool, dict[str, Any]]:\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "\n",
    "    if len(tree) == 1 and isinstance(tree[0], str) and len(pattern) != 1:\n",
    "        return False, mapping\n",
    "\n",
    "    if len(pattern) == 1 and isinstance(pattern[0], str):\n",
    "        if pattern[0].startswith('_'):\n",
    "            if pattern[0] not in mapping:\n",
    "                mapping[pattern[0]] = tree\n",
    "            elif mapping[pattern[0]] != tree:\n",
    "                return False, mapping\n",
    "            return True, mapping\n",
    "        \n",
    "        if tree != pattern:\n",
    "            return False, mapping\n",
    "        return True, mapping\n",
    "\n",
    "    tree_operator, tree_operands = tree\n",
    "    pattern_operator, pattern_operands = pattern\n",
    "\n",
    "    if tree_operator != pattern_operator:\n",
    "        return False, mapping\n",
    "    \n",
    "    for tree_operand, pattern_operand in zip(tree_operands, pattern_operands):\n",
    "        if isinstance(pattern_operand, str):\n",
    "            if pattern_operand not in mapping:\n",
    "                mapping[pattern_operand] = tree_operand\n",
    "            elif mapping[pattern_operand] != tree_operand:\n",
    "                return False, mapping\n",
    "        else:\n",
    "            does_match, mapping = pattern_match(tree_operand, pattern_operand, mapping)\n",
    "            if not does_match:\n",
    "                return False, mapping\n",
    "\n",
    "    return True, mapping\n",
    "\n",
    "def apply_mapping(tree: list, mapping: dict[str, Any]) -> list:\n",
    "    if len(tree) == 1 and isinstance(tree[0], str):\n",
    "        if tree[0].startswith('_'):\n",
    "            return mapping[tree[0]]\n",
    "        return tree\n",
    "\n",
    "    operator, operands = tree\n",
    "    return [operator] + [apply_mapping(operand, mapping) for operand in operands]\n",
    "\n",
    "def _subtree_simplify(expression: list[str], rules_trees: dict[int, set[tuple[list[str], list[str]]]]) -> list[str]:\n",
    "    stack: list = []\n",
    "    i = len(expression) - 1\n",
    "\n",
    "    while i >= 0:\n",
    "        token = expression[i]\n",
    "        applied_rule = False\n",
    "\n",
    "        if token in space.operator_arity_compat or token in space.operator_aliases:\n",
    "            operator = space.operator_aliases.get(token, token)\n",
    "            arity = space.operator_arity_compat[operator]\n",
    "            operands = list(reversed(stack[-arity:]))\n",
    "\n",
    "            # Check if a pattern matches the current subtree\n",
    "            for rule in rules_trees.get(arity, []):\n",
    "                subtree = [operator, operands]\n",
    "                does_match, mapping = pattern_match(subtree, rule[0], mapping=None)\n",
    "                if does_match:\n",
    "\n",
    "                    # Replace the placeholders (keys of the mapping) with the actual subtrees (values of the mapping) in the entire subtree at any depth\n",
    "                    _ = [stack.pop() for _ in range(arity)]\n",
    "                    stack.append(apply_mapping(deepcopy(rule[1]), mapping))\n",
    "                    i -= 1\n",
    "                    applied_rule = True\n",
    "                    break\n",
    "\n",
    "            if not applied_rule:\n",
    "                # print('No rule applied')\n",
    "                _ = [stack.pop() for _ in range(arity)]\n",
    "                stack.append([operator, operands])\n",
    "                i -= 1\n",
    "                continue\n",
    "\n",
    "        if not applied_rule:\n",
    "            stack.append([token])\n",
    "            i -= 1\n",
    "\n",
    "    return flatten_nested_list(stack)[::-1]\n",
    "\n",
    "def subtree_simplify(expression: list[str], rules_trees: dict[int, set[tuple[list[str], list[str]]]], max_iter: int = 1) -> list[str]:\n",
    "    new_expression = expression\n",
    "    for _ in range(max_iter):\n",
    "        new_expression = _subtree_simplify(new_expression, rules_trees)\n",
    "        if new_expression == expression:\n",
    "            break\n",
    "        expression = new_expression\n",
    "    return new_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_to_tree(['x1'], space.operator_arity_compat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', [['x1'], ['x2']]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_to_tree(['+', 'x1', 'x2'], space.operator_arity_compat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*', [['x1'], ['cos', [['x2']]]]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "prefix_to_tree(['*', 'x1', 'cos', 'x2'], space.operator_arity_compat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['x3'], '_2': ['x2']})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', 'cos', 'x3', 'x2'], space.operator_arity), prefix_to_tree(['+', 'cos', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', [['cos', [['x1']]], ['x2']]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_to_tree(['+', 'cos', 'x1', 'x2'], space.operator_arity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['cos', [['x1']]], '_2': ['x2']})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', 'cos', 'x1', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['+', [['cos', [['x1']]], ['x2']]], '_2': ['x2']})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', '+', 'cos', 'x1', 'x2', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, {})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['-', '+', 'cos', 'x1', 'x2', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['-', [['cos', [['x1']]], ['x2']]], '_2': ['x2']})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', '-', 'cos', 'x1', 'x2', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rules = [\n",
    "    [['-', 'x1', 'x1'], ['0']],\n",
    "    [['*', 'x1', 'x1'], ['pow2', 'x1']],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 2\n",
      "Number of unique rules: 2 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate the rules\n",
    "deduplicated_rules = set()\n",
    "for rule in example_rules:\n",
    "    # Rename variables in the source expression\n",
    "    remapped_source, variable_mapping = remap_expression(list(rule[0]), dummy_variables=['x1'])\n",
    "    remapped_target, _ = remap_expression(list(rule[1]), variable_mapping)\n",
    "    deduplicated_rules.add((tuple(remapped_source), tuple(remapped_target)))\n",
    "\n",
    "print(f'Number of rules: {len(example_rules)}')\n",
    "print(f'Number of unique rules: {len(deduplicated_rules)} ({100 * len(deduplicated_rules) / len(example_rules):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [(('-', '_0', '_0'), ('0',)), (('*', '_0', '_0'), ('pow2', '_0'))]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplicated_rules_of_arity = defaultdict(list)\n",
    "for rule in deduplicated_rules:\n",
    "    arity = space.operator_arity[rule[0][0]]\n",
    "    deduplicated_rules_of_arity[arity].append(rule)\n",
    "\n",
    "deduplicated_rules_of_arity = dict(deduplicated_rules_of_arity)\n",
    "deduplicated_rules_of_arity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_trees = {a: [\n",
    "    (\n",
    "        prefix_to_tree(list(rule[0]), space.operator_arity_compat),\n",
    "        prefix_to_tree(list(rule[1]), space.operator_arity_compat)\n",
    "    )\n",
    "        for rule in deduplicated_rules_of_arity_a] for a, deduplicated_rules_of_arity_a in deduplicated_rules_of_arity.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_rules(rules_list: list[tuple[list[str], list[str]]], dummy_variables: list[str]) -> list[tuple[list[str], list[str]]]:\n",
    "    deduplicated_rules = set()\n",
    "    for rule in rules_list:\n",
    "        # Rename variables in the source expression\n",
    "        remapped_source, variable_mapping = remap_expression(list(rule[0]), dummy_variables=dummy_variables)\n",
    "        remapped_target, _ = remap_expression(list(rule[1]), variable_mapping)\n",
    "        deduplicated_rules.add((tuple(remapped_source), tuple(remapped_target)))\n",
    "\n",
    "    return list(deduplicated_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules_trees_from_rules_list(rules_list: list[tuple[list[str], list[str]]], dummy_variables: list[str]) -> dict[int, set[tuple[list, list]]]:\n",
    "    deduplicated_rules = deduplicate_rules(rules_list, dummy_variables)\n",
    "\n",
    "    deduplicated_rules_of_arity = defaultdict(list)\n",
    "    for rule in deduplicated_rules:\n",
    "        arity = space.operator_arity[rule[0][0]]\n",
    "        deduplicated_rules_of_arity[arity].append(rule)\n",
    "\n",
    "    deduplicated_rules_of_arity = dict(deduplicated_rules_of_arity)\n",
    "\n",
    "    rules_trees = {a: [\n",
    "        (\n",
    "            prefix_to_tree(list(rule[0]), space.operator_arity_compat),\n",
    "            prefix_to_tree(list(rule[1]), space.operator_arity_compat)\n",
    "        )\n",
    "            for rule in deduplicated_rules_of_arity_a] for a, deduplicated_rules_of_arity_a in deduplicated_rules_of_arity.items()}\n",
    "    \n",
    "    return rules_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*', '-', 'x2', 'x2', 'x3']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtree_simplify(['*', '-', 'x2', 'x2', 'x3'], rules_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-', '*', 'x2', 'x2', '*', 'x2', 'x2']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtree_simplify(['-', '*', 'x2', 'x2', '*', 'x2', 'x2'], rules_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtree_simplify(['x1'], rules_trees, max_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [35:04<00:00,  4.75it/s, Rules found: 406, Current Expression: ('pow1_3', '-', 'x5', '(-1)') -> ['pow1_3', '-', 'x5', '(-1)'] -> ...]                 \n"
     ]
    }
   ],
   "source": [
    "size = 200_000\n",
    "constants_retries = 5\n",
    "max_simplify_steps = 5\n",
    "# timeout_seconds = 60 * 60 * 10  # 10 hours\n",
    "\n",
    "rules = []\n",
    "rules_trees = {}\n",
    "\n",
    "hashes_of_size = defaultdict(list)\n",
    "\n",
    "dummy_variables = [f\"x{i}\" for i in range(10)]\n",
    "\n",
    "X = np.random.normal(loc=0, scale=5, size=(1024, len(dummy_variables)))\n",
    "C = np.random.normal(loc=0, scale=5, size=128)\n",
    "\n",
    "leaf_nodes = dummy_variables + [\"<num>\"] + ['0', '1', '2', '(-1)', '(-2)', 'float(\"inf\")', 'float(\"-inf\")', 'float(\"nan\")']\n",
    "non_leaf_nodes = dict(sorted(space.operator_arity.items(), key=lambda x: x[1]))\n",
    "\n",
    "pbar = tqdm(total=size)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        # Create all leaf nodes\n",
    "        for leaf in leaf_nodes[:size]:\n",
    "            simplified_skeleton = subtree_simplify([leaf], rules_trees, max_iter=max_simplify_steps)\n",
    "            \n",
    "            executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "            prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "            code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "            code = codify(code_string, dummy_variables + constants)\n",
    "\n",
    "            hashes_of_size[len(simplified_skeleton)].append(tuple(simplified_skeleton))\n",
    "\n",
    "        n_scanned = 0\n",
    "\n",
    "        while n_scanned < size:\n",
    "            simplified_hashes_of_size = defaultdict(set)\n",
    "            for l, hashes_list in hashes_of_size.items():\n",
    "                for h in hashes_list:\n",
    "                    simplified_skeleton = subtree_simplify(h, rules_trees, max_iter=max_simplify_steps)\n",
    "                    simplified_hashes_of_size[len(simplified_skeleton)].add(tuple(simplified_skeleton))\n",
    "            hashes_of_size = {l: list(h) for l, h in simplified_hashes_of_size.items()}\n",
    "\n",
    "            new_hashes_of_size = defaultdict(list)\n",
    "            for combination in expression_generator(hashes_of_size, non_leaf_nodes):\n",
    "                # TODO: Think about when to simplify the rules\n",
    "                for i, rule in enumerate(rules):\n",
    "                    rules[i] = (rule[0], subtree_simplify(rule[1], rules_trees, max_iter=max_simplify_steps))\n",
    "\n",
    "                rules = deduplicate_rules(rules, dummy_variables)\n",
    "                rules_trees = rules_trees_from_rules_list(rules, dummy_variables)\n",
    "\n",
    "                simplified_skeleton = subtree_simplify(list(combination), rules_trees, max_iter=max_simplify_steps)\n",
    "                h = tuple(simplified_skeleton)\n",
    "\n",
    "                pbar.set_postfix_str(f\"Rules found: {len(rules):,}, Current Expression: {combination} -> {simplified_skeleton} -> ...\")\n",
    "\n",
    "                executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "                prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "                code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "                code = codify(code_string, dummy_variables + constants)\n",
    "                \n",
    "                f = space.code_to_lambda(code)\n",
    "\n",
    "                # Record the image\n",
    "                if len(constants) == 0:\n",
    "                    y = safe_f(f, X)\n",
    "                    if not isinstance(y, np.ndarray):\n",
    "                        y = np.full(X.shape[0], y)\n",
    "\n",
    "                    new_rule_candidates = []\n",
    "                    for candidate_hashes_of_size in (hashes_of_size, new_hashes_of_size):\n",
    "                        for l, candidate_hashes_list in candidate_hashes_of_size.items():\n",
    "                            # Ignore simplification candidates that do not shorten the expression\n",
    "                            if l >= len(h):\n",
    "                                continue\n",
    "\n",
    "                            for candidate_hash in candidate_hashes_list:\n",
    "                                if candidate_hash == h:\n",
    "                                    continue\n",
    "                                executable_prefix_candidate_hash = space.operators_to_realizations(candidate_hash)\n",
    "                                prefix_candidate_hash_with_constants, constants_candidate_hash = num_to_constants(executable_prefix_candidate_hash, convert_numbers_to_constant=False)\n",
    "                                code_string_candidate_hash = space.prefix_to_infix(prefix_candidate_hash_with_constants, realization=True)\n",
    "                                code_candidate_hash = codify(code_string_candidate_hash, dummy_variables + constants_candidate_hash)\n",
    "\n",
    "                                # Record the image\n",
    "                                if len(constants_candidate_hash) == 0:\n",
    "                                    f_candidate = space.code_to_lambda(code_candidate_hash)\n",
    "                                    y_candidate = safe_f(f_candidate, X)\n",
    "                                    if not isinstance(y_candidate, np.ndarray):\n",
    "                                        y_candidate = np.full(X.shape[0], y_candidate)\n",
    "\n",
    "                                    if np.allclose(y, y_candidate, equal_nan=True):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                                else:\n",
    "                                    if any([exist_constants_that_fit(candidate_hash, dummy_variables, X, y) for _ in range(constants_retries)]):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                            \n",
    "                    # Find the shortest rule\n",
    "                    if len(new_rule_candidates) > 0:\n",
    "                        new_rule_candidates = sorted(new_rule_candidates, key=lambda x: len(x[1]))\n",
    "                        new_rule_candidates_of_minimum_length = [c for c in new_rule_candidates if len(c[1]) == len(new_rule_candidates[0][1])]\n",
    "                        # If there are rules with and without <num>, prefer the ones without\n",
    "                        new_rule_candidates_of_minimum_length_without_num = [c for c in new_rule_candidates_of_minimum_length if '<num>' not in c[1]]\n",
    "                        if len(new_rule_candidates_of_minimum_length_without_num) > 0:\n",
    "                            new_rule_candidates_of_minimum_length = new_rule_candidates_of_minimum_length_without_num\n",
    "                        rules.append(new_rule_candidates_of_minimum_length[0])\n",
    "\n",
    "                else:\n",
    "                    # Create an image from X and randomly sampled constants\n",
    "                    y = safe_f(f, X, C[:len(constants)])\n",
    "\n",
    "                    new_rule_candidates = []\n",
    "                    for candidate_hashes_of_size in (hashes_of_size, new_hashes_of_size):\n",
    "                        for l, candidate_hashes_list in candidate_hashes_of_size.items():\n",
    "                            # Ignore simplification candidates that do not shorten the expression\n",
    "                            if l >= len(h):\n",
    "                                continue\n",
    "\n",
    "                            for candidate_hash in candidate_hashes_list:\n",
    "                                if candidate_hash == h:\n",
    "                                    continue\n",
    "                                executable_prefix_candidate_hash = space.operators_to_realizations(candidate_hash)\n",
    "                                prefix_candidate_hash_with_constants, constants_candidate_hash = num_to_constants(executable_prefix_candidate_hash, convert_numbers_to_constant=False)\n",
    "                                code_string_candidate_hash = space.prefix_to_infix(prefix_candidate_hash_with_constants, realization=True)\n",
    "                                code_candidate_hash = codify(code_string_candidate_hash, dummy_variables + constants_candidate_hash)\n",
    "\n",
    "                                f_candidate = space.code_to_lambda(code_candidate_hash)\n",
    "                                \n",
    "                                # Record the image\n",
    "                                if len(constants_candidate_hash) == 0:\n",
    "                                    y_candidate = safe_f(f_candidate, X)                                \n",
    "                                    if not isinstance(y_candidate, np.ndarray):\n",
    "                                        y_candidate = np.full(X.shape[0], y_candidate)\n",
    "\n",
    "                                    if np.allclose(y, y_candidate, equal_nan=True):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                                else:\n",
    "                                    if any([exist_constants_that_fit(candidate_hash, dummy_variables, X, y) for _ in range(constants_retries)]):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "\n",
    "                    # Find the shortest rule\n",
    "                    if len(new_rule_candidates) > 0:\n",
    "                        new_rule_candidates = sorted(new_rule_candidates, key=lambda x: len(x[1]))\n",
    "                        new_rule_candidates_of_minimum_length = [c for c in new_rule_candidates if len(c[1]) == len(new_rule_candidates[0][1])]\n",
    "                        # If there are rules with and without <num>, prefer the ones without\n",
    "                        new_rule_candidates_of_minimum_length_without_num = [c for c in new_rule_candidates_of_minimum_length if '<num>' not in c[1]]\n",
    "                        if len(new_rule_candidates_of_minimum_length_without_num) > 0:\n",
    "                            new_rule_candidates_of_minimum_length = new_rule_candidates_of_minimum_length_without_num\n",
    "                        rules.append(new_rule_candidates_of_minimum_length[0])\n",
    "                    \n",
    "\n",
    "                new_hashes_of_size[len(h)].append(h)\n",
    "\n",
    "                n_scanned += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                if n_scanned >= size:\n",
    "                    break\n",
    "\n",
    "            hashes_of_size.update(new_hashes_of_size)\n",
    "\n",
    "        pbar.close()\n",
    "except:\n",
    "    pbar.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the rules one last time\n",
    "for i, rule in enumerate(rules):\n",
    "    rules[i] = (rule[0], subtree_simplify(rule[1], rules_trees, max_iter=max_simplify_steps))\n",
    "\n",
    "rules_trees = rules_trees_from_rules_list(rules, dummy_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtree_simplify(['-', 'x1', 'x1'], rules_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the rules to a file\n",
    "with open(f'./rules_constants_{size}.txt', 'w') as f:\n",
    "    for rule in rules:\n",
    "        f.write(f\"{rule[0]} -> {rule[1]}\\n\")\n",
    "\n",
    "with open(f'./rules_constants_{size}.json', 'w') as f:\n",
    "    json.dump(rules, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-ansr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
