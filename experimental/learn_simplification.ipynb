{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_ansr.expressions import ExpressionSpace\n",
    "from flash_ansr.expressions.utils import codify, num_to_constants\n",
    "from flash_ansr import get_path\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Generator, Callable\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'v7.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_path('configs', MODEL, 'expression_space.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = ExpressionSpace.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'x3', '<num>']\n",
      "{'neg': 1, 'abs': 1, 'inv': 1, 'pow2': 1, 'pow3': 1, 'pow4': 1, 'pow5': 1, 'pow1_2': 1, 'pow1_3': 1, 'pow1_4': 1, 'pow1_5': 1, 'sin': 1, 'cos': 1, 'tan': 1, 'asin': 1, 'acos': 1, 'atan': 1, 'exp': 1, 'log': 1, '+': 2, '-': 2, '*': 2, '/': 2}\n"
     ]
    }
   ],
   "source": [
    "leaf_nodes = space.variables + [\"<num>\"]\n",
    "non_leaf_nodes = space.operator_arity\n",
    "non_leaf_nodes = dict(sorted(non_leaf_nodes.items(), key=lambda x: x[1]))\n",
    "\n",
    "print(leaf_nodes)\n",
    "print(non_leaf_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+', 'cos', 'x', 'sin', '*', 'pow2', 'x', 'pow2', 'x']\n"
     ]
    }
   ],
   "source": [
    "def apply_rule(X, A, B):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        # Check if sublist A is found at current position\n",
    "        if i <= len(X) - len(A) and X[i:i+len(A)] == A:\n",
    "            # Add replacement sublist B\n",
    "            result.extend(B)\n",
    "            # Skip past the matched sublist A\n",
    "            i += len(A)\n",
    "        else:\n",
    "            # Add current element and move to next\n",
    "            result.append(X[i])\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "X = ['+', 'cos', 'x', 'sin', '*', '*', 'x', 'x', '*', 'x', 'x']\n",
    "A = ['*', 'x', 'x']\n",
    "B = ['pow2', 'x']\n",
    "print(apply_rule(X, A, B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(expression: list[str], rules: set[tuple[list[str], list[str]]], max_iter: int = 1) -> list[str]:\n",
    "    if isinstance(expression, tuple):\n",
    "        expression = list(expression)\n",
    "\n",
    "    previous_expression = None\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        for pattern, replacement in rules:\n",
    "            expression = apply_rule(expression, pattern, replacement)\n",
    "        if previous_expression == expression:\n",
    "            break\n",
    "        previous_expression = expression\n",
    "\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', 'cos', 'x', 'sin', '*', 'pow2', 'x', 'pow2', 'x']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplify(['+', 'cos', 'x', 'sin', '*', '*', 'x', 'x', '*', 'x', 'x'], [(['*', 'x', 'x'], ['pow2', 'x'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', '0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplify(['neg', '-', 'x1', 'x1'], [(['-', 'x1', 'x1'], ['0'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<num>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplify(['<num>'], rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expression_generator(hashes_of_size: dict[int, list[tuple[str]]], non_leaf_nodes: dict[str, int]) -> Generator[tuple[str], None, None]:\n",
    "    # Append existing trees to every operator\n",
    "    for new_root_operator, arity in non_leaf_nodes.items():\n",
    "        # Start with the smallest arity-tuples of trees\n",
    "        for child_lengths in sorted(itertools.product(list(hashes_of_size.keys()), repeat=arity), key=lambda x: sum(x)):\n",
    "            # Check all possible combinations of child trees\n",
    "            for child_combination in itertools.product(*[hashes_of_size[child_length] for child_length in child_lengths]):\n",
    "                yield (new_root_operator,) + tuple(itertools.chain.from_iterable(child_combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(loc=0, scale=5, size=(1024, space.n_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "size = 0\n",
    "rules = []\n",
    "\n",
    "hashes_of_size = defaultdict(list)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    # Create all leaf nodes\n",
    "    for leaf in leaf_nodes[:size]:\n",
    "        simplified_skeleton = simplify([leaf], rules)\n",
    "        \n",
    "        executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "        prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression)\n",
    "        code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "        code = codify(code_string, space.variables + constants)\n",
    "\n",
    "        hashes_of_size[len(simplified_skeleton)].append(tuple(simplified_skeleton))\n",
    "\n",
    "    pbar = tqdm(total=size)\n",
    "    n_scanned = 0\n",
    "\n",
    "    while n_scanned < size:\n",
    "        simplified_hashes_of_size = defaultdict(list)\n",
    "        for l, hashes_list in hashes_of_size.items():\n",
    "            for h in hashes_list:\n",
    "                simplified_skeleton = simplify(h, rules)\n",
    "                simplified_hashes_of_size[len(simplified_skeleton)].append(simplified_skeleton)\n",
    "        hashes_of_size = simplified_hashes_of_size\n",
    "\n",
    "        new_hashes_of_size = defaultdict(list)\n",
    "        for combination in expression_generator(hashes_of_size, non_leaf_nodes):\n",
    "            for i, rule in enumerate(rules):\n",
    "                rules[i] = (rule[0], simplify(rule[1], rules))\n",
    "\n",
    "            simplified_skeleton = simplify(list(combination), rules)\n",
    "            h = tuple(simplified_skeleton)\n",
    "\n",
    "            executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "            prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression)\n",
    "            code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "            code = codify(code_string, space.variables + constants)\n",
    "\n",
    "            # Record the image\n",
    "            if len(constants) == 0:\n",
    "                f = space.code_to_lambda(code)\n",
    "                y = f(*X.T)\n",
    "\n",
    "                for candidate_hashes_of_size in (hashes_of_size, new_hashes_of_size):\n",
    "                    for l, candidate_hashes_list in candidate_hashes_of_size.items():\n",
    "                        # Ignore simplification candidates that do not shorten the expression\n",
    "                        if l >= len(h):\n",
    "                            continue\n",
    "\n",
    "                        for candidate_hash in candidate_hashes_list:\n",
    "                            if candidate_hash == h:\n",
    "                                continue\n",
    "                            executable_prefix_candidate_hash = space.operators_to_realizations(candidate_hash)\n",
    "                            prefix_candidate_hash_with_constants, constants_candidate_hash = num_to_constants(executable_prefix_candidate_hash)\n",
    "                            code_string_candidate_hash = space.prefix_to_infix(prefix_candidate_hash_with_constants, realization=True)\n",
    "                            code_candidate_hash = codify(code_string_candidate_hash, space.variables + constants_candidate_hash)\n",
    "\n",
    "                            # Record the image\n",
    "                            if len(constants_candidate_hash) == 0:\n",
    "                                f_candidate = space.code_to_lambda(code_candidate_hash)\n",
    "                                y_candidate = f_candidate(*X.T)\n",
    "\n",
    "                                if np.allclose(y, y_candidate, equal_nan=True):\n",
    "                                    rules.append((simplified_skeleton, list(candidate_hash)))\n",
    "\n",
    "            new_hashes_of_size[len(h)].append(h)\n",
    "\n",
    "            n_scanned += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Rules found: {len(rules):,}\")\n",
    "\n",
    "            if n_scanned >= size:\n",
    "                break\n",
    "\n",
    "        hashes_of_size.update(new_hashes_of_size)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "# Write the rules to a file\n",
    "with open('./rules.txt', 'w') as f:\n",
    "    for rule in rules:\n",
    "        f.write(f\"{rule[0]} -> {rule[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit, OptimizeWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exist_constants_that_fit(expression: list[str], variables: list[str], X: np.ndarray, y_target: np.ndarray, debug: bool = False):\n",
    "    if isinstance(expression, tuple):\n",
    "        expression = list(expression)\n",
    "\n",
    "    executable_prefix_expression = space.operators_to_realizations(expression)\n",
    "    prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "    code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "    code = codify(code_string, variables + constants)\n",
    "    f = space.code_to_lambda(code)\n",
    "\n",
    "    def pred_function(X: np.ndarray, *constants: np.ndarray | None) -> float:\n",
    "        if len(constants) == 0:\n",
    "            y = f(*X.T)\n",
    "        y =  f(*X.T, *constants)\n",
    "\n",
    "        # If the numbers are complex, return nan\n",
    "        if np.iscomplexobj(y):\n",
    "            return np.full(y.shape, np.nan)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    p0 = np.random.normal(loc=0, scale=5, size=len(constants))\n",
    "\n",
    "    is_valid = np.isfinite(X).all(axis=1) & np.isfinite(y_target)\n",
    "\n",
    "    if not np.any(is_valid):\n",
    "        if debug:\n",
    "            print(\"No valid data points\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=OptimizeWarning)\n",
    "            popt, _ = curve_fit(pred_function, X[is_valid], y_target[is_valid].flatten(), p0=p0)\n",
    "    except RuntimeError:\n",
    "        if debug:\n",
    "            print(\"RuntimeError\")\n",
    "        return False\n",
    "\n",
    "    y = f(*X.T, *popt)\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.full(X.shape[0], y)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Constants: {popt}\")\n",
    "        print(f\"y_target: {y_target}\")\n",
    "        print(f\"y: {y}\")\n",
    "        print(f'All close: {np.allclose(y_target, y, equal_nan=True)}')\n",
    "\n",
    "    return np.allclose(y_target, y, equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "C = np.random.normal(loc=0, scale=5, size=128)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_f(f, X, constants = None):\n",
    "    try:\n",
    "        if constants is None:\n",
    "            y = f(*X.T)\n",
    "        else:\n",
    "            y = f(*X.T, *constants)\n",
    "        if not isinstance(y, np.ndarray) or y.shape[0] == 1:\n",
    "            y = np.full(X.shape[0], y)\n",
    "        return y\n",
    "    except ZeroDivisionError:\n",
    "        return np.full(X.shape[0], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s, Rules found: 0, Current Expression: ('neg', '<num>') -> ['neg', '<num>'] -> ...]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exist_constants_that_fit() missing 1 required positional argument: 'y_target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m                     new_rule_candidates\u001b[38;5;241m.\u001b[39mappend((simplified_skeleton, \u001b[38;5;28mlist\u001b[39m(candidate_hash)))\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([exist_constants_that_fit(candidate_hash, X, y) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(constants_retries)]):\n\u001b[1;32m    131\u001b[0m                     new_rule_candidates\u001b[38;5;241m.\u001b[39mappend((simplified_skeleton, \u001b[38;5;28mlist\u001b[39m(candidate_hash)))\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Find the shortest rule\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 130\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     new_rule_candidates\u001b[38;5;241m.\u001b[39mappend((simplified_skeleton, \u001b[38;5;28mlist\u001b[39m(candidate_hash)))\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([exist_constants_that_fit(candidate_hash, X, y) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(constants_retries)]):\n\u001b[1;32m    131\u001b[0m                     new_rule_candidates\u001b[38;5;241m.\u001b[39mappend((simplified_skeleton, \u001b[38;5;28mlist\u001b[39m(candidate_hash)))\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Find the shortest rule\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: exist_constants_that_fit() missing 1 required positional argument: 'y_target'"
     ]
    }
   ],
   "source": [
    "size = 10_000\n",
    "constants_retries = 5\n",
    "max_simplify_steps = 5\n",
    "rules = []\n",
    "\n",
    "hashes_of_size = defaultdict(list)\n",
    "\n",
    "leaf_nodes = space.variables + [\"<num>\"] + ['0', '1', '2', '(-1)', '(-2)', 'float(\"inf\")', 'float(\"-inf\")', 'float(\"nan\")']\n",
    "\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        # Create all leaf nodes\n",
    "        for leaf in leaf_nodes[:size]:\n",
    "            simplified_skeleton = simplify([leaf], rules, max_iter=max_simplify_steps)\n",
    "            \n",
    "            executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "            prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "            code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "            code = codify(code_string, space.variables + constants)\n",
    "\n",
    "            hashes_of_size[len(simplified_skeleton)].append(tuple(simplified_skeleton))\n",
    "\n",
    "        pbar = tqdm(total=size)\n",
    "        n_scanned = 0\n",
    "\n",
    "        while n_scanned < size:\n",
    "            simplified_hashes_of_size = defaultdict(set)\n",
    "            for l, hashes_list in hashes_of_size.items():\n",
    "                for h in hashes_list:\n",
    "                    simplified_skeleton = simplify(h, rules, max_iter=max_simplify_steps)\n",
    "                    simplified_hashes_of_size[len(simplified_skeleton)].add(tuple(simplified_skeleton))\n",
    "            hashes_of_size = {l: list(h) for l, h in simplified_hashes_of_size.items()}\n",
    "\n",
    "            new_hashes_of_size = defaultdict(list)\n",
    "            for combination in expression_generator(hashes_of_size, non_leaf_nodes):\n",
    "                for i, rule in enumerate(rules):\n",
    "                    rules[i] = (rule[0], simplify(rule[1], rules, max_iter=max_simplify_steps))\n",
    "\n",
    "                simplified_skeleton = simplify(list(combination), rules, max_iter=max_simplify_steps)\n",
    "                h = tuple(simplified_skeleton)\n",
    "\n",
    "                pbar.set_postfix_str(f\"Rules found: {len(rules):,}, Current Expression: {combination} -> {simplified_skeleton} -> ...\")\n",
    "\n",
    "                executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "                prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "                code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "                code = codify(code_string, space.variables + constants)\n",
    "                \n",
    "                f = space.code_to_lambda(code)\n",
    "\n",
    "                # Record the image\n",
    "                if len(constants) == 0:\n",
    "                    y = safe_f(f, X)\n",
    "                    if not isinstance(y, np.ndarray):\n",
    "                        y = np.full(X.shape[0], y)\n",
    "\n",
    "                    new_rule_candidates = []\n",
    "                    for candidate_hashes_of_size in (hashes_of_size, new_hashes_of_size):\n",
    "                        for l, candidate_hashes_list in candidate_hashes_of_size.items():\n",
    "                            # Ignore simplification candidates that do not shorten the expression\n",
    "                            if l >= len(h):\n",
    "                                continue\n",
    "\n",
    "                            for candidate_hash in candidate_hashes_list:\n",
    "                                if candidate_hash == h:\n",
    "                                    continue\n",
    "                                executable_prefix_candidate_hash = space.operators_to_realizations(candidate_hash)\n",
    "                                prefix_candidate_hash_with_constants, constants_candidate_hash = num_to_constants(executable_prefix_candidate_hash, convert_numbers_to_constant=False)\n",
    "                                code_string_candidate_hash = space.prefix_to_infix(prefix_candidate_hash_with_constants, realization=True)\n",
    "                                code_candidate_hash = codify(code_string_candidate_hash, space.variables + constants_candidate_hash)\n",
    "\n",
    "                                # Record the image\n",
    "                                if len(constants_candidate_hash) == 0:\n",
    "                                    f_candidate = space.code_to_lambda(code_candidate_hash)\n",
    "                                    y_candidate = safe_f(f_candidate, X)\n",
    "                                    if not isinstance(y_candidate, np.ndarray):\n",
    "                                        y_candidate = np.full(X.shape[0], y_candidate)\n",
    "\n",
    "                                    if np.allclose(y, y_candidate, equal_nan=True):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                                else:\n",
    "                                    if any([exist_constants_that_fit(candidate_hash, X, y) for _ in range(constants_retries)]):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                            \n",
    "                    # Find the shortest rule\n",
    "                    if len(new_rule_candidates) > 0:\n",
    "                        new_rule_candidates = sorted(new_rule_candidates, key=lambda x: len(x[1]))\n",
    "                        new_rule_candidates_of_minimum_length = [c for c in new_rule_candidates if len(c[1]) == len(new_rule_candidates[0][1])]\n",
    "                        # If there are rules with and without <num>, prefer the ones without\n",
    "                        new_rule_candidates_of_minimum_length_without_num = [c for c in new_rule_candidates_of_minimum_length if '<num>' not in c[1]]\n",
    "                        if len(new_rule_candidates_of_minimum_length_without_num) > 0:\n",
    "                            new_rule_candidates_of_minimum_length = new_rule_candidates_of_minimum_length_without_num\n",
    "                        rules.append(new_rule_candidates_of_minimum_length[0])\n",
    "\n",
    "                else:\n",
    "                    # Create an image from X and randomly sampled constants\n",
    "                    y = safe_f(f, X, C[:len(constants)])\n",
    "\n",
    "                    new_rule_candidates = []\n",
    "                    for candidate_hashes_of_size in (hashes_of_size, new_hashes_of_size):\n",
    "                        for l, candidate_hashes_list in candidate_hashes_of_size.items():\n",
    "                            # Ignore simplification candidates that do not shorten the expression\n",
    "                            if l >= len(h):\n",
    "                                continue\n",
    "\n",
    "                            for candidate_hash in candidate_hashes_list:\n",
    "                                if candidate_hash == h:\n",
    "                                    continue\n",
    "                                executable_prefix_candidate_hash = space.operators_to_realizations(candidate_hash)\n",
    "                                prefix_candidate_hash_with_constants, constants_candidate_hash = num_to_constants(executable_prefix_candidate_hash, convert_numbers_to_constant=False)\n",
    "                                code_string_candidate_hash = space.prefix_to_infix(prefix_candidate_hash_with_constants, realization=True)\n",
    "                                code_candidate_hash = codify(code_string_candidate_hash, space.variables + constants_candidate_hash)\n",
    "\n",
    "                                f_candidate = space.code_to_lambda(code_candidate_hash)\n",
    "                                \n",
    "                                # Record the image\n",
    "                                if len(constants_candidate_hash) == 0:\n",
    "                                    y_candidate = safe_f(f_candidate, X)\n",
    "                                    if combination == ('*', '<num>', '<num>') and candidate_hash == ('<num>',):\n",
    "                                        print(y_candidate)\n",
    "                                \n",
    "                                    if not isinstance(y_candidate, np.ndarray):\n",
    "                                        y_candidate = np.full(X.shape[0], y_candidate)\n",
    "\n",
    "                                    if np.allclose(y, y_candidate, equal_nan=True):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                                else:\n",
    "                                    if any([exist_constants_that_fit(candidate_hash, X, y) for _ in range(constants_retries)]):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "\n",
    "                    # Find the shortest rule\n",
    "                    if len(new_rule_candidates) > 0:\n",
    "                        new_rule_candidates = sorted(new_rule_candidates, key=lambda x: len(x[1]))\n",
    "                        new_rule_candidates_of_minimum_length = [c for c in new_rule_candidates if len(c[1]) == len(new_rule_candidates[0][1])]\n",
    "                        # If there are rules with and without <num>, prefer the ones without\n",
    "                        new_rule_candidates_of_minimum_length_without_num = [c for c in new_rule_candidates_of_minimum_length if '<num>' not in c[1]]\n",
    "                        if len(new_rule_candidates_of_minimum_length_without_num) > 0:\n",
    "                            new_rule_candidates_of_minimum_length = new_rule_candidates_of_minimum_length_without_num\n",
    "                        rules.append(new_rule_candidates_of_minimum_length[0])\n",
    "\n",
    "                new_hashes_of_size[len(h)].append(h)\n",
    "\n",
    "                n_scanned += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                if n_scanned >= size:\n",
    "                    break\n",
    "\n",
    "            hashes_of_size.update(new_hashes_of_size)\n",
    "\n",
    "        pbar.close()\n",
    "except:\n",
    "    pbar.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.97it/s, Rules found: 468, Current Expression: ('-', '1', '2') -> ['-', '1', '2'] -> ...]                                               \n"
     ]
    }
   ],
   "source": [
    "size = 1_000\n",
    "constants_retries = 5\n",
    "max_simplify_steps = 5\n",
    "rules = []\n",
    "\n",
    "hashes_of_size = defaultdict(list)\n",
    "\n",
    "dummy_variables = [f\"x{i}\" for i in range(10)]\n",
    "\n",
    "X = np.random.normal(loc=0, scale=5, size=(1024, len(dummy_variables)))\n",
    "C = np.random.normal(loc=0, scale=5, size=128)\n",
    "\n",
    "leaf_nodes = dummy_variables + [\"<num>\"] + ['0', '1', '2', '(-1)', '(-2)', 'float(\"inf\")', 'float(\"-inf\")', 'float(\"nan\")']\n",
    "\n",
    "try:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        # Create all leaf nodes\n",
    "        for leaf in leaf_nodes[:size]:\n",
    "            simplified_skeleton = simplify([leaf], rules, max_iter=max_simplify_steps)\n",
    "            \n",
    "            executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "            prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "            code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "            code = codify(code_string, dummy_variables + constants)\n",
    "\n",
    "            hashes_of_size[len(simplified_skeleton)].append(tuple(simplified_skeleton))\n",
    "\n",
    "        pbar = tqdm(total=size)\n",
    "        n_scanned = 0\n",
    "\n",
    "        while n_scanned < size:\n",
    "            simplified_hashes_of_size = defaultdict(set)\n",
    "            for l, hashes_list in hashes_of_size.items():\n",
    "                for h in hashes_list:\n",
    "                    simplified_skeleton = simplify(h, rules, max_iter=max_simplify_steps)\n",
    "                    simplified_hashes_of_size[len(simplified_skeleton)].add(tuple(simplified_skeleton))\n",
    "            hashes_of_size = {l: list(h) for l, h in simplified_hashes_of_size.items()}\n",
    "\n",
    "            new_hashes_of_size = defaultdict(list)\n",
    "            for combination in expression_generator(hashes_of_size, non_leaf_nodes):\n",
    "                for i, rule in enumerate(rules):\n",
    "                    rules[i] = (rule[0], simplify(rule[1], rules, max_iter=max_simplify_steps))\n",
    "\n",
    "                simplified_skeleton = simplify(list(combination), rules, max_iter=max_simplify_steps)\n",
    "                h = tuple(simplified_skeleton)\n",
    "\n",
    "                pbar.set_postfix_str(f\"Rules found: {len(rules):,}, Current Expression: {combination} -> {simplified_skeleton} -> ...\")\n",
    "\n",
    "                executable_prefix_expression = space.operators_to_realizations(simplified_skeleton)\n",
    "                prefix_expression_with_constants, constants = num_to_constants(executable_prefix_expression, convert_numbers_to_constant=False)\n",
    "                code_string = space.prefix_to_infix(prefix_expression_with_constants, realization=True)\n",
    "                code = codify(code_string, dummy_variables + constants)\n",
    "                \n",
    "                f = space.code_to_lambda(code)\n",
    "\n",
    "                # Record the image\n",
    "                if len(constants) == 0:\n",
    "                    y = safe_f(f, X)\n",
    "                    if not isinstance(y, np.ndarray):\n",
    "                        y = np.full(X.shape[0], y)\n",
    "\n",
    "                    new_rule_candidates = []\n",
    "                    for candidate_hashes_of_size in (hashes_of_size, new_hashes_of_size):\n",
    "                        for l, candidate_hashes_list in candidate_hashes_of_size.items():\n",
    "                            # Ignore simplification candidates that do not shorten the expression\n",
    "                            if l >= len(h):\n",
    "                                continue\n",
    "\n",
    "                            for candidate_hash in candidate_hashes_list:\n",
    "                                if candidate_hash == h:\n",
    "                                    continue\n",
    "                                executable_prefix_candidate_hash = space.operators_to_realizations(candidate_hash)\n",
    "                                prefix_candidate_hash_with_constants, constants_candidate_hash = num_to_constants(executable_prefix_candidate_hash, convert_numbers_to_constant=False)\n",
    "                                code_string_candidate_hash = space.prefix_to_infix(prefix_candidate_hash_with_constants, realization=True)\n",
    "                                code_candidate_hash = codify(code_string_candidate_hash, dummy_variables + constants_candidate_hash)\n",
    "\n",
    "                                # Record the image\n",
    "                                if len(constants_candidate_hash) == 0:\n",
    "                                    f_candidate = space.code_to_lambda(code_candidate_hash)\n",
    "                                    y_candidate = safe_f(f_candidate, X)\n",
    "                                    if not isinstance(y_candidate, np.ndarray):\n",
    "                                        y_candidate = np.full(X.shape[0], y_candidate)\n",
    "\n",
    "                                    if np.allclose(y, y_candidate, equal_nan=True):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                                else:\n",
    "                                    if any([exist_constants_that_fit(candidate_hash, dummy_variables, X, y) for _ in range(constants_retries)]):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                            \n",
    "                    # Find the shortest rule\n",
    "                    if len(new_rule_candidates) > 0:\n",
    "                        new_rule_candidates = sorted(new_rule_candidates, key=lambda x: len(x[1]))\n",
    "                        new_rule_candidates_of_minimum_length = [c for c in new_rule_candidates if len(c[1]) == len(new_rule_candidates[0][1])]\n",
    "                        # If there are rules with and without <num>, prefer the ones without\n",
    "                        new_rule_candidates_of_minimum_length_without_num = [c for c in new_rule_candidates_of_minimum_length if '<num>' not in c[1]]\n",
    "                        if len(new_rule_candidates_of_minimum_length_without_num) > 0:\n",
    "                            new_rule_candidates_of_minimum_length = new_rule_candidates_of_minimum_length_without_num\n",
    "                        rules.append(new_rule_candidates_of_minimum_length[0])\n",
    "\n",
    "                else:\n",
    "                    # Create an image from X and randomly sampled constants\n",
    "                    y = safe_f(f, X, C[:len(constants)])\n",
    "\n",
    "                    new_rule_candidates = []\n",
    "                    for candidate_hashes_of_size in (hashes_of_size, new_hashes_of_size):\n",
    "                        for l, candidate_hashes_list in candidate_hashes_of_size.items():\n",
    "                            # Ignore simplification candidates that do not shorten the expression\n",
    "                            if l >= len(h):\n",
    "                                continue\n",
    "\n",
    "                            for candidate_hash in candidate_hashes_list:\n",
    "                                if candidate_hash == h:\n",
    "                                    continue\n",
    "                                executable_prefix_candidate_hash = space.operators_to_realizations(candidate_hash)\n",
    "                                prefix_candidate_hash_with_constants, constants_candidate_hash = num_to_constants(executable_prefix_candidate_hash, convert_numbers_to_constant=False)\n",
    "                                code_string_candidate_hash = space.prefix_to_infix(prefix_candidate_hash_with_constants, realization=True)\n",
    "                                code_candidate_hash = codify(code_string_candidate_hash, dummy_variables + constants_candidate_hash)\n",
    "\n",
    "                                f_candidate = space.code_to_lambda(code_candidate_hash)\n",
    "                                \n",
    "                                # Record the image\n",
    "                                if len(constants_candidate_hash) == 0:\n",
    "                                    y_candidate = safe_f(f_candidate, X)                                \n",
    "                                    if not isinstance(y_candidate, np.ndarray):\n",
    "                                        y_candidate = np.full(X.shape[0], y_candidate)\n",
    "\n",
    "                                    if np.allclose(y, y_candidate, equal_nan=True):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "                                else:\n",
    "                                    if any([exist_constants_that_fit(candidate_hash, dummy_variables, X, y) for _ in range(constants_retries)]):\n",
    "                                        new_rule_candidates.append((simplified_skeleton, list(candidate_hash)))\n",
    "\n",
    "                    # Find the shortest rule\n",
    "                    if len(new_rule_candidates) > 0:\n",
    "                        new_rule_candidates = sorted(new_rule_candidates, key=lambda x: len(x[1]))\n",
    "                        new_rule_candidates_of_minimum_length = [c for c in new_rule_candidates if len(c[1]) == len(new_rule_candidates[0][1])]\n",
    "                        # If there are rules with and without <num>, prefer the ones without\n",
    "                        new_rule_candidates_of_minimum_length_without_num = [c for c in new_rule_candidates_of_minimum_length if '<num>' not in c[1]]\n",
    "                        if len(new_rule_candidates_of_minimum_length_without_num) > 0:\n",
    "                            new_rule_candidates_of_minimum_length = new_rule_candidates_of_minimum_length_without_num\n",
    "                        rules.append(new_rule_candidates_of_minimum_length[0])\n",
    "\n",
    "                new_hashes_of_size[len(h)].append(h)\n",
    "\n",
    "                n_scanned += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                if n_scanned >= size:\n",
    "                    break\n",
    "\n",
    "            hashes_of_size.update(new_hashes_of_size)\n",
    "\n",
    "        pbar.close()\n",
    "except:\n",
    "    pbar.close()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_expression(source_expression, variable_mapping: dict | None = None):\n",
    "    if variable_mapping is None:\n",
    "        variable_mapping = {}\n",
    "        for i, token in enumerate(source_expression):\n",
    "            if token in dummy_variables:\n",
    "                if token not in variable_mapping:\n",
    "                    variable_mapping[token] = f'_{len(variable_mapping)}'\n",
    "    \n",
    "    for i, token in enumerate(source_expression):\n",
    "        if token in dummy_variables:\n",
    "            source_expression[i] = variable_mapping[token]\n",
    "\n",
    "    return source_expression, variable_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 469\n",
      "Number of unique rules: 331 (70.58%)\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate the rules\n",
    "deduplicated_rules = set()\n",
    "for rule in rules:\n",
    "    # Rename variables in the source expression\n",
    "    remapped_source, variable_mapping = remap_expression(list(rule[0]))\n",
    "    remapped_target, _ = remap_expression(list(rule[1]), variable_mapping)\n",
    "    deduplicated_rules.add((tuple(remapped_source), tuple(remapped_target)))\n",
    "\n",
    "print(f'Number of rules: {len(rules)}')\n",
    "print(f'Number of unique rules: {len(deduplicated_rules)} ({100 * len(deduplicated_rules) / len(rules):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_to_tree(expr, operator_arity):\n",
    "    \"\"\"\n",
    "    Convert a prefix notation expression to a nested list representing the expression tree.\n",
    "\n",
    "    Args:\n",
    "        expr: List representing the expression in prefix notation\n",
    "        operator_arity: Dictionary mapping operators to their arities\n",
    "\n",
    "    Returns:\n",
    "        Nested list representing the expression tree\n",
    "    \"\"\"\n",
    "    def build_tree(index):\n",
    "        if index >= len(expr):\n",
    "            return None, index\n",
    "\n",
    "        token = expr[index]\n",
    "\n",
    "        # If token is not an operator or is an operator with arity 0\n",
    "        if isinstance(token, dict) or token not in operator_arity or operator_arity[token] == 0:\n",
    "            return [token], index + 1\n",
    "\n",
    "        # If token is an operator\n",
    "        operands = []\n",
    "        current_index = index + 1\n",
    "\n",
    "        # Process operands based on the operator's arity\n",
    "        for _ in range(operator_arity[token]):\n",
    "            if current_index >= len(expr):\n",
    "                break\n",
    "\n",
    "            subtree, current_index = build_tree(current_index)\n",
    "            if subtree:\n",
    "                operands.append(subtree)\n",
    "\n",
    "        return [token, operands], current_index\n",
    "\n",
    "    result, _ = build_tree(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_to_tree(['x1'], space.operator_arity_compat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', [['x1'], ['x2']]]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_to_tree(['+', 'x1', 'x2'], space.operator_arity_compat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*', [['x1'], ['cos', [['x2']]]]]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "prefix_to_tree(['*', 'x1', 'cos', 'x2'], space.operator_arity_compat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_rules_of_arity = defaultdict(list)\n",
    "for rule in deduplicated_rules:\n",
    "    arity = space.operator_arity[rule[0][0]]\n",
    "    deduplicated_rules_of_arity[arity].append(rule)\n",
    "\n",
    "deduplicated_rules_of_arity = dict(deduplicated_rules_of_arity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_trees = {a: [\n",
    "    (\n",
    "        prefix_to_tree(list(rule[0]), space.operator_arity_compat),\n",
    "        prefix_to_tree(list(rule[1]), space.operator_arity_compat)\n",
    "    )\n",
    "        for rule in deduplicated_rules_of_arity_a] for a, deduplicated_rules_of_arity_a in deduplicated_rules_of_arity.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(tree: list, pattern: list, mapping: dict[str, Any] | None = None) -> tuple[bool, dict[str, Any]]:\n",
    "    print(f'tree: {tree}')\n",
    "    print(f'pattern: {pattern}')\n",
    "    print()\n",
    "\n",
    "    if mapping is None:\n",
    "        mapping = {}\n",
    "\n",
    "    if len(pattern) == 1 and isinstance(pattern[0], str):\n",
    "        if pattern[0].startswith('_'):\n",
    "            if pattern[0] not in mapping:\n",
    "                mapping[pattern[0]] = tree\n",
    "            elif mapping[pattern[0]] != tree:\n",
    "                return False, mapping\n",
    "            return True, mapping\n",
    "        \n",
    "        if tree != pattern:\n",
    "            return False, mapping\n",
    "        return True, mapping\n",
    "\n",
    "    print(f'tree: {tree}')\n",
    "    print(f'pattern: {pattern}')\n",
    "    tree_operator, tree_operands = tree\n",
    "    print(f'tree_operator: {tree_operator}')\n",
    "    print(f'tree_operands: {tree_operands}')\n",
    "    pattern_operator, pattern_operands = pattern\n",
    "    print(f'pattern_operator: {pattern_operator}')\n",
    "    print(f'pattern_operands: {pattern_operands}')\n",
    "    print()\n",
    "\n",
    "    if tree_operator != pattern_operator:\n",
    "        return False, mapping\n",
    "    \n",
    "    for tree_operand, pattern_operand in zip(tree_operands, pattern_operands):\n",
    "        if isinstance(pattern_operand, str):\n",
    "            print(f'pattern_operand: {pattern_operand}')\n",
    "            if pattern_operand not in mapping:\n",
    "                mapping[pattern_operand] = tree_operand\n",
    "            elif mapping[pattern_operand] != tree_operand:\n",
    "                return False, mapping\n",
    "        else:\n",
    "            does_match, mapping = pattern_match(tree_operand, pattern_operand, mapping)\n",
    "            if not does_match:\n",
    "                return False, mapping\n",
    "\n",
    "    return True, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: ['+', [['cos', [['x3']]], ['x2']]]\n",
      "pattern: ['+', [['cos', [['_1']]], ['_2']]]\n",
      "\n",
      "tree: ['+', [['cos', [['x3']]], ['x2']]]\n",
      "pattern: ['+', [['cos', [['_1']]], ['_2']]]\n",
      "tree_operator: +\n",
      "tree_operands: [['cos', [['x3']]], ['x2']]\n",
      "pattern_operator: +\n",
      "pattern_operands: [['cos', [['_1']]], ['_2']]\n",
      "\n",
      "tree: ['cos', [['x3']]]\n",
      "pattern: ['cos', [['_1']]]\n",
      "\n",
      "tree: ['cos', [['x3']]]\n",
      "pattern: ['cos', [['_1']]]\n",
      "tree_operator: cos\n",
      "tree_operands: [['x3']]\n",
      "pattern_operator: cos\n",
      "pattern_operands: [['_1']]\n",
      "\n",
      "tree: ['x3']\n",
      "pattern: ['_1']\n",
      "\n",
      "tree: ['x2']\n",
      "pattern: ['_2']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['x3'], '_2': ['x2']})"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', 'cos', 'x3', 'x2'], space.operator_arity), prefix_to_tree(['+', 'cos', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', [['cos', [['x1']]], ['x2']]]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_to_tree(['+', 'cos', 'x1', 'x2'], space.operator_arity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: ['+', [['cos', [['x1']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "\n",
      "tree: ['+', [['cos', [['x1']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "tree_operator: +\n",
      "tree_operands: [['cos', [['x1']]], ['x2']]\n",
      "pattern_operator: +\n",
      "pattern_operands: [['_1'], ['_2']]\n",
      "\n",
      "tree: ['cos', [['x1']]]\n",
      "pattern: ['_1']\n",
      "\n",
      "tree: ['x2']\n",
      "pattern: ['_2']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['cos', [['x1']]], '_2': ['x2']})"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', 'cos', 'x1', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: ['+', [['+', [['cos', [['x1']]], ['x2']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "\n",
      "tree: ['+', [['+', [['cos', [['x1']]], ['x2']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "tree_operator: +\n",
      "tree_operands: [['+', [['cos', [['x1']]], ['x2']]], ['x2']]\n",
      "pattern_operator: +\n",
      "pattern_operands: [['_1'], ['_2']]\n",
      "\n",
      "tree: ['+', [['cos', [['x1']]], ['x2']]]\n",
      "pattern: ['_1']\n",
      "\n",
      "tree: ['x2']\n",
      "pattern: ['_2']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['+', [['cos', [['x1']]], ['x2']]], '_2': ['x2']})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', '+', 'cos', 'x1', 'x2', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: ['-', [['+', [['cos', [['x1']]], ['x2']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "\n",
      "tree: ['-', [['+', [['cos', [['x1']]], ['x2']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "tree_operator: -\n",
      "tree_operands: [['+', [['cos', [['x1']]], ['x2']]], ['x2']]\n",
      "pattern_operator: +\n",
      "pattern_operands: [['_1'], ['_2']]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, {})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['-', '+', 'cos', 'x1', 'x2', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: ['+', [['-', [['cos', [['x1']]], ['x2']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "\n",
      "tree: ['+', [['-', [['cos', [['x1']]], ['x2']]], ['x2']]]\n",
      "pattern: ['+', [['_1'], ['_2']]]\n",
      "tree_operator: +\n",
      "tree_operands: [['-', [['cos', [['x1']]], ['x2']]], ['x2']]\n",
      "pattern_operator: +\n",
      "pattern_operands: [['_1'], ['_2']]\n",
      "\n",
      "tree: ['-', [['cos', [['x1']]], ['x2']]]\n",
      "pattern: ['_1']\n",
      "\n",
      "tree: ['x2']\n",
      "pattern: ['_2']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, {'_1': ['-', [['cos', [['x1']]], ['x2']]], '_2': ['x2']})"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_match(prefix_to_tree(['+', '-', 'cos', 'x1', 'x2', 'x2'], space.operator_arity), prefix_to_tree(['+', '_1', '_2'], space.operator_arity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _subtree_simplify(expression: list[str], rules_trees: dict[int, set[tuple[list[str], list[str]]]]) -> list[str]:\n",
    "    stack: list = []\n",
    "    i = len(expression) - 1\n",
    "\n",
    "    while i >= 0:\n",
    "        token = expression[i]\n",
    "\n",
    "        if token in space.operator_arity_compat or token in space.operator_aliases:\n",
    "            operator = space.operator_aliases.get(token, token)\n",
    "            arity = space.operator_arity_compat[operator]\n",
    "            operands = list(reversed(stack[-arity:]))\n",
    "\n",
    "            # Check if a pattern matches the current subtree\n",
    "            print(rules_trees[1][0])\n",
    "            \n",
    "\n",
    "            _ = [stack.pop() for _ in range(arity)]\n",
    "            stack.append([operator, operands])\n",
    "            i -= 1\n",
    "            continue\n",
    "\n",
    "        stack.append([token])\n",
    "        \n",
    "        i -= 1\n",
    "\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['pow1_2', ['<num>']], ['<num>'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['-', [['x1'], ['x1']]]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subtree_simplify(['-', 'x1', 'x1'], rules_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the rules to a file\n",
    "with open(f'./rules_constants_{size}.txt', 'w') as f:\n",
    "    for rule in rules:\n",
    "        f.write(f\"{rule[0]} -> {rule[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./rules_constants_{size}.json', 'w') as f:\n",
    "    json.dump(rules, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('*', ('x1',), ('cos', ('x2',)))\n",
      "('*', ({},), ('cos', ({},)))\n",
      "matched\n",
      "Applying backup ('*', ({},), ('cos', ({},))) to filled ('*', ('x1',), ('cos', ('x2',)))\n",
      "\n",
      "('*', ('x1',), ('sin', ('x2',)))\n",
      "('*', ({},), ('cos', ({},)))\n",
      "also matched\n"
     ]
    }
   ],
   "source": [
    "example_expression_positive = ['*', 'x1', 'cos', 'x2']\n",
    "example_tree_positive = prefix_to_tree(example_expression_positive, space.operator_arity_compat)\n",
    "\n",
    "example_pattern_expression = [(t if t not in dummy_variables else _) for t in example_expression_positive]\n",
    "example_pattern_tree = prefix_to_tree(example_pattern_expression, space.operator_arity_compat)\n",
    "example_pattern_tree_backup = deepcopy(example_pattern_tree)\n",
    "\n",
    "example_expression_negative = ['*', 'x1', 'sin', 'x2']\n",
    "example_tree_negative = prefix_to_tree(example_expression_negative, space.operator_arity_compat)\n",
    "\n",
    "print()\n",
    "print(example_tree_positive)\n",
    "print(example_pattern_tree)\n",
    "match example_tree_positive:\n",
    "    case example_pattern_tree:\n",
    "        print(\"matched\")\n",
    "\n",
    "print(f'Applying backup {example_pattern_tree_backup} to filled {example_pattern_tree}')\n",
    "example_pattern_tree = example_pattern_tree_backup\n",
    "print()\n",
    "print(example_tree_negative)\n",
    "print(example_pattern_tree)\n",
    "match example_tree_negative:\n",
    "    case example_pattern_tree:\n",
    "        print(\"also matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_tree(tree: list):\n",
    "    if isinstance(tree, (list, tuple)):\n",
    "        yield tree[0]\n",
    "        for subtree in tree[1:]:\n",
    "            yield from iterate_tree(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "match ['*', ['x1'], ['sin', ['x2']]]:\n",
    "    case ['*', [{}], ['cos', [{}]]]:\n",
    "        print(\"also matched\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-ansr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
