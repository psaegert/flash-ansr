"""Training orchestration logic for Flash-ANSR models.

This module provides the `Trainer` class, which encapsulates the full
training/validation loop as well as utilities for configuration driven
initialisation and experiment logging.
"""

import copy
import os
from collections import Counter
from typing import Any, Literal

import torch

from torch.optim.lr_scheduler import LRScheduler
from torch import nn

from tqdm import tqdm

from flash_ansr.model import FlashANSRModel
from flash_ansr.model.flash_ansr_model import NumericHeadPrediction
from flash_ansr.data import FlashANSRDataset
from flash_ansr.utils import load_config, save_config, substitute_root_path, unfold_config
from flash_ansr.eval.token_prediction import correct_token_predictions_at_k, reciprocal_rank
from flash_ansr.train.utils import OptimizerFactory, pw_linear_schedule

# ---------------------------------------------------------------------------
# Compatibility patches
# ---------------------------------------------------------------------------

# Pydantic 2.x emits UnsupportedFieldAttributeWarning when libraries such as
# wandb still pass ``repr``/``frozen`` to ``Field``. Strip these keywords before
# wandb imports its autogenerated models to keep logs clean without suppressing
# warnings globally.
try:  # pragma: no cover - defensive import guard
    import pydantic
    from pydantic import Field as _original_pydantic_field
    from pydantic import fields as _pydantic_fields
except Exception:  # noqa: BLE001 - best-effort shim; fall back silently
    _original_pydantic_field = None  # type: ignore[assignment]
else:
    assert _original_pydantic_field is not None

    def _field_without_unused_attrs(*args: Any, **kwargs: Any) -> Any:
        kwargs.pop('repr', None)
        kwargs.pop('frozen', None)
        return _original_pydantic_field(*args, **kwargs)

    pydantic.Field = _field_without_unused_attrs  # type: ignore[attr-defined]

    _orig_fieldinfo_init = _pydantic_fields.FieldInfo.__init__

    def _fieldinfo_init_without_unused_attrs(self: Any, *args: Any, **kwargs: Any) -> None:
        kwargs.pop('repr', None)
        kwargs.pop('frozen', None)
        _orig_fieldinfo_init(self, *args, **kwargs)

    _pydantic_fields.FieldInfo.__init__ = _fieldinfo_init_without_unused_attrs  # type: ignore[method-assign]

import wandb


def _enable_tf32_precision() -> None:
    if not torch.cuda.is_available():
        return

    matmul_backend = getattr(torch.backends.cuda, 'matmul', None)
    conv_backend = getattr(torch.backends.cudnn, 'conv', None)

    try:
        if matmul_backend is not None and hasattr(matmul_backend, 'fp32_precision'):
            matmul_backend.fp32_precision = 'tf32'
        if conv_backend is not None and hasattr(conv_backend, 'fp32_precision'):
            conv_backend.fp32_precision = 'tf32'
    except AttributeError:
        # Older Torch builds without the new API still honour the legacy helper.
        torch.set_float32_matmul_precision('high')


_enable_tf32_precision()


class Trainer:
    """Manage end-to-end training for a ``FlashANSRModel``.

    Notes
    -----
    Parameters mirror the components required to run training while being
    flexible enough to support configuration driven instantiation via
    :meth:`Trainer.from_config`.
    """

    def __init__(
            self,
            model: FlashANSRModel,
            optimizer: torch.optim.Optimizer,
            amp_dtype: torch.dtype,
            scaler: torch.amp.GradScaler,
            lr_scheduler: LRScheduler | None,
            batch_size: int,
            train_dataset: FlashANSRDataset,
            val_dataset: FlashANSRDataset,
            gradient_accumulation_steps: int = 1,
            config: dict[str, Any] = None) -> None:
        """Create a fully configured trainer instance.

        Parameters
        ----------
        model : FlashANSRModel
            The model to optimise.
        optimizer : torch.optim.Optimizer
            Optimiser responsible for updating model parameters.
        amp_dtype : torch.dtype
            Mixed precision dtype used with autocast.
        scaler : torch.amp.GradScaler
            Gradient scaler used for automatic mixed precision.
        lr_scheduler : LRScheduler or None
            Optional learning rate scheduler.
        batch_size : int
            Number of samples processed per training step.
        train_dataset : FlashANSRDataset
            Dataset providing training batches.
        val_dataset : FlashANSRDataset
            Dataset providing validation batches.
        gradient_accumulation_steps : int, default=1
            Number of micro-batches per optimizer step.
        config : dict[str, Any] or None, optional
            Trainer configuration metadata (used for logging).
        """

        self.model = model
        self.optimizer = optimizer
        self.amp_dtype = amp_dtype
        self.scaler = scaler
        self.lr_scheduler = lr_scheduler
        self.batch_size = batch_size
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.config = config or {}
        self.device = torch.device("cpu")  # Updated during ``run``
        default_worker_preprocess = self.train_dataset.preprocessor is not None
        if isinstance(self.config, dict) and 'worker_preprocess' in self.config:
            self.worker_preprocess = bool(self.config.get('worker_preprocess'))
        else:
            self.worker_preprocess = default_worker_preprocess

        # Metrics and Loss Functions
        self.metrics_ignore_index = self.model.tokenizer["<pad>"]
        self.cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=self.metrics_ignore_index)
        self.numeric_loss_weight = float(self.config.get('numeric_loss_weight', 1.0))
        self.numeric_loss_fn = nn.MSELoss(reduction='none')
        self.constant_token_id = self.model.tokenizer['<constant>']
        self.numeric_sigma_clamp_epsilon = float(self.config.get('numeric_sigma_clamp_epsilon', 1e-6))

        self.total_pflops = 0.0
        self.encoder_parameters = sum(p.numel() for p in self.model.encoder.parameters() if p.requires_grad)
        self.decoder_parameters = sum(p.numel() for p in self.model.decoder.parameters() if p.requires_grad)
        tokenizer_mapping = getattr(self.model.tokenizer, "token2idx", {})
        if not isinstance(tokenizer_mapping, dict):
            tokenizer_mapping = {}
        self._prompt_token_ids: dict[str, int | None] = {
            "complexity": tokenizer_mapping.get("<complexity>"),
        }
        self.prompt_combo_counts: Counter[tuple[tuple[Any, ...], tuple[Any, ...], tuple[Any, ...]]] = Counter()
        self.prompt_total_samples = 0
        self.decoder_max_seq_len = self._resolve_decoder_max_seq_len()

    @classmethod
    def from_config(cls, config: dict[str, Any] | str) -> "Trainer":
        """Instantiate a trainer from a config dictionary or YAML file path.

        Parameters
        ----------
        config : dict[str, Any] or str
            Dictionary of trainer settings or path to a YAML configuration file.

        Returns
        -------
        Trainer
            Trainer populated using the provided configuration.
        """
        config_ = load_config(config)

        if "trainer" in config_.keys():
            config_ = config_["trainer"]

        # Handle relative model paths
        if isinstance(config, str) and isinstance(config_["model"], str) and config_["model"].startswith('.'):
            config_["model"] = os.path.join(os.path.dirname(config), config_["model"])

        print(f'Creating model from {config_["model"]}')
        model = FlashANSRModel.from_config(config_["model"])

        print(f'Loading optimizer with config {config_["optimizer"]}')
        optimizer = OptimizerFactory.get_optimizer(config_['optimizer']['name'], params=model.parameters(), **config_['optimizer'].get('kwargs', {}))

        amp_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        scaler = torch.amp.GradScaler(enabled=(amp_dtype == torch.float16))

        print(f'Loading lr_scheduler with config {config_["lr_schedule"]}')
        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(
            optimizer=optimizer,
            lr_lambda=lambda step: pw_linear_schedule(step, config_['lr_schedule'])  # Pass the step and the points that define the piecewise linear schedule
        )

        print(f'Loading train_dataset with config {config_["train_dataset"]}')
        train_dataset = FlashANSRDataset.from_config(config_["train_dataset"])

        print(f'Loading val_dataset with config {config_["val_dataset"]}')
        if isinstance(config_["val_dataset"], str):
            resolved_config_path = substitute_root_path(config_["val_dataset"])
            if os.path.isfile(resolved_config_path):
                val_dataset = FlashANSRDataset.from_config(resolved_config_path)
            elif os.path.isdir(resolved_config_path):
                _, val_dataset = FlashANSRDataset.load(directory=resolved_config_path)
            else:
                raise ValueError(f"val_dataset must be a file or directory, not {resolved_config_path}")
        elif isinstance(config_["val_dataset"], dict):
            val_dataset = FlashANSRDataset.from_config(config_["val_dataset"])
        else:
            raise ValueError(f"val_dataset must be a dict or path to a file or directory, not {config_['val_dataset']}")

        config_.setdefault('preprocess', False)

        return cls(
            model=model,
            optimizer=optimizer,
            amp_dtype=amp_dtype,
            scaler=scaler,
            lr_scheduler=lr_scheduler,
            batch_size=config_['batch_size'],
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            gradient_accumulation_steps=config_.get("gradient_accumulation_steps", 1),
            config=config_
        )

    def _setup_training_state(self, device: str, verbose: bool) -> None:
        """Move the model to ``device`` and reset training state.

        Parameters
        ----------
        device : str
            Torch device string used for training (for example ``"cuda"``).
        verbose : bool
            If ``True`` emit human readable status messages.
        """
        self.device = torch.device(device)
        self.model.to(self.device)

        self.total_pflops = 0.0

        if verbose:
            config_value = os.environ.get('PYTORCH_CUDA_ALLOC_CONF')
            if config_value:
                print(f"PYTORCH_CUDA_ALLOC_CONF is set to: '{config_value}'")
            else:
                print("PYTORCH_CUDA_ALLOC_CONF is not set.")

    def _resolve_decoder_max_seq_len(self) -> int:
        max_seq_len = getattr(self.model, 'decoder_max_seq_len', None)
        if max_seq_len is None:
            decoder = getattr(self.model, 'decoder', None)
            rope = getattr(decoder, 'rope', None) if decoder is not None else None
            max_seq_len = getattr(rope, 'max_seq_len', None) if rope is not None else None

        if max_seq_len is None:
            raise ValueError("Unable to determine decoder max sequence length from model.")

        if hasattr(max_seq_len, 'item'):
            max_seq_len = max_seq_len.item()

        return int(max_seq_len)

    def run_training(
            self,
            steps: int,
            preprocess: bool = False,
            device: str = "cpu",
            compile_mode: str | None = None,
            checkpoint_interval: int | None = None,
            checkpoint_directory: str | None = None,
            validate_interval: int | None = None,
            validate_size: int | None = None,
            validate_batch_size: int = 128,
            preprocess_in_worker: bool | None = None,
            verbose: bool = False) -> FlashANSRModel:
        """Execute the core training loop.

        Parameters
        ----------
        steps : int
            Number of optimisation steps to perform.
        preprocess : bool, default=False
            Whether to run dataset preprocessing before each batch.
        device : str, default='cpu'
            Target device identifier passed to :func:`torch.device`.
        compile_mode : str or None, optional
            Torch.compile mode for model compilation.
        checkpoint_interval : int or None, optional
            Save checkpoints every ``n`` steps when provided.
        checkpoint_directory : str or None, optional
            Directory for persisted checkpoints.
        validate_interval : int or None, optional
            Frequency (in steps) to run validation.
        validate_size : int or None, optional
            Limit the number of validation examples processed.
        validate_batch_size : int, default=128
            Batch size used for validation passes.
        preprocess_in_worker : bool or None, optional
            When ``True`` run dataset preprocessing inside producer workers
            instead of the main process. Defaults to the trainer configuration
            when ``None`` (which enables worker preprocessing whenever the
            dataset has a preprocessor).
        verbose : bool, default=False
            If ``True`` display progress bars and additional logs.

        Returns
        -------
        FlashANSRModel
            The trained model instance.
        """

        worker_preprocess = self.worker_preprocess if preprocess_in_worker is None else bool(preprocess_in_worker)
        if not preprocess:
            worker_preprocess = False

        try:
            if compile_mode is not None:
                self.model = torch.compile(self.model, mode=compile_mode)

            self._setup_training_state(device, verbose=verbose)

            pbar = tqdm(range(steps), disable=not verbose, smoothing=0, desc="Training")
            for step, batch in enumerate(
                    self.train_dataset.iterate(
                        steps=steps,
                        batch_size=self.batch_size,
                        preprocess=preprocess,
                        preprocess_in_worker=worker_preprocess,
                        max_seq_len=self.decoder_max_seq_len)):
                self._train_step(batch, step, preprocess, do_optimizer_step=True)

                pbar.update(1)

                if validate_interval is not None and ((step + 1) % validate_interval == 0 or step == steps - 1):
                    self._validate_step(step + 1, validate_size, validate_batch_size, preprocess, worker_preprocess, verbose)

                if checkpoint_interval is not None and checkpoint_directory is not None and (step + 1) % checkpoint_interval == 0:
                    self._save_checkpoint(step + 1, checkpoint_directory)

            pbar.close()
            return self.model

        except Exception:
            self.train_dataset.shutdown()
            self.val_dataset.shutdown()
            raise

    def run(
            self,
            project_name: str,
            entity: str,
            name: str,
            steps: int,
            preprocess: bool = False,
            device: str = "cpu",
            compile_mode: str | None = None,
            checkpoint_interval: int | None = None,
            checkpoint_directory: str | None = None,
            validate_interval: int | None = None,
            validate_size: int | None = None,
            validate_batch_size: int = 128,
            wandb_mode: Literal['online', 'offline', 'disabled'] = 'online',
            wandb_watch_log: Literal['gradients', 'parameters', 'all'] | None = None,
            wandb_watch_log_freq: int = 1000,
            preprocess_in_worker: bool | None = None,
            verbose: bool = False) -> FlashANSRModel:
        """Train the model while managing the experiment lifecycle via W&B.

        Parameters
        ----------
        project_name : str
            Name of the Weights & Biases project.
        entity : str
            W&B entity (team or user) under which to log the run.
        name : str
            Run name displayed in W&B.
        steps : int
            Number of training iterations to execute.
        preprocess : bool, default=False
            Whether to preprocess dataset batches.
        device : str, default='cpu'
            Torch device string (for example ``"cpu"`` or ``"cuda"``).
        compile_mode : str or None, optional
            Torch.compile mode, if supported.
        checkpoint_interval : int or None, optional
            Step interval for checkpointing.
        checkpoint_directory : str or None, optional
            Directory where checkpoints are written when enabled.
        validate_interval : int or None, optional
            Step cadence for validation.
        validate_size : int or None, optional
            Maximum number of validation samples, if limited.
        validate_batch_size : int, default=128
            Batch size to use during validation.
        wandb_mode : {'online', 'offline', 'disabled'}, default='online'
            W&B initialisation mode.
        wandb_watch_log : {'gradients', 'parameters', 'all'} or None, optional
            W&B ``watch`` setting controlling what to log.
        wandb_watch_log_freq : int, default=1000
            Frequency (steps) for W&B gradient/parameter logging.
        preprocess_in_worker : bool or None, optional
            When ``True`` run preprocessing inside dataset workers. Defaults to
            the trainer configuration when omitted (which enables worker
            preprocessing when available).
        verbose : bool, default=False
            When ``True`` print progress information to stdout.

        Returns
        -------
        FlashANSRModel
            The trained model instance.
        """

        if verbose:
            print(f"Training model ({self.model.n_params:,} parameters) for {steps:,} steps on device {device}")

        wandb_config = unfold_config(copy.deepcopy(self.config))
        wandb_config.update({"steps": steps, "device": device, "verbose": verbose})

        with wandb.init(config=wandb_config, project=project_name, entity=entity, name=name, mode=wandb_mode):  # type: ignore
            if wandb_mode != 'disabled':
                wandb.watch(self.model, log=wandb_watch_log, log_freq=wandb_watch_log_freq)  # type: ignore
                if verbose and wandb_watch_log is not None:
                    print(f'Watching model with wandb log={wandb_watch_log} at frequency {wandb_watch_log_freq}')

            return self.run_training(
                steps=steps,
                preprocess=preprocess,
                device=device,
                compile_mode=compile_mode,
                checkpoint_interval=checkpoint_interval,
                checkpoint_directory=checkpoint_directory,
                validate_interval=validate_interval,
                validate_size=validate_size,
                validate_batch_size=validate_batch_size,
                preprocess_in_worker=preprocess_in_worker,
                verbose=verbose
            )

    def _update_total_pflops(self, encoder_tokens: int, decoder_tokens: int, batch_size: int) -> None:
        """Accumulate an estimate of total pico floating-point operations.

        Parameters
        ----------
        encoder_tokens : int
            Number of encoder tokens processed in the current step.
        decoder_tokens : int
            Number of decoder tokens processed in the current step.
        batch_size : int
            Size of the current batch (prior to micro batching).
        """
        self.total_pflops += 6 * (self.encoder_parameters * encoder_tokens + self.decoder_parameters * decoder_tokens) * batch_size * 1e-15

    def _apply_prompt_mask(self, batch: dict[str, torch.Tensor]) -> None:
        """Mask loss contributions for prompt tokens when available."""
        prompt_mask = batch.get('prompt_mask')
        if prompt_mask is None:
            return

        if not isinstance(prompt_mask, torch.Tensor):
            prompt_mask = torch.as_tensor(prompt_mask, device=self.device, dtype=torch.bool)
            batch['prompt_mask'] = prompt_mask
        else:
            if prompt_mask.dtype != torch.bool:
                prompt_mask = prompt_mask.to(dtype=torch.bool)
            batch['prompt_mask'] = prompt_mask.to(device=self.device)

        labels = batch.get('labels')
        if labels is None or not isinstance(labels, torch.Tensor):
            return

        if labels.device != self.device:
            labels = labels.to(self.device)
            batch['labels'] = labels

        target_mask = batch['prompt_mask'][..., 1:]
        if target_mask.shape[-1] > labels.shape[-1]:
            target_mask = target_mask[..., :labels.shape[-1]]
        elif target_mask.shape[-1] < labels.shape[-1]:
            labels = labels[..., :target_mask.shape[-1]]
            batch['labels'] = labels

        labels[target_mask] = self.metrics_ignore_index

    @staticmethod
    def _canonicalize_prompt_terms(terms: Any) -> tuple[tuple[Any, ...], ...]:
        if not terms:
            return tuple()

        normalised: list[tuple[Any, ...]] = []
        for term in terms:
            if isinstance(term, (list, tuple)):
                normalised.append(tuple(term))
            else:
                normalised.append((term,))
        return tuple(normalised)

    def _update_prompt_statistics(self, batch: dict[str, Any]) -> None:
        prompt_metadata = batch.get('prompt_metadata')
        if not prompt_metadata:
            return

        for metadata in prompt_metadata:
            if not isinstance(metadata, dict):
                continue

            allowed_terms = self._canonicalize_prompt_terms(metadata.get('allowed_terms'))
            include_terms = self._canonicalize_prompt_terms(metadata.get('include_terms'))
            exclude_terms = self._canonicalize_prompt_terms(metadata.get('exclude_terms'))

            combo_key = (allowed_terms, include_terms, exclude_terms)
            self.prompt_combo_counts[combo_key] += 1
            self.prompt_total_samples += 1

    def _train_step(self, batch: dict[str, torch.Tensor], step: int, preprocess: bool, do_optimizer_step: bool = True) -> None:
        """Perform a single optimisation step with optional gradient accumulation.

        Parameters
        ----------
        batch : dict[str, torch.Tensor]
            Batched tensors produced by the training dataset.
        step : int
            Zero-indexed global training step.
        preprocess : bool
            Whether to preprocess batch data prior to collation.
        do_optimizer_step : bool, default=True
            When ``False`` skip the optimiser update (useful for gradient
            accumulation across micro-batches).
        """
        self.model.train()
        self.optimizer.zero_grad()

        if preprocess and batch.get('prompt_metadata'):
            self._update_prompt_statistics(batch)

        total_ce_loss = 0.0
        total_numeric_loss_sum = 0.0
        total_numeric_count = 0
        total_token_count = 0
        total_loss = 0.0
        total_sigma_sum = 0.0
        total_sigma_count = 0
        sigma_min_value = float('inf')
        sigma_max_value = float('-inf')
        sigma_clamp_count = 0

        # Split the batch into micro-batches to support gradient accumulation
        for acc_step in range(self.gradient_accumulation_steps):
            micro_batch_size = len(batch['x_tensors']) // self.gradient_accumulation_steps
            micro_batch = {k: v[acc_step * micro_batch_size:(acc_step + 1) * micro_batch_size] for k, v in batch.items()}
            micro_batch = self.train_dataset.collate(micro_batch, device=self.device)
            self._apply_prompt_mask(micro_batch)

            data_tensor = torch.cat([micro_batch['x_tensors'], micro_batch['y_tensors']], dim=-1)

            with torch.autocast(device_type=self.device.type, dtype=self.amp_dtype):
                logits, numeric_pred = self.model(
                    micro_batch['input_ids'],
                    data_tensor,
                    input_num=micro_batch.get('input_num', None),
                    data_attn_mask=micro_batch['data_attn_mask'].to(self.device),
                )
                flat_logits = logits[:, :-1].reshape(-1, logits.shape[-1])
                flat_labels = micro_batch['labels'].reshape(-1)
                ce_loss = self.cross_entropy_loss(flat_logits, flat_labels)

                token_mask = flat_labels != self.metrics_ignore_index
                token_count_tensor = torch.sum(token_mask, dtype=logits.dtype)
                token_count_tensor = token_count_tensor.to(device=logits.device)
                token_count = int(token_count_tensor.item())
                total_token_count += token_count

                numeric_loss_val: torch.Tensor | None = None
                if numeric_pred is not None and 'input_num' in micro_batch:
                    numeric_targets = micro_batch['input_num'].squeeze(-1)
                    constant_mask = (micro_batch['input_ids'] == self.constant_token_id) & torch.isfinite(numeric_targets)

                    if constant_mask.any():
                        numeric_loss_val, _ = self.model.compute_numeric_loss(
                            numeric_pred,
                            numeric_targets,
                            constant_mask,
                        )
                        numeric_count_tensor = torch.sum(constant_mask, dtype=logits.dtype).to(device=logits.device)
                        numeric_count_value = int(numeric_count_tensor.item())
                        if isinstance(numeric_pred, NumericHeadPrediction) and numeric_pred.kind == 'mdn' and numeric_pred.sigma is not None:
                            sigma_values = numeric_pred.sigma
                            sigma_mask = constant_mask.unsqueeze(-1).unsqueeze(-1).expand_as(sigma_values)
                            masked_sigma = sigma_values.masked_select(sigma_mask)
                            if masked_sigma.numel() > 0:
                                masked_sigma_det = masked_sigma.detach()
                                finite_sigma = torch.isfinite(masked_sigma_det)
                                if not finite_sigma.all():
                                    masked_sigma_det = masked_sigma_det[finite_sigma]
                                if masked_sigma_det.numel() > 0:
                                    total_sigma_sum += float(masked_sigma_det.sum().item())
                                    total_sigma_count += masked_sigma_det.numel()
                                    min_value = float(masked_sigma_det.min().item())
                                    max_value = float(masked_sigma_det.max().item())
                                    if min_value < sigma_min_value:
                                        sigma_min_value = min_value
                                    if max_value > sigma_max_value:
                                        sigma_max_value = max_value
                                    sigma_clamp_count += int((masked_sigma_det <= self.numeric_sigma_clamp_epsilon).sum().item())

                        if numeric_loss_val is not None:
                            per_element_mean = numeric_loss_val
                            numeric_sum_tensor = per_element_mean * numeric_count_tensor

                            total_numeric_loss_sum += float(numeric_sum_tensor.detach().item())
                            total_numeric_count += numeric_count_value

                            if token_count > 0:
                                numeric_loss_val = numeric_sum_tensor / token_count_tensor
                            else:
                                numeric_loss_val = torch.zeros((), device=self.device, dtype=logits.dtype)
                        else:
                            numeric_loss_val = torch.zeros((), device=self.device, dtype=logits.dtype)
                    else:
                        numeric_loss_val = torch.zeros((), device=self.device, dtype=logits.dtype)

                # Force every parameter to contribute to the loss so that gradient
                # tracking tools (e.g. wandb.watch) do not encounter ``None`` gradients
                # for frozen or unused tensors.
                param_sum = sum(p.sum() for p in self.model.parameters())
                zero_loss = 0.0 * param_sum

                loss = ce_loss / self.gradient_accumulation_steps + zero_loss
                if numeric_loss_val is not None:
                    loss = loss + self.numeric_loss_weight * numeric_loss_val / self.gradient_accumulation_steps

            # If the loss is nan or inf, stop the training
            if not torch.isfinite(loss):
                raise ValueError(f"Loss is {loss.item()}, stopping training")

            self.scaler.scale(loss).backward()
            total_ce_loss += ce_loss.item()
            total_loss += loss.item() * self.gradient_accumulation_steps

        # Perform the optimizer step
        self.scaler.unscale_(self.optimizer)

        self._update_total_pflops(encoder_tokens=data_tensor.shape[1], decoder_tokens=micro_batch['input_ids'].shape[1], batch_size=len(batch['x_tensors']))

        total_gradient_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), 2.0)
        if do_optimizer_step:
            self.scaler.step(self.optimizer)
            self.scaler.update()

            # Log metrics and update scheduler after the optimizer step
            if total_token_count > 0:
                per_token_numeric_loss = total_numeric_loss_sum / total_token_count
                numeric_token_fraction = total_numeric_count / total_token_count
            else:
                per_token_numeric_loss = 0.0
                numeric_token_fraction = 0.0

            if total_numeric_count > 0:
                per_constant_numeric_loss = total_numeric_loss_sum / total_numeric_count
            else:
                per_constant_numeric_loss = 0.0
            if total_sigma_count > 0:
                sigma_mean = total_sigma_sum / total_sigma_count
                sigma_min = sigma_min_value
                sigma_max = sigma_max_value
                sigma_clamp_fraction = sigma_clamp_count / total_sigma_count
                sigma_count_logged = total_sigma_count
            else:
                sigma_mean = None
                sigma_min = None
                sigma_max = None
                sigma_clamp_fraction = None
                sigma_count_logged = None
            self._log_metrics(
                step,
                flat_logits,
                flat_labels,
                total_ce_loss,
                total_loss,
                total_gradient_norm,
                per_token_numeric_loss,
                per_constant_numeric_loss,
                numeric_token_fraction,
                total_numeric_count,
                sigma_mean,
                sigma_min,
                sigma_max,
                sigma_clamp_fraction,
                sigma_count_logged,
            )
            if self.lr_scheduler is not None:
                self.lr_scheduler.step()

    def _validate_step(self, step: int, size: int | None, batch_size: int, preprocess: bool, worker_preprocess: bool, verbose: bool) -> None:
        """Evaluate the model on the validation split and log aggregate metrics.

        Parameters
        ----------
        step : int
            Global training step at which validation is triggered.
        size : int or None
            Optional limit on the number of validation examples.
        batch_size : int
            Number of samples per validation batch.
        preprocess : bool
            Whether to preprocess validation batches.
        worker_preprocess : bool
            Whether preprocessing should run inside dataset workers.
        verbose : bool
            If ``True`` display a validation progress bar.
        """
        self.model.eval()

        val_ce_loss = 0.0
        val_mrr = 0.0
        val_acc_at_1 = 0.0
        val_numeric_loss_sum = 0.0
        val_numeric_count = 0
        val_token_count = 0
        total_items = 0
        total_batches = 0
        val_sigma_sum = 0.0
        val_sigma_count = 0
        val_sigma_min_value = float('inf')
        val_sigma_max_value = float('-inf')
        val_sigma_clamp_count = 0

        with torch.no_grad():
            if size is None:
                steps = len(self.val_dataset) // batch_size
            else:
                steps = size // batch_size

            pbar = tqdm(total=steps, leave=False, position=1, disable=not verbose, desc="Validating", smoothing=0.0)
            for batch in self.val_dataset.iterate(
                    size=size,
                    batch_size=batch_size,
                    preprocess=preprocess,
                    preprocess_in_worker=worker_preprocess,
                    max_seq_len=self.decoder_max_seq_len):
                batch = self.val_dataset.collate(batch, device=self.device)
                self._apply_prompt_mask(batch)
                data_tensor = torch.cat([batch['x_tensors'], batch['y_tensors']], dim=-1)

                with torch.autocast(device_type=self.device.type, dtype=self.amp_dtype):
                    logits, numeric_pred = self.model(
                        batch['input_ids'],
                        data_tensor,
                        input_num=batch.get('input_num', None),
                        data_attn_mask=batch['data_attn_mask'].to(self.device),
                    )

                    flat_logits = logits[:, :-1].reshape(-1, logits.shape[-1])
                    flat_labels = batch['labels'].reshape(-1)
                    ce_loss = self.cross_entropy_loss(flat_logits, flat_labels)

                    # Accumulate metrics for each batch
                    val_ce_loss += ce_loss.item() * flat_labels.shape[0]
                    total_items += flat_labels.shape[0]

                    valid_indices = flat_labels != self.metrics_ignore_index
                    val_token_count += int(valid_indices.sum().item())

                    if numeric_pred is not None and 'input_num' in batch:
                        numeric_targets = batch['input_num'].squeeze(-1)
                        constant_mask = (batch['input_ids'] == self.constant_token_id) & torch.isfinite(numeric_targets)
                        if constant_mask.any():
                            numeric_loss_val, _ = self.model.compute_numeric_loss(
                                numeric_pred,
                                numeric_targets,
                                constant_mask,
                            )
                            numeric_count_tensor = torch.sum(constant_mask, dtype=logits.dtype).to(device=logits.device)
                            numeric_count_value = int(numeric_count_tensor.item())
                            if isinstance(numeric_pred, NumericHeadPrediction) and numeric_pred.kind == 'mdn' and numeric_pred.sigma is not None:
                                sigma_values = numeric_pred.sigma
                                sigma_mask = constant_mask.unsqueeze(-1).unsqueeze(-1).expand_as(sigma_values)
                                masked_sigma = sigma_values.masked_select(sigma_mask)
                                if masked_sigma.numel() > 0:
                                    masked_sigma_det = masked_sigma.detach()
                                    finite_sigma = torch.isfinite(masked_sigma_det)
                                    if not finite_sigma.all():
                                        masked_sigma_det = masked_sigma_det[finite_sigma]
                                    if masked_sigma_det.numel() > 0:
                                        val_sigma_sum += float(masked_sigma_det.sum().item())
                                        val_sigma_count += masked_sigma_det.numel()
                                        min_value = float(masked_sigma_det.min().item())
                                        max_value = float(masked_sigma_det.max().item())
                                        if min_value < val_sigma_min_value:
                                            val_sigma_min_value = min_value
                                        if max_value > val_sigma_max_value:
                                            val_sigma_max_value = max_value
                                        val_sigma_clamp_count += int((masked_sigma_det <= self.numeric_sigma_clamp_epsilon).sum().item())

                            if numeric_loss_val is not None:
                                numeric_sum_tensor = numeric_loss_val * numeric_count_tensor
                                val_numeric_loss_sum += float(numeric_sum_tensor.detach().item())
                                val_numeric_count += numeric_count_value

                # Filter out ignored indices for metric calculation
                if valid_indices.any():
                    val_mrr += reciprocal_rank(flat_logits[valid_indices], flat_labels[valid_indices])
                    val_acc_at_1 += correct_token_predictions_at_k(flat_logits[valid_indices], flat_labels[valid_indices], k=1)
                    total_batches += 1

                pbar.update(1)
            pbar.close()

        # Calculate average metrics
        avg_val_ce_loss = val_ce_loss / total_items if total_items > 0 else 0.0
        if val_token_count > 0:
            avg_val_numeric_loss_per_token = val_numeric_loss_sum / val_token_count
            val_numeric_token_fraction = val_numeric_count / val_token_count
        else:
            avg_val_numeric_loss_per_token = 0.0
            val_numeric_token_fraction = 0.0
        if val_numeric_count > 0:
            avg_val_numeric_loss_per_constant = val_numeric_loss_sum / val_numeric_count
        else:
            avg_val_numeric_loss_per_constant = 0.0
        if val_sigma_count > 0:
            val_numeric_sigma_mean = val_sigma_sum / val_sigma_count
            val_numeric_sigma_min = val_sigma_min_value
            val_numeric_sigma_max = val_sigma_max_value
            val_numeric_sigma_clamp_fraction = val_sigma_clamp_count / val_sigma_count
            val_sigma_count_logged = val_sigma_count
        else:
            val_numeric_sigma_mean = None
            val_numeric_sigma_min = None
            val_numeric_sigma_max = None
            val_numeric_sigma_clamp_fraction = None
            val_sigma_count_logged = None
        avg_val_mrr = val_mrr / total_batches if total_batches > 0 else 0.0
        avg_val_acc_at_1 = val_acc_at_1 / total_batches if total_batches > 0 else 0.0

        # Log averaged validation metrics
        self._log_validation_metrics(
            step,
            avg_val_ce_loss,
            avg_val_mrr,
            avg_val_acc_at_1,
            avg_val_numeric_loss_per_token,
            avg_val_numeric_loss_per_constant,
            val_numeric_token_fraction,
            val_numeric_count,
            val_numeric_sigma_mean,
            val_numeric_sigma_min,
            val_numeric_sigma_max,
            val_numeric_sigma_clamp_fraction,
            val_sigma_count_logged,
        )

    def _save_checkpoint(self, step: int, checkpoint_directory: str) -> None:
        """Persist model weights, optimiser state, and config for ``step``.

        Parameters
        ----------
        step : int
            Training step associated with the persisted checkpoint.
        checkpoint_directory : str
            Directory into which the checkpoint will be written.
        """
        save_directory = os.path.join(checkpoint_directory, f"checkpoint_{step}")
        self.model.save(directory=save_directory, errors='ignore')
        torch.save(self.optimizer.state_dict(), os.path.join(save_directory, "optimizer.pt"))
        save_config(
            load_config(self.config, resolve_paths=True),
            directory=save_directory,
            filename='train.yaml',
            reference='relative',
            recursive=True,
            resolve_paths=True)
        print(f"Checkpoint saved at {save_directory}")

    def _log_metrics(
            self,
            step: int,
            logits: torch.Tensor,
            labels: torch.Tensor,
            ce_loss: float,
            total_loss: float,
            total_gradient_norm: torch.Tensor,
            numeric_loss_per_token: float | None,
            numeric_loss_per_constant: float | None = None,
            numeric_token_fraction: float | None = None,
            numeric_token_count: int | None = None,
            numeric_sigma_mean: float | None = None,
            numeric_sigma_min: float | None = None,
            numeric_sigma_max: float | None = None,
            numeric_sigma_clamp_fraction: float | None = None,
            numeric_sigma_count: int | None = None) -> None:
        """Submit training metrics for the current batch to Weights & Biases."""
        log_data = {
            "total_gradient_norm": total_gradient_norm.item(),
            "train_ce_loss": ce_loss,
            "train_loss": total_loss,
            "lr": self.optimizer.param_groups[0]['lr'],
            "train_mean_reciprocal_rank": reciprocal_rank(logits, labels, ignore_index=self.metrics_ignore_index),
            "train_correct_token_predictions_at_1": correct_token_predictions_at_k(logits, labels, k=1, ignore_index=self.metrics_ignore_index),
            "total_pflops": self.total_pflops,
        }
        if numeric_loss_per_token is not None:
            log_data["train_numeric_loss_per_token"] = numeric_loss_per_token
        if numeric_loss_per_constant is not None:
            log_data["train_numeric_loss_per_constant"] = numeric_loss_per_constant
        if numeric_token_fraction is not None:
            log_data["train_numeric_token_fraction"] = numeric_token_fraction
        if numeric_token_count is not None:
            log_data["train_numeric_token_count"] = numeric_token_count
        if numeric_sigma_mean is not None:
            log_data["train_numeric_sigma_mean"] = numeric_sigma_mean
        if numeric_sigma_min is not None:
            log_data["train_numeric_sigma_min"] = numeric_sigma_min
        if numeric_sigma_max is not None:
            log_data["train_numeric_sigma_max"] = numeric_sigma_max
        if numeric_sigma_clamp_fraction is not None:
            log_data["train_numeric_sigma_clamp_fraction"] = numeric_sigma_clamp_fraction
        if numeric_sigma_count is not None:
            log_data["train_numeric_sigma_count"] = numeric_sigma_count

        wandb.log(log_data, step=step)  # type: ignore

    def _log_validation_metrics(
            self,
            step: int,
            val_ce_loss: float,
            val_mrr: float,
            val_acc_at_1: float,
            val_numeric_loss_per_token: float | None,
            val_numeric_loss_per_constant: float | None = None,
            val_numeric_token_fraction: float | None = None,
            val_numeric_token_count: int | None = None,
            val_numeric_sigma_mean: float | None = None,
            val_numeric_sigma_min: float | None = None,
            val_numeric_sigma_max: float | None = None,
            val_numeric_sigma_clamp_fraction: float | None = None,
            val_numeric_sigma_count: int | None = None) -> None:
        log_data = {
            "val_ce_loss": val_ce_loss,
            "val_mean_reciprocal_rank": val_mrr,
            "val_correct_token_predictions_at_1": val_acc_at_1,
        }
        if val_numeric_loss_per_token is not None:
            log_data["val_numeric_loss_per_token"] = val_numeric_loss_per_token
        if val_numeric_loss_per_constant is not None:
            log_data["val_numeric_loss_per_constant"] = val_numeric_loss_per_constant
        if val_numeric_token_fraction is not None:
            log_data["val_numeric_token_fraction"] = val_numeric_token_fraction
        if val_numeric_token_count is not None:
            log_data["val_numeric_token_count"] = val_numeric_token_count
        if val_numeric_sigma_mean is not None:
            log_data["val_numeric_sigma_mean"] = val_numeric_sigma_mean
        if val_numeric_sigma_min is not None:
            log_data["val_numeric_sigma_min"] = val_numeric_sigma_min
        if val_numeric_sigma_max is not None:
            log_data["val_numeric_sigma_max"] = val_numeric_sigma_max
        if val_numeric_sigma_clamp_fraction is not None:
            log_data["val_numeric_sigma_clamp_fraction"] = val_numeric_sigma_clamp_fraction
        if val_numeric_sigma_count is not None:
            log_data["val_numeric_sigma_count"] = val_numeric_sigma_count
        wandb.log(log_data, step=step)  # type: ignore
