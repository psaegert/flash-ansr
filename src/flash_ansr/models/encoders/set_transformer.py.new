import math

import torch
import torch.nn.functional as F
from torch import nn

from flash_ansr.models.encoders.set_encoder import SetEncoder


class MAB(nn.Module):
    def __init__(
            self,
            size: int,
            n_heads: int,
            dim_feedforward: int | None = None,
            dropout: float = 0.0) -> None:
        super().__init__()

        self.mha = nn.MultiheadAttention(embed_dim=size, num_heads=n_heads, batch_first=True)

        self.dim_feedforward = dim_feedforward or size * 4

        self.ff = nn.Sequential(
            nn.Linear(size, self.dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(self.dim_feedforward, size)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, query: torch.Tensor, key_value: torch.Tensor) -> torch.Tensor:
        attn_output = self.mha(query, key_value, key_value)[0]
        query = query + self.dropout(attn_output)

        # Apply feed-forward network with residual connection
        ff_output = self.ff(query)
        query = query + self.dropout(ff_output)

        return query


class SAB(nn.Module):
    # https://github.com/juho-lee/set_transformer
    def __init__(self, size: int, n_heads: int) -> None:
        super().__init__()
        self.mab = MAB(size, n_heads)

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        return self.mab(X, X)


class ISAB(nn.Module):
    # https://github.com/juho-lee/set_transformer
    def __init__(self, size: int, n_heads: int, n_induce: int) -> None:
        super().__init__()

        self.inducing_points = nn.Parameter(torch.Tensor(1, n_induce, size))
        nn.init.xavier_uniform_(self.inducing_points)

        self.mab0 = MAB(size, n_heads)
        self.mab1 = MAB(size, n_heads)

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        H = self.mab0(self.inducing_points.repeat(X.size(0), 1, 1), X)
        return self.mab1(X, H)


class PMA(nn.Module):
    # https://github.com/juho-lee/set_transformer
    def __init__(self, size: int, n_heads: int, n_seeds: int) -> None:
        super().__init__()

        self.S = nn.Parameter(torch.Tensor(1, n_seeds, size))
        nn.init.xavier_uniform_(self.S)

        self.mab = MAB(size, n_heads)

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        return self.mab(self.S.repeat(X.size(0), 1, 1), X)


class SetTransformer(SetEncoder):
    # https://github.com/juho-lee/set_transformer
    def __init__(
            self,
            input_embedding_size: int,
            input_dimension_size: int,
            output_embedding_size: int,
            n_seeds: int,
            hidden_size: int = 512,
            n_enc_isab: int = 2,
            n_dec_sab: int = 2,
            n_induce: int | list[int] = 64,
            n_heads: int = 4,
            layer_norm: bool = False) -> None:
        super().__init__()
        if n_enc_isab < 1:
            raise ValueError(f"Number of ISABs in encoder `n_enc_isab` ({n_enc_isab}) must be greater than 0")

        if n_dec_sab < 0:
            raise ValueError(f"Number of SABs in decoder `n_dec_sab` ({n_dec_sab}) cannot be negative")

        if isinstance(n_induce, int):
            n_induce = [n_induce] * n_enc_isab
        elif len(n_induce) != n_enc_isab:
            raise ValueError(
                f"Number of inducing points `n_induce` ({n_induce}) must be an integer or a list of length {n_enc_isab}")

        self.enc = nn.Sequential(
            nn.Linear(input_embedding_size * input_dimension_size, hidden_size),
            *[ISAB(hidden_size, n_heads, n_induce[i]) for i in range(n_enc_isab)])

        self.dec = nn.Sequential(
            PMA(hidden_size, n_heads, n_seeds),
            *[SAB(hidden_size, n_heads) for _ in range(n_dec_sab)],
            nn.Linear(hidden_size, output_embedding_size))

        self.input_embedding_size = input_embedding_size
        self.input_dimension_size = input_dimension_size
        self.output_embedding_size = output_embedding_size
        self.n_seeds = n_seeds

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        # X: (B, M, D * E)

        return self.dec(self.enc(X))
