- **Architecture** FlashANSR pairs a SetTransformer encoder with a TransformerDecoder (`src/flash_ansr/model/flash_ansr_model.py`) to map tabular data to prefix programs.
- The orchestrator (`src/flash_ansr/flash_ansr.py`) layers decoding, prompt handling, constant refinement, and scoring; keep it the entry point for inference-facing changes.
- `Refiner` (`src/flash_ansr/refine.py`) recompiles expressions through SimpliPy to fit constants with curve-fit or BFGS and logs every restart; reuse its helpers instead of re-implementing numpy/scipy loops.
- Simplification and operator metadata come from SimpliPy configs; they are auto-downloaded when `SimpliPyEngine.load(..., install=True)` runs.
- Tokenization is config-driven (`configs/**/tokenizer.yaml`), and special prompt tokens (`<prompt>`, `<complexity>`, `<expression>`, etc.) must exist before enabling prompt features.
- **Data Flow** Training/inference batches use prefix skeletons + numeric support points built by `SkeletonPool` (`src/flash_ansr/expressions/skeleton_pool.py`) and stored in YAML+pickle bundles.
- `FlashANSRDataset` (`src/flash_ansr/data.py`) streams procedurally generated samples via multiprocessing; always call `dataset.shutdown()` on early exits to release shared memory.
- Preprocessing (`src/flash_ansr/preprocess.py`) can inject prompt constraints on the fly; set probabilities in `preprocessor.prompt` configs and guard for missing tokenizer tokens via `serialize_prompt_prefix` warnings.
- Datasets, models, and trainers read YAML through `flash_ansr.utils.load_config`, which normalises `./` and `{{ROOT}}` paths; keep relative paths stable when copying configs.
- **Generation** Use `BeamSearchConfig`, `SoftmaxSamplingConfig`, or `MCTSGenerationConfig` (all in `flash_ansr/utils/generation.py`) to tune decoding; pass overrides through these configs instead of branching inside model code.
- Beam search/equivalence pruning rely on the simplifier cache inside `FlashANSRModel.beam_search`; if you extend pruning logic, maintain the heap+dict invariants.
- Monte Carlo search (`src/flash_ansr/decoding/mcts.py`) expects a callable reward; reuse `FlashANSR._build_mcts_config` when wiring new policies.
- **Training** `Trainer` (`src/flash_ansr/train/train.py`) wires the model, optimizer factory, LR schedule, datasets, and W&B logging; use `Trainer.from_config` to keep CLI, scripts, and tests aligned.
- Training configs (`configs/*/train.yaml`) reference sibling dataset/model YAMLs via relative paths; `save_config(..., reference='relative')` writes portable snapshots after runs.
- Mixed precision defaults to bf16 when CUDA supports it; for fp16 runs, leave gradient scaler enabled (`torch.amp.GradScaler`) to avoid silent overflow.
- Batch iteration pulls from `FlashANSRDataset.iterate`; respect `max_seq_len` from the model when creating new sampling code to avoid decoder shape mismatches.
- Checkpointing relies on `FlashANSRModel.save` writing `model.yaml`, `tokenizer.yaml`, and `state_dict.pt`; keep this trio intact when designing export/import utilities.
- **CLI & Scripts** The package entry point (`flash_ansr.__main__`) drives data import, skeleton generation, training, and evaluation; scripts in `scripts/` are thin wrappers calling these subcommands.
- Use `flash_ansr install <repo>` or `install_model` (`src/flash_ansr/model/manage.py`) to pull Hugging Face checkpoints into `models/`; tests assume `psaegert/flash-ansr-v21.0-60M` is cached.
- Generation/evaluation pipelines expect datasets under `data/ansr-data/**`; helper scripts (`import_test_sets.sh`, `generate_validation_set.sh`) bake in the directory layout from the README instructions.
- When using the CLI, you must activate the `flash-ansr` conda environment to ensure dependencies are found.
- **Coding Conventions** Keep tensors float32 except explicit precision modules; numeric embeddings live in `FlashANSRModel.numeric_embedding` and expect NaNs for padding.
- Treat model configs as immutable at runtime; prefer cloning (`copy.deepcopy`) when you need per-run tweaks to avoid leaking state into saved YAMLs.
- Prompt metadata is propagated through batches as lists; when adding dataloader fields, update `FlashANSRDataset.collate` to pad/stack them consistently.
- Prefer `get_path(...)` for writing assets so files stay relative to the package root and compatible with wheel installs.
- **Testing & QA** `./scripts/pytest.sh` runs the full suite with coverage; linting via `./scripts/pylint.sh` respects relaxed line-length but keeps other defaults.
- Training tests mock W&B; when adding wandb calls, guard them with the provided `wandb_mode` flag so `Trainer.run(..., wandb_mode='disabled')` remains quiet.
- Integration tests download models and data; consider setting `HF_HOME`/`HF_HUB_CACHE` in CI notes if you add larger checkpoints.
- **External Dependencies** Core runtime depends on `torch`, `simplipy`, `datasets`, `wandb`, and `torch_optimizer`; keep new deps inside `requirements.txt` and gate experimental-only imports behind optional extras.
- Hugging Face downloads go through `huggingface_hub.snapshot_download`; avoid manual HTTP code so credential handling stays centralised.
- SimpliPy rule updates produce large diffs; store them in `configs/` directories and reference via YAML instead of hardcoding rule text in Python.
- **Workflow Tips** Always resolve configs with `load_config(..., resolve_paths=True)` inside tools/scripts; this keeps nested references working when users run outside the repo root.
- Prefer reusing dataset/test fixtures under `configs/test/**` for unit tests; they create small skeleton pools and avoid touching production-scale assets.
- When touching decoding paths, update `tests/test_inference.py` and ensure beams still decode to valid expressions (no stray `<constant>` placeholders).
- Save evaluation outputs as pickles following the existing schema (`expression`, `log_prob`, `fits`, etc.) so downstream notebooks under `experimental/` keep working.
- Ask the user before deleting or overwriting models/datasets; existing CLIs already promptâ€”follow that pattern in new commands.